{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-09T15:17:35.284808Z",
     "start_time": "2024-12-09T15:17:35.278442Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "13e09ed436376dbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T15:17:35.584758Z",
     "start_time": "2024-12-09T15:17:35.577993Z"
    }
   },
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "e1d2f6222ee7ad89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T15:17:35.941754Z",
     "start_time": "2024-12-09T15:17:35.933998Z"
    }
   },
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_sizes, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_dim\n",
    "        for hs in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hs))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hs\n",
    "        layers.append(nn.Linear(prev_size, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.model(x)\n",
    "        return torch.softmax(logits, dim=-1)\n",
    "\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_sizes):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_dim\n",
    "        for hs in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hs))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hs\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "39d88d020add6902",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T15:32:11.969266Z",
     "start_time": "2024-12-09T15:32:11.962535Z"
    }
   },
   "source": [
    "def actor_critic(env,\n",
    "                 input_dim,\n",
    "                 output_dim,\n",
    "                 hidden_sizes,\n",
    "                 alpha_theta=0.001,\n",
    "                 alpha_w=0.001,\n",
    "                 episodes=500,\n",
    "                 gamma=0.99,\n",
    "                 log_dir=\"runs/actor_critic\"):\n",
    "    \"\"\"\n",
    "    One-step Actor-Critic (episodic) training loop.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI gym environment\n",
    "        input_dim: dimension of input space\n",
    "        output_dim: number of actions\n",
    "        hidden_sizes: list of sizes for each hidden layer\n",
    "        alpha_theta: learning rate for the policy network\n",
    "        alpha_w: learning rate for the value network\n",
    "        episodes: number of training episodes\n",
    "        gamma: discount factor\n",
    "        log_dir: directory for tensorboard logs\n",
    "\n",
    "    Returns:\n",
    "        policy_network: the trained policy network\n",
    "        rewards_per_episode: a list containing the total reward per episode\n",
    "    \"\"\"\n",
    "\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    policy_network = PolicyNetwork(input_dim, hidden_sizes, output_dim).to(device)\n",
    "    value_network = ValueNetwork(input_dim, hidden_sizes).to(device)\n",
    "\n",
    "    policy_optimizer = optim.Adam(policy_network.parameters(), lr=alpha_theta)\n",
    "    value_optimizer = optim.Adam(value_network.parameters(), lr=alpha_w)\n",
    "\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "\n",
    "        # Initialize eligibility trace (I)\n",
    "        I = 1.0\n",
    "        # steps = 0\n",
    "        while not done and not truncated:\n",
    "            # steps += 1\n",
    "            # if steps % 50 == 0:\n",
    "            #     print(f\"Episode {episode + 1}/{episodes}, Steps: {steps}\")\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            # Policy network outputs action probabilities\n",
    "            action_probs = policy_network(state_tensor)\n",
    "            action_dist = torch.distributions.Categorical(action_probs)\n",
    "            action = action_dist.sample()\n",
    "            log_prob = action_dist.log_prob(action)\n",
    "\n",
    "            # Take the action\n",
    "            next_state, reward, done, truncated, info = env.step(action.item())\n",
    "            total_reward += reward\n",
    "\n",
    "            # Compute value predictions\n",
    "            value = value_network(state_tensor)\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            next_value = value_network(next_state_tensor) if not done else torch.tensor(0.0, device=device)\n",
    "\n",
    "            # TD-error δ = R + γ * V(S') - V(S)\n",
    "            td_error = reward + gamma * next_value - value\n",
    "\n",
    "            # Update value network\n",
    "            value_loss = -I * td_error.detach() * value\n",
    "            value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            value_optimizer.step()\n",
    "\n",
    "            # Update policy network\n",
    "            policy_loss = -I * td_error.detach() * log_prob\n",
    "            policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            policy_optimizer.step()\n",
    "\n",
    "            # Update eligibility trace\n",
    "            I *= gamma\n",
    "\n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}\")\n",
    "\n",
    "        # Log metrics to TensorBoard\n",
    "        writer.add_scalar(\"Total Reward\", total_reward, episode)\n",
    "\n",
    "        # Early stopping if solved\n",
    "        if len(rewards_per_episode) >= 100 and np.mean(rewards_per_episode[-100:]) >= 475.0:\n",
    "            print(f\"Solved in {episode + 1} episodes!\")\n",
    "            break\n",
    "\n",
    "    writer.close()\n",
    "    return policy_network, value_network, rewards_per_episode"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "id": "fb5ef7913176ef84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T15:32:12.421715Z",
     "start_time": "2024-12-09T15:32:12.418901Z"
    }
   },
   "source": [
    "def test_policy(env, policy_network, episodes=10):\n",
    "    policy_network.eval()\n",
    "    avg_reward = 0\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                action_probs = policy_network(state_tensor)\n",
    "            action = torch.argmax(action_probs).item()\n",
    "            state, reward, done, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if total_reward >= 10000:\n",
    "                break\n",
    "\n",
    "        avg_reward += total_reward\n",
    "        print(f\"Test Episode {ep + 1}, Total Reward: {total_reward}\")\n",
    "    return avg_reward / episodes"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "id": "60e3b164384bc6a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T15:38:23.764999Z",
     "start_time": "2024-12-09T15:32:12.973900Z"
    }
   },
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "hidden_sizes = [16, 32, 16]\n",
    "\n",
    "episodes = 2000\n",
    "gamma = 0.99\n",
    "alpha_theta = 0.001\n",
    "alpha_w = 0.001\n",
    "\n",
    "policy_network, value_network, rewards = actor_critic(env,\n",
    "                                       input_dim=input_dim,\n",
    "                                       output_dim=output_dim,\n",
    "                                       hidden_sizes=hidden_sizes,\n",
    "                                       alpha_theta=alpha_theta,\n",
    "                                       alpha_w=alpha_w,\n",
    "                                       episodes=episodes,\n",
    "                                       gamma=gamma)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(rewards)\n",
    "plt.title(\"Total Rewards per Episode (Actor-Critic)\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/2000, Total Reward: 21.0\n",
      "Episode 2/2000, Total Reward: 36.0\n",
      "Episode 3/2000, Total Reward: 16.0\n",
      "Episode 4/2000, Total Reward: 30.0\n",
      "Episode 5/2000, Total Reward: 29.0\n",
      "Episode 6/2000, Total Reward: 17.0\n",
      "Episode 7/2000, Total Reward: 13.0\n",
      "Episode 8/2000, Total Reward: 16.0\n",
      "Episode 9/2000, Total Reward: 11.0\n",
      "Episode 10/2000, Total Reward: 12.0\n",
      "Episode 11/2000, Total Reward: 17.0\n",
      "Episode 12/2000, Total Reward: 25.0\n",
      "Episode 13/2000, Total Reward: 26.0\n",
      "Episode 14/2000, Total Reward: 34.0\n",
      "Episode 15/2000, Total Reward: 12.0\n",
      "Episode 16/2000, Total Reward: 15.0\n",
      "Episode 17/2000, Total Reward: 12.0\n",
      "Episode 18/2000, Total Reward: 11.0\n",
      "Episode 19/2000, Total Reward: 16.0\n",
      "Episode 20/2000, Total Reward: 14.0\n",
      "Episode 21/2000, Total Reward: 15.0\n",
      "Episode 22/2000, Total Reward: 15.0\n",
      "Episode 23/2000, Total Reward: 17.0\n",
      "Episode 24/2000, Total Reward: 23.0\n",
      "Episode 25/2000, Total Reward: 27.0\n",
      "Episode 26/2000, Total Reward: 17.0\n",
      "Episode 27/2000, Total Reward: 23.0\n",
      "Episode 28/2000, Total Reward: 11.0\n",
      "Episode 29/2000, Total Reward: 32.0\n",
      "Episode 30/2000, Total Reward: 31.0\n",
      "Episode 31/2000, Total Reward: 17.0\n",
      "Episode 32/2000, Total Reward: 16.0\n",
      "Episode 33/2000, Total Reward: 13.0\n",
      "Episode 34/2000, Total Reward: 11.0\n",
      "Episode 35/2000, Total Reward: 24.0\n",
      "Episode 36/2000, Total Reward: 20.0\n",
      "Episode 37/2000, Total Reward: 18.0\n",
      "Episode 38/2000, Total Reward: 23.0\n",
      "Episode 39/2000, Total Reward: 48.0\n",
      "Episode 40/2000, Total Reward: 18.0\n",
      "Episode 41/2000, Total Reward: 32.0\n",
      "Episode 42/2000, Total Reward: 134.0\n",
      "Episode 43/2000, Total Reward: 25.0\n",
      "Episode 44/2000, Total Reward: 27.0\n",
      "Episode 45/2000, Total Reward: 22.0\n",
      "Episode 46/2000, Total Reward: 19.0\n",
      "Episode 47/2000, Total Reward: 19.0\n",
      "Episode 48/2000, Total Reward: 21.0\n",
      "Episode 49/2000, Total Reward: 32.0\n",
      "Episode 50/2000, Total Reward: 17.0\n",
      "Episode 51/2000, Total Reward: 12.0\n",
      "Episode 52/2000, Total Reward: 28.0\n",
      "Episode 53/2000, Total Reward: 16.0\n",
      "Episode 54/2000, Total Reward: 69.0\n",
      "Episode 55/2000, Total Reward: 10.0\n",
      "Episode 56/2000, Total Reward: 37.0\n",
      "Episode 57/2000, Total Reward: 12.0\n",
      "Episode 58/2000, Total Reward: 12.0\n",
      "Episode 59/2000, Total Reward: 53.0\n",
      "Episode 60/2000, Total Reward: 15.0\n",
      "Episode 61/2000, Total Reward: 24.0\n",
      "Episode 62/2000, Total Reward: 17.0\n",
      "Episode 63/2000, Total Reward: 10.0\n",
      "Episode 64/2000, Total Reward: 14.0\n",
      "Episode 65/2000, Total Reward: 15.0\n",
      "Episode 66/2000, Total Reward: 31.0\n",
      "Episode 67/2000, Total Reward: 13.0\n",
      "Episode 68/2000, Total Reward: 56.0\n",
      "Episode 69/2000, Total Reward: 17.0\n",
      "Episode 70/2000, Total Reward: 11.0\n",
      "Episode 71/2000, Total Reward: 13.0\n",
      "Episode 72/2000, Total Reward: 10.0\n",
      "Episode 73/2000, Total Reward: 24.0\n",
      "Episode 74/2000, Total Reward: 27.0\n",
      "Episode 75/2000, Total Reward: 49.0\n",
      "Episode 76/2000, Total Reward: 19.0\n",
      "Episode 77/2000, Total Reward: 13.0\n",
      "Episode 78/2000, Total Reward: 51.0\n",
      "Episode 79/2000, Total Reward: 37.0\n",
      "Episode 80/2000, Total Reward: 28.0\n",
      "Episode 81/2000, Total Reward: 35.0\n",
      "Episode 82/2000, Total Reward: 16.0\n",
      "Episode 83/2000, Total Reward: 73.0\n",
      "Episode 84/2000, Total Reward: 16.0\n",
      "Episode 85/2000, Total Reward: 27.0\n",
      "Episode 86/2000, Total Reward: 22.0\n",
      "Episode 87/2000, Total Reward: 34.0\n",
      "Episode 88/2000, Total Reward: 35.0\n",
      "Episode 89/2000, Total Reward: 34.0\n",
      "Episode 90/2000, Total Reward: 17.0\n",
      "Episode 91/2000, Total Reward: 17.0\n",
      "Episode 92/2000, Total Reward: 38.0\n",
      "Episode 93/2000, Total Reward: 47.0\n",
      "Episode 94/2000, Total Reward: 43.0\n",
      "Episode 95/2000, Total Reward: 46.0\n",
      "Episode 96/2000, Total Reward: 31.0\n",
      "Episode 97/2000, Total Reward: 27.0\n",
      "Episode 98/2000, Total Reward: 18.0\n",
      "Episode 99/2000, Total Reward: 16.0\n",
      "Episode 100/2000, Total Reward: 43.0\n",
      "Episode 101/2000, Total Reward: 19.0\n",
      "Episode 102/2000, Total Reward: 22.0\n",
      "Episode 103/2000, Total Reward: 44.0\n",
      "Episode 104/2000, Total Reward: 75.0\n",
      "Episode 105/2000, Total Reward: 95.0\n",
      "Episode 106/2000, Total Reward: 45.0\n",
      "Episode 107/2000, Total Reward: 51.0\n",
      "Episode 108/2000, Total Reward: 35.0\n",
      "Episode 109/2000, Total Reward: 73.0\n",
      "Episode 110/2000, Total Reward: 46.0\n",
      "Episode 111/2000, Total Reward: 44.0\n",
      "Episode 112/2000, Total Reward: 34.0\n",
      "Episode 113/2000, Total Reward: 30.0\n",
      "Episode 114/2000, Total Reward: 24.0\n",
      "Episode 115/2000, Total Reward: 20.0\n",
      "Episode 116/2000, Total Reward: 39.0\n",
      "Episode 117/2000, Total Reward: 44.0\n",
      "Episode 118/2000, Total Reward: 79.0\n",
      "Episode 119/2000, Total Reward: 63.0\n",
      "Episode 120/2000, Total Reward: 26.0\n",
      "Episode 121/2000, Total Reward: 23.0\n",
      "Episode 122/2000, Total Reward: 25.0\n",
      "Episode 123/2000, Total Reward: 20.0\n",
      "Episode 124/2000, Total Reward: 20.0\n",
      "Episode 125/2000, Total Reward: 25.0\n",
      "Episode 126/2000, Total Reward: 40.0\n",
      "Episode 127/2000, Total Reward: 41.0\n",
      "Episode 128/2000, Total Reward: 55.0\n",
      "Episode 129/2000, Total Reward: 52.0\n",
      "Episode 130/2000, Total Reward: 49.0\n",
      "Episode 131/2000, Total Reward: 46.0\n",
      "Episode 132/2000, Total Reward: 127.0\n",
      "Episode 133/2000, Total Reward: 50.0\n",
      "Episode 134/2000, Total Reward: 30.0\n",
      "Episode 135/2000, Total Reward: 24.0\n",
      "Episode 136/2000, Total Reward: 35.0\n",
      "Episode 137/2000, Total Reward: 39.0\n",
      "Episode 138/2000, Total Reward: 34.0\n",
      "Episode 139/2000, Total Reward: 50.0\n",
      "Episode 140/2000, Total Reward: 36.0\n",
      "Episode 141/2000, Total Reward: 30.0\n",
      "Episode 142/2000, Total Reward: 73.0\n",
      "Episode 143/2000, Total Reward: 41.0\n",
      "Episode 144/2000, Total Reward: 63.0\n",
      "Episode 145/2000, Total Reward: 30.0\n",
      "Episode 146/2000, Total Reward: 38.0\n",
      "Episode 147/2000, Total Reward: 42.0\n",
      "Episode 148/2000, Total Reward: 52.0\n",
      "Episode 149/2000, Total Reward: 65.0\n",
      "Episode 150/2000, Total Reward: 78.0\n",
      "Episode 151/2000, Total Reward: 74.0\n",
      "Episode 152/2000, Total Reward: 43.0\n",
      "Episode 153/2000, Total Reward: 40.0\n",
      "Episode 154/2000, Total Reward: 36.0\n",
      "Episode 155/2000, Total Reward: 35.0\n",
      "Episode 156/2000, Total Reward: 84.0\n",
      "Episode 157/2000, Total Reward: 40.0\n",
      "Episode 158/2000, Total Reward: 91.0\n",
      "Episode 159/2000, Total Reward: 64.0\n",
      "Episode 160/2000, Total Reward: 54.0\n",
      "Episode 161/2000, Total Reward: 58.0\n",
      "Episode 162/2000, Total Reward: 132.0\n",
      "Episode 163/2000, Total Reward: 73.0\n",
      "Episode 164/2000, Total Reward: 104.0\n",
      "Episode 165/2000, Total Reward: 78.0\n",
      "Episode 166/2000, Total Reward: 118.0\n",
      "Episode 167/2000, Total Reward: 137.0\n",
      "Episode 168/2000, Total Reward: 55.0\n",
      "Episode 169/2000, Total Reward: 149.0\n",
      "Episode 170/2000, Total Reward: 153.0\n",
      "Episode 171/2000, Total Reward: 196.0\n",
      "Episode 172/2000, Total Reward: 56.0\n",
      "Episode 173/2000, Total Reward: 85.0\n",
      "Episode 174/2000, Total Reward: 126.0\n",
      "Episode 175/2000, Total Reward: 125.0\n",
      "Episode 176/2000, Total Reward: 40.0\n",
      "Episode 177/2000, Total Reward: 49.0\n",
      "Episode 178/2000, Total Reward: 29.0\n",
      "Episode 179/2000, Total Reward: 30.0\n",
      "Episode 180/2000, Total Reward: 46.0\n",
      "Episode 181/2000, Total Reward: 42.0\n",
      "Episode 182/2000, Total Reward: 52.0\n",
      "Episode 183/2000, Total Reward: 77.0\n",
      "Episode 184/2000, Total Reward: 125.0\n",
      "Episode 185/2000, Total Reward: 128.0\n",
      "Episode 186/2000, Total Reward: 72.0\n",
      "Episode 187/2000, Total Reward: 143.0\n",
      "Episode 188/2000, Total Reward: 120.0\n",
      "Episode 189/2000, Total Reward: 168.0\n",
      "Episode 190/2000, Total Reward: 97.0\n",
      "Episode 191/2000, Total Reward: 92.0\n",
      "Episode 192/2000, Total Reward: 136.0\n",
      "Episode 193/2000, Total Reward: 139.0\n",
      "Episode 194/2000, Total Reward: 105.0\n",
      "Episode 195/2000, Total Reward: 100.0\n",
      "Episode 196/2000, Total Reward: 111.0\n",
      "Episode 197/2000, Total Reward: 103.0\n",
      "Episode 198/2000, Total Reward: 253.0\n",
      "Episode 199/2000, Total Reward: 116.0\n",
      "Episode 200/2000, Total Reward: 104.0\n",
      "Episode 201/2000, Total Reward: 123.0\n",
      "Episode 202/2000, Total Reward: 153.0\n",
      "Episode 203/2000, Total Reward: 109.0\n",
      "Episode 204/2000, Total Reward: 146.0\n",
      "Episode 205/2000, Total Reward: 78.0\n",
      "Episode 206/2000, Total Reward: 75.0\n",
      "Episode 207/2000, Total Reward: 127.0\n",
      "Episode 208/2000, Total Reward: 140.0\n",
      "Episode 209/2000, Total Reward: 62.0\n",
      "Episode 210/2000, Total Reward: 171.0\n",
      "Episode 211/2000, Total Reward: 176.0\n",
      "Episode 212/2000, Total Reward: 125.0\n",
      "Episode 213/2000, Total Reward: 186.0\n",
      "Episode 214/2000, Total Reward: 150.0\n",
      "Episode 215/2000, Total Reward: 173.0\n",
      "Episode 216/2000, Total Reward: 151.0\n",
      "Episode 217/2000, Total Reward: 131.0\n",
      "Episode 218/2000, Total Reward: 75.0\n",
      "Episode 219/2000, Total Reward: 95.0\n",
      "Episode 220/2000, Total Reward: 112.0\n",
      "Episode 221/2000, Total Reward: 64.0\n",
      "Episode 222/2000, Total Reward: 117.0\n",
      "Episode 223/2000, Total Reward: 123.0\n",
      "Episode 224/2000, Total Reward: 139.0\n",
      "Episode 225/2000, Total Reward: 67.0\n",
      "Episode 226/2000, Total Reward: 60.0\n",
      "Episode 227/2000, Total Reward: 39.0\n",
      "Episode 228/2000, Total Reward: 38.0\n",
      "Episode 229/2000, Total Reward: 41.0\n",
      "Episode 230/2000, Total Reward: 78.0\n",
      "Episode 231/2000, Total Reward: 140.0\n",
      "Episode 232/2000, Total Reward: 159.0\n",
      "Episode 233/2000, Total Reward: 93.0\n",
      "Episode 234/2000, Total Reward: 171.0\n",
      "Episode 235/2000, Total Reward: 173.0\n",
      "Episode 236/2000, Total Reward: 138.0\n",
      "Episode 237/2000, Total Reward: 142.0\n",
      "Episode 238/2000, Total Reward: 163.0\n",
      "Episode 239/2000, Total Reward: 150.0\n",
      "Episode 240/2000, Total Reward: 86.0\n",
      "Episode 241/2000, Total Reward: 61.0\n",
      "Episode 242/2000, Total Reward: 107.0\n",
      "Episode 243/2000, Total Reward: 141.0\n",
      "Episode 244/2000, Total Reward: 155.0\n",
      "Episode 245/2000, Total Reward: 78.0\n",
      "Episode 246/2000, Total Reward: 91.0\n",
      "Episode 247/2000, Total Reward: 115.0\n",
      "Episode 248/2000, Total Reward: 142.0\n",
      "Episode 249/2000, Total Reward: 126.0\n",
      "Episode 250/2000, Total Reward: 143.0\n",
      "Episode 251/2000, Total Reward: 80.0\n",
      "Episode 252/2000, Total Reward: 86.0\n",
      "Episode 253/2000, Total Reward: 127.0\n",
      "Episode 254/2000, Total Reward: 131.0\n",
      "Episode 255/2000, Total Reward: 147.0\n",
      "Episode 256/2000, Total Reward: 180.0\n",
      "Episode 257/2000, Total Reward: 210.0\n",
      "Episode 258/2000, Total Reward: 333.0\n",
      "Episode 259/2000, Total Reward: 119.0\n",
      "Episode 260/2000, Total Reward: 99.0\n",
      "Episode 261/2000, Total Reward: 86.0\n",
      "Episode 262/2000, Total Reward: 72.0\n",
      "Episode 263/2000, Total Reward: 96.0\n",
      "Episode 264/2000, Total Reward: 70.0\n",
      "Episode 265/2000, Total Reward: 59.0\n",
      "Episode 266/2000, Total Reward: 77.0\n",
      "Episode 267/2000, Total Reward: 65.0\n",
      "Episode 268/2000, Total Reward: 56.0\n",
      "Episode 269/2000, Total Reward: 51.0\n",
      "Episode 270/2000, Total Reward: 50.0\n",
      "Episode 271/2000, Total Reward: 73.0\n",
      "Episode 272/2000, Total Reward: 75.0\n",
      "Episode 273/2000, Total Reward: 60.0\n",
      "Episode 274/2000, Total Reward: 86.0\n",
      "Episode 275/2000, Total Reward: 54.0\n",
      "Episode 276/2000, Total Reward: 58.0\n",
      "Episode 277/2000, Total Reward: 69.0\n",
      "Episode 278/2000, Total Reward: 37.0\n",
      "Episode 279/2000, Total Reward: 64.0\n",
      "Episode 280/2000, Total Reward: 40.0\n",
      "Episode 281/2000, Total Reward: 38.0\n",
      "Episode 282/2000, Total Reward: 67.0\n",
      "Episode 283/2000, Total Reward: 51.0\n",
      "Episode 284/2000, Total Reward: 84.0\n",
      "Episode 285/2000, Total Reward: 51.0\n",
      "Episode 286/2000, Total Reward: 55.0\n",
      "Episode 287/2000, Total Reward: 67.0\n",
      "Episode 288/2000, Total Reward: 49.0\n",
      "Episode 289/2000, Total Reward: 65.0\n",
      "Episode 290/2000, Total Reward: 77.0\n",
      "Episode 291/2000, Total Reward: 100.0\n",
      "Episode 292/2000, Total Reward: 70.0\n",
      "Episode 293/2000, Total Reward: 100.0\n",
      "Episode 294/2000, Total Reward: 82.0\n",
      "Episode 295/2000, Total Reward: 138.0\n",
      "Episode 296/2000, Total Reward: 88.0\n",
      "Episode 297/2000, Total Reward: 64.0\n",
      "Episode 298/2000, Total Reward: 80.0\n",
      "Episode 299/2000, Total Reward: 72.0\n",
      "Episode 300/2000, Total Reward: 72.0\n",
      "Episode 301/2000, Total Reward: 86.0\n",
      "Episode 302/2000, Total Reward: 53.0\n",
      "Episode 303/2000, Total Reward: 66.0\n",
      "Episode 304/2000, Total Reward: 40.0\n",
      "Episode 305/2000, Total Reward: 43.0\n",
      "Episode 306/2000, Total Reward: 53.0\n",
      "Episode 307/2000, Total Reward: 50.0\n",
      "Episode 308/2000, Total Reward: 52.0\n",
      "Episode 309/2000, Total Reward: 50.0\n",
      "Episode 310/2000, Total Reward: 40.0\n",
      "Episode 311/2000, Total Reward: 75.0\n",
      "Episode 312/2000, Total Reward: 77.0\n",
      "Episode 313/2000, Total Reward: 74.0\n",
      "Episode 314/2000, Total Reward: 100.0\n",
      "Episode 315/2000, Total Reward: 86.0\n",
      "Episode 316/2000, Total Reward: 110.0\n",
      "Episode 317/2000, Total Reward: 132.0\n",
      "Episode 318/2000, Total Reward: 59.0\n",
      "Episode 319/2000, Total Reward: 60.0\n",
      "Episode 320/2000, Total Reward: 38.0\n",
      "Episode 321/2000, Total Reward: 59.0\n",
      "Episode 322/2000, Total Reward: 53.0\n",
      "Episode 323/2000, Total Reward: 81.0\n",
      "Episode 324/2000, Total Reward: 75.0\n",
      "Episode 325/2000, Total Reward: 215.0\n",
      "Episode 326/2000, Total Reward: 59.0\n",
      "Episode 327/2000, Total Reward: 70.0\n",
      "Episode 328/2000, Total Reward: 151.0\n",
      "Episode 329/2000, Total Reward: 71.0\n",
      "Episode 330/2000, Total Reward: 118.0\n",
      "Episode 331/2000, Total Reward: 86.0\n",
      "Episode 332/2000, Total Reward: 207.0\n",
      "Episode 333/2000, Total Reward: 99.0\n",
      "Episode 334/2000, Total Reward: 95.0\n",
      "Episode 335/2000, Total Reward: 77.0\n",
      "Episode 336/2000, Total Reward: 92.0\n",
      "Episode 337/2000, Total Reward: 234.0\n",
      "Episode 338/2000, Total Reward: 352.0\n",
      "Episode 339/2000, Total Reward: 118.0\n",
      "Episode 340/2000, Total Reward: 165.0\n",
      "Episode 341/2000, Total Reward: 134.0\n",
      "Episode 342/2000, Total Reward: 67.0\n",
      "Episode 343/2000, Total Reward: 88.0\n",
      "Episode 344/2000, Total Reward: 69.0\n",
      "Episode 345/2000, Total Reward: 139.0\n",
      "Episode 346/2000, Total Reward: 107.0\n",
      "Episode 347/2000, Total Reward: 83.0\n",
      "Episode 348/2000, Total Reward: 132.0\n",
      "Episode 349/2000, Total Reward: 357.0\n",
      "Episode 350/2000, Total Reward: 268.0\n",
      "Episode 351/2000, Total Reward: 500.0\n",
      "Episode 352/2000, Total Reward: 436.0\n",
      "Episode 353/2000, Total Reward: 337.0\n",
      "Episode 354/2000, Total Reward: 500.0\n",
      "Episode 355/2000, Total Reward: 500.0\n",
      "Episode 356/2000, Total Reward: 500.0\n",
      "Episode 357/2000, Total Reward: 500.0\n",
      "Episode 358/2000, Total Reward: 500.0\n",
      "Episode 359/2000, Total Reward: 400.0\n",
      "Episode 360/2000, Total Reward: 490.0\n",
      "Episode 361/2000, Total Reward: 500.0\n",
      "Episode 362/2000, Total Reward: 364.0\n",
      "Episode 363/2000, Total Reward: 262.0\n",
      "Episode 364/2000, Total Reward: 459.0\n",
      "Episode 365/2000, Total Reward: 342.0\n",
      "Episode 366/2000, Total Reward: 296.0\n",
      "Episode 367/2000, Total Reward: 339.0\n",
      "Episode 368/2000, Total Reward: 258.0\n",
      "Episode 369/2000, Total Reward: 185.0\n",
      "Episode 370/2000, Total Reward: 181.0\n",
      "Episode 371/2000, Total Reward: 206.0\n",
      "Episode 372/2000, Total Reward: 500.0\n",
      "Episode 373/2000, Total Reward: 206.0\n",
      "Episode 374/2000, Total Reward: 410.0\n",
      "Episode 375/2000, Total Reward: 240.0\n",
      "Episode 376/2000, Total Reward: 298.0\n",
      "Episode 377/2000, Total Reward: 312.0\n",
      "Episode 378/2000, Total Reward: 308.0\n",
      "Episode 379/2000, Total Reward: 293.0\n",
      "Episode 380/2000, Total Reward: 276.0\n",
      "Episode 381/2000, Total Reward: 214.0\n",
      "Episode 382/2000, Total Reward: 189.0\n",
      "Episode 383/2000, Total Reward: 180.0\n",
      "Episode 384/2000, Total Reward: 167.0\n",
      "Episode 385/2000, Total Reward: 202.0\n",
      "Episode 386/2000, Total Reward: 237.0\n",
      "Episode 387/2000, Total Reward: 425.0\n",
      "Episode 388/2000, Total Reward: 376.0\n",
      "Episode 389/2000, Total Reward: 316.0\n",
      "Episode 390/2000, Total Reward: 500.0\n",
      "Episode 391/2000, Total Reward: 383.0\n",
      "Episode 392/2000, Total Reward: 227.0\n",
      "Episode 393/2000, Total Reward: 437.0\n",
      "Episode 394/2000, Total Reward: 243.0\n",
      "Episode 395/2000, Total Reward: 251.0\n",
      "Episode 396/2000, Total Reward: 223.0\n",
      "Episode 397/2000, Total Reward: 199.0\n",
      "Episode 398/2000, Total Reward: 273.0\n",
      "Episode 399/2000, Total Reward: 280.0\n",
      "Episode 400/2000, Total Reward: 268.0\n",
      "Episode 401/2000, Total Reward: 275.0\n",
      "Episode 402/2000, Total Reward: 301.0\n",
      "Episode 403/2000, Total Reward: 459.0\n",
      "Episode 404/2000, Total Reward: 386.0\n",
      "Episode 405/2000, Total Reward: 450.0\n",
      "Episode 406/2000, Total Reward: 500.0\n",
      "Episode 407/2000, Total Reward: 500.0\n",
      "Episode 408/2000, Total Reward: 12.0\n",
      "Episode 409/2000, Total Reward: 10.0\n",
      "Episode 410/2000, Total Reward: 85.0\n",
      "Episode 411/2000, Total Reward: 10.0\n",
      "Episode 412/2000, Total Reward: 10.0\n",
      "Episode 413/2000, Total Reward: 9.0\n",
      "Episode 414/2000, Total Reward: 9.0\n",
      "Episode 415/2000, Total Reward: 10.0\n",
      "Episode 416/2000, Total Reward: 10.0\n",
      "Episode 417/2000, Total Reward: 9.0\n",
      "Episode 418/2000, Total Reward: 8.0\n",
      "Episode 419/2000, Total Reward: 10.0\n",
      "Episode 420/2000, Total Reward: 10.0\n",
      "Episode 421/2000, Total Reward: 9.0\n",
      "Episode 422/2000, Total Reward: 8.0\n",
      "Episode 423/2000, Total Reward: 8.0\n",
      "Episode 424/2000, Total Reward: 9.0\n",
      "Episode 425/2000, Total Reward: 10.0\n",
      "Episode 426/2000, Total Reward: 10.0\n",
      "Episode 427/2000, Total Reward: 9.0\n",
      "Episode 428/2000, Total Reward: 10.0\n",
      "Episode 429/2000, Total Reward: 9.0\n",
      "Episode 430/2000, Total Reward: 10.0\n",
      "Episode 431/2000, Total Reward: 10.0\n",
      "Episode 432/2000, Total Reward: 10.0\n",
      "Episode 433/2000, Total Reward: 10.0\n",
      "Episode 434/2000, Total Reward: 10.0\n",
      "Episode 435/2000, Total Reward: 8.0\n",
      "Episode 436/2000, Total Reward: 8.0\n",
      "Episode 437/2000, Total Reward: 10.0\n",
      "Episode 438/2000, Total Reward: 10.0\n",
      "Episode 439/2000, Total Reward: 10.0\n",
      "Episode 440/2000, Total Reward: 9.0\n",
      "Episode 441/2000, Total Reward: 10.0\n",
      "Episode 442/2000, Total Reward: 10.0\n",
      "Episode 443/2000, Total Reward: 8.0\n",
      "Episode 444/2000, Total Reward: 10.0\n",
      "Episode 445/2000, Total Reward: 9.0\n",
      "Episode 446/2000, Total Reward: 9.0\n",
      "Episode 447/2000, Total Reward: 10.0\n",
      "Episode 448/2000, Total Reward: 10.0\n",
      "Episode 449/2000, Total Reward: 9.0\n",
      "Episode 450/2000, Total Reward: 10.0\n",
      "Episode 451/2000, Total Reward: 10.0\n",
      "Episode 452/2000, Total Reward: 10.0\n",
      "Episode 453/2000, Total Reward: 9.0\n",
      "Episode 454/2000, Total Reward: 9.0\n",
      "Episode 455/2000, Total Reward: 9.0\n",
      "Episode 456/2000, Total Reward: 10.0\n",
      "Episode 457/2000, Total Reward: 9.0\n",
      "Episode 458/2000, Total Reward: 8.0\n",
      "Episode 459/2000, Total Reward: 10.0\n",
      "Episode 460/2000, Total Reward: 9.0\n",
      "Episode 461/2000, Total Reward: 9.0\n",
      "Episode 462/2000, Total Reward: 8.0\n",
      "Episode 463/2000, Total Reward: 10.0\n",
      "Episode 464/2000, Total Reward: 9.0\n",
      "Episode 465/2000, Total Reward: 9.0\n",
      "Episode 466/2000, Total Reward: 9.0\n",
      "Episode 467/2000, Total Reward: 9.0\n",
      "Episode 468/2000, Total Reward: 10.0\n",
      "Episode 469/2000, Total Reward: 8.0\n",
      "Episode 470/2000, Total Reward: 8.0\n",
      "Episode 471/2000, Total Reward: 9.0\n",
      "Episode 472/2000, Total Reward: 10.0\n",
      "Episode 473/2000, Total Reward: 11.0\n",
      "Episode 474/2000, Total Reward: 9.0\n",
      "Episode 475/2000, Total Reward: 10.0\n",
      "Episode 476/2000, Total Reward: 11.0\n",
      "Episode 477/2000, Total Reward: 8.0\n",
      "Episode 478/2000, Total Reward: 10.0\n",
      "Episode 479/2000, Total Reward: 9.0\n",
      "Episode 480/2000, Total Reward: 10.0\n",
      "Episode 481/2000, Total Reward: 10.0\n",
      "Episode 482/2000, Total Reward: 10.0\n",
      "Episode 483/2000, Total Reward: 11.0\n",
      "Episode 484/2000, Total Reward: 10.0\n",
      "Episode 485/2000, Total Reward: 8.0\n",
      "Episode 486/2000, Total Reward: 9.0\n",
      "Episode 487/2000, Total Reward: 9.0\n",
      "Episode 488/2000, Total Reward: 8.0\n",
      "Episode 489/2000, Total Reward: 8.0\n",
      "Episode 490/2000, Total Reward: 10.0\n",
      "Episode 491/2000, Total Reward: 9.0\n",
      "Episode 492/2000, Total Reward: 10.0\n",
      "Episode 493/2000, Total Reward: 10.0\n",
      "Episode 494/2000, Total Reward: 10.0\n",
      "Episode 495/2000, Total Reward: 9.0\n",
      "Episode 496/2000, Total Reward: 10.0\n",
      "Episode 497/2000, Total Reward: 9.0\n",
      "Episode 498/2000, Total Reward: 8.0\n",
      "Episode 499/2000, Total Reward: 9.0\n",
      "Episode 500/2000, Total Reward: 9.0\n",
      "Episode 501/2000, Total Reward: 8.0\n",
      "Episode 502/2000, Total Reward: 9.0\n",
      "Episode 503/2000, Total Reward: 9.0\n",
      "Episode 504/2000, Total Reward: 10.0\n",
      "Episode 505/2000, Total Reward: 8.0\n",
      "Episode 506/2000, Total Reward: 10.0\n",
      "Episode 507/2000, Total Reward: 10.0\n",
      "Episode 508/2000, Total Reward: 9.0\n",
      "Episode 509/2000, Total Reward: 10.0\n",
      "Episode 510/2000, Total Reward: 8.0\n",
      "Episode 511/2000, Total Reward: 9.0\n",
      "Episode 512/2000, Total Reward: 9.0\n",
      "Episode 513/2000, Total Reward: 9.0\n",
      "Episode 514/2000, Total Reward: 8.0\n",
      "Episode 515/2000, Total Reward: 10.0\n",
      "Episode 516/2000, Total Reward: 10.0\n",
      "Episode 517/2000, Total Reward: 9.0\n",
      "Episode 518/2000, Total Reward: 9.0\n",
      "Episode 519/2000, Total Reward: 9.0\n",
      "Episode 520/2000, Total Reward: 10.0\n",
      "Episode 521/2000, Total Reward: 9.0\n",
      "Episode 522/2000, Total Reward: 9.0\n",
      "Episode 523/2000, Total Reward: 8.0\n",
      "Episode 524/2000, Total Reward: 11.0\n",
      "Episode 525/2000, Total Reward: 11.0\n",
      "Episode 526/2000, Total Reward: 11.0\n",
      "Episode 527/2000, Total Reward: 10.0\n",
      "Episode 528/2000, Total Reward: 10.0\n",
      "Episode 529/2000, Total Reward: 10.0\n",
      "Episode 530/2000, Total Reward: 9.0\n",
      "Episode 531/2000, Total Reward: 10.0\n",
      "Episode 532/2000, Total Reward: 10.0\n",
      "Episode 533/2000, Total Reward: 10.0\n",
      "Episode 534/2000, Total Reward: 9.0\n",
      "Episode 535/2000, Total Reward: 10.0\n",
      "Episode 536/2000, Total Reward: 10.0\n",
      "Episode 537/2000, Total Reward: 9.0\n",
      "Episode 538/2000, Total Reward: 8.0\n",
      "Episode 539/2000, Total Reward: 9.0\n",
      "Episode 540/2000, Total Reward: 9.0\n",
      "Episode 541/2000, Total Reward: 9.0\n",
      "Episode 542/2000, Total Reward: 10.0\n",
      "Episode 543/2000, Total Reward: 10.0\n",
      "Episode 544/2000, Total Reward: 10.0\n",
      "Episode 545/2000, Total Reward: 9.0\n",
      "Episode 546/2000, Total Reward: 9.0\n",
      "Episode 547/2000, Total Reward: 10.0\n",
      "Episode 548/2000, Total Reward: 10.0\n",
      "Episode 549/2000, Total Reward: 11.0\n",
      "Episode 550/2000, Total Reward: 10.0\n",
      "Episode 551/2000, Total Reward: 9.0\n",
      "Episode 552/2000, Total Reward: 8.0\n",
      "Episode 553/2000, Total Reward: 9.0\n",
      "Episode 554/2000, Total Reward: 10.0\n",
      "Episode 555/2000, Total Reward: 8.0\n",
      "Episode 556/2000, Total Reward: 9.0\n",
      "Episode 557/2000, Total Reward: 10.0\n",
      "Episode 558/2000, Total Reward: 10.0\n",
      "Episode 559/2000, Total Reward: 10.0\n",
      "Episode 560/2000, Total Reward: 9.0\n",
      "Episode 561/2000, Total Reward: 9.0\n",
      "Episode 562/2000, Total Reward: 10.0\n",
      "Episode 563/2000, Total Reward: 10.0\n",
      "Episode 564/2000, Total Reward: 8.0\n",
      "Episode 565/2000, Total Reward: 10.0\n",
      "Episode 566/2000, Total Reward: 9.0\n",
      "Episode 567/2000, Total Reward: 9.0\n",
      "Episode 568/2000, Total Reward: 9.0\n",
      "Episode 569/2000, Total Reward: 10.0\n",
      "Episode 570/2000, Total Reward: 8.0\n",
      "Episode 571/2000, Total Reward: 10.0\n",
      "Episode 572/2000, Total Reward: 9.0\n",
      "Episode 573/2000, Total Reward: 8.0\n",
      "Episode 574/2000, Total Reward: 8.0\n",
      "Episode 575/2000, Total Reward: 9.0\n",
      "Episode 576/2000, Total Reward: 10.0\n",
      "Episode 577/2000, Total Reward: 9.0\n",
      "Episode 578/2000, Total Reward: 8.0\n",
      "Episode 579/2000, Total Reward: 10.0\n",
      "Episode 580/2000, Total Reward: 9.0\n",
      "Episode 581/2000, Total Reward: 9.0\n",
      "Episode 582/2000, Total Reward: 9.0\n",
      "Episode 583/2000, Total Reward: 9.0\n",
      "Episode 584/2000, Total Reward: 9.0\n",
      "Episode 585/2000, Total Reward: 8.0\n",
      "Episode 586/2000, Total Reward: 8.0\n",
      "Episode 587/2000, Total Reward: 10.0\n",
      "Episode 588/2000, Total Reward: 10.0\n",
      "Episode 589/2000, Total Reward: 9.0\n",
      "Episode 590/2000, Total Reward: 10.0\n",
      "Episode 591/2000, Total Reward: 10.0\n",
      "Episode 592/2000, Total Reward: 10.0\n",
      "Episode 593/2000, Total Reward: 10.0\n",
      "Episode 594/2000, Total Reward: 11.0\n",
      "Episode 595/2000, Total Reward: 8.0\n",
      "Episode 596/2000, Total Reward: 10.0\n",
      "Episode 597/2000, Total Reward: 9.0\n",
      "Episode 598/2000, Total Reward: 9.0\n",
      "Episode 599/2000, Total Reward: 9.0\n",
      "Episode 600/2000, Total Reward: 8.0\n",
      "Episode 601/2000, Total Reward: 9.0\n",
      "Episode 602/2000, Total Reward: 9.0\n",
      "Episode 603/2000, Total Reward: 10.0\n",
      "Episode 604/2000, Total Reward: 9.0\n",
      "Episode 605/2000, Total Reward: 9.0\n",
      "Episode 606/2000, Total Reward: 9.0\n",
      "Episode 607/2000, Total Reward: 10.0\n",
      "Episode 608/2000, Total Reward: 10.0\n",
      "Episode 609/2000, Total Reward: 10.0\n",
      "Episode 610/2000, Total Reward: 10.0\n",
      "Episode 611/2000, Total Reward: 10.0\n",
      "Episode 612/2000, Total Reward: 10.0\n",
      "Episode 613/2000, Total Reward: 10.0\n",
      "Episode 614/2000, Total Reward: 9.0\n",
      "Episode 615/2000, Total Reward: 8.0\n",
      "Episode 616/2000, Total Reward: 9.0\n",
      "Episode 617/2000, Total Reward: 9.0\n",
      "Episode 618/2000, Total Reward: 9.0\n",
      "Episode 619/2000, Total Reward: 9.0\n",
      "Episode 620/2000, Total Reward: 10.0\n",
      "Episode 621/2000, Total Reward: 8.0\n",
      "Episode 622/2000, Total Reward: 10.0\n",
      "Episode 623/2000, Total Reward: 9.0\n",
      "Episode 624/2000, Total Reward: 9.0\n",
      "Episode 625/2000, Total Reward: 10.0\n",
      "Episode 626/2000, Total Reward: 10.0\n",
      "Episode 627/2000, Total Reward: 10.0\n",
      "Episode 628/2000, Total Reward: 8.0\n",
      "Episode 629/2000, Total Reward: 10.0\n",
      "Episode 630/2000, Total Reward: 9.0\n",
      "Episode 631/2000, Total Reward: 10.0\n",
      "Episode 632/2000, Total Reward: 9.0\n",
      "Episode 633/2000, Total Reward: 10.0\n",
      "Episode 634/2000, Total Reward: 10.0\n",
      "Episode 635/2000, Total Reward: 9.0\n",
      "Episode 636/2000, Total Reward: 9.0\n",
      "Episode 637/2000, Total Reward: 10.0\n",
      "Episode 638/2000, Total Reward: 9.0\n",
      "Episode 639/2000, Total Reward: 10.0\n",
      "Episode 640/2000, Total Reward: 10.0\n",
      "Episode 641/2000, Total Reward: 9.0\n",
      "Episode 642/2000, Total Reward: 10.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[51], line 12\u001B[0m\n\u001B[1;32m      9\u001B[0m alpha_theta \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.001\u001B[39m\n\u001B[1;32m     10\u001B[0m alpha_w \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.001\u001B[39m\n\u001B[0;32m---> 12\u001B[0m policy_network, value_network, rewards \u001B[38;5;241m=\u001B[39m actor_critic(env,\n\u001B[1;32m     13\u001B[0m                                        input_dim\u001B[38;5;241m=\u001B[39minput_dim,\n\u001B[1;32m     14\u001B[0m                                        output_dim\u001B[38;5;241m=\u001B[39moutput_dim,\n\u001B[1;32m     15\u001B[0m                                        hidden_sizes\u001B[38;5;241m=\u001B[39mhidden_sizes,\n\u001B[1;32m     16\u001B[0m                                        alpha_theta\u001B[38;5;241m=\u001B[39malpha_theta,\n\u001B[1;32m     17\u001B[0m                                        alpha_w\u001B[38;5;241m=\u001B[39malpha_w,\n\u001B[1;32m     18\u001B[0m                                        episodes\u001B[38;5;241m=\u001B[39mepisodes,\n\u001B[1;32m     19\u001B[0m                                        gamma\u001B[38;5;241m=\u001B[39mgamma)\n\u001B[1;32m     21\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure()\n\u001B[1;32m     22\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(rewards)\n",
      "Cell \u001B[0;32mIn[49], line 76\u001B[0m, in \u001B[0;36mactor_critic\u001B[0;34m(env, input_dim, output_dim, hidden_sizes, alpha_theta, alpha_w, episodes, gamma, log_dir)\u001B[0m\n\u001B[1;32m     74\u001B[0m value_optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     75\u001B[0m value_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 76\u001B[0m value_optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     78\u001B[0m \u001B[38;5;66;03m# Update policy network\u001B[39;00m\n\u001B[1;32m     79\u001B[0m policy_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mI \u001B[38;5;241m*\u001B[39m td_error\u001B[38;5;241m.\u001B[39mdetach() \u001B[38;5;241m*\u001B[39m log_prob\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/optimizer.py:487\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    482\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    483\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    484\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    485\u001B[0m             )\n\u001B[0;32m--> 487\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    488\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    490\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/optimizer.py:91\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     89\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     90\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 91\u001B[0m     ret \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     93\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/adam.py:223\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    211\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    213\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    214\u001B[0m         group,\n\u001B[1;32m    215\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    220\u001B[0m         state_steps,\n\u001B[1;32m    221\u001B[0m     )\n\u001B[0;32m--> 223\u001B[0m     adam(\n\u001B[1;32m    224\u001B[0m         params_with_grad,\n\u001B[1;32m    225\u001B[0m         grads,\n\u001B[1;32m    226\u001B[0m         exp_avgs,\n\u001B[1;32m    227\u001B[0m         exp_avg_sqs,\n\u001B[1;32m    228\u001B[0m         max_exp_avg_sqs,\n\u001B[1;32m    229\u001B[0m         state_steps,\n\u001B[1;32m    230\u001B[0m         amsgrad\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mamsgrad\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    231\u001B[0m         has_complex\u001B[38;5;241m=\u001B[39mhas_complex,\n\u001B[1;32m    232\u001B[0m         beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[1;32m    233\u001B[0m         beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[1;32m    234\u001B[0m         lr\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    235\u001B[0m         weight_decay\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweight_decay\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    236\u001B[0m         eps\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meps\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    237\u001B[0m         maximize\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    238\u001B[0m         foreach\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mforeach\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    239\u001B[0m         capturable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcapturable\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    240\u001B[0m         differentiable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    241\u001B[0m         fused\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfused\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    242\u001B[0m         grad_scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad_scale\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    243\u001B[0m         found_inf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    244\u001B[0m     )\n\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/optimizer.py:154\u001B[0m, in \u001B[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 154\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/adam.py:784\u001B[0m, in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    781\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    782\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[0;32m--> 784\u001B[0m func(\n\u001B[1;32m    785\u001B[0m     params,\n\u001B[1;32m    786\u001B[0m     grads,\n\u001B[1;32m    787\u001B[0m     exp_avgs,\n\u001B[1;32m    788\u001B[0m     exp_avg_sqs,\n\u001B[1;32m    789\u001B[0m     max_exp_avg_sqs,\n\u001B[1;32m    790\u001B[0m     state_steps,\n\u001B[1;32m    791\u001B[0m     amsgrad\u001B[38;5;241m=\u001B[39mamsgrad,\n\u001B[1;32m    792\u001B[0m     has_complex\u001B[38;5;241m=\u001B[39mhas_complex,\n\u001B[1;32m    793\u001B[0m     beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[1;32m    794\u001B[0m     beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[1;32m    795\u001B[0m     lr\u001B[38;5;241m=\u001B[39mlr,\n\u001B[1;32m    796\u001B[0m     weight_decay\u001B[38;5;241m=\u001B[39mweight_decay,\n\u001B[1;32m    797\u001B[0m     eps\u001B[38;5;241m=\u001B[39meps,\n\u001B[1;32m    798\u001B[0m     maximize\u001B[38;5;241m=\u001B[39mmaximize,\n\u001B[1;32m    799\u001B[0m     capturable\u001B[38;5;241m=\u001B[39mcapturable,\n\u001B[1;32m    800\u001B[0m     differentiable\u001B[38;5;241m=\u001B[39mdifferentiable,\n\u001B[1;32m    801\u001B[0m     grad_scale\u001B[38;5;241m=\u001B[39mgrad_scale,\n\u001B[1;32m    802\u001B[0m     found_inf\u001B[38;5;241m=\u001B[39mfound_inf,\n\u001B[1;32m    803\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/adam.py:379\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[1;32m    377\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[1;32m    378\u001B[0m exp_avg\u001B[38;5;241m.\u001B[39mlerp_(grad, \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta1)\n\u001B[0;32m--> 379\u001B[0m exp_avg_sq\u001B[38;5;241m.\u001B[39mmul_(beta2)\u001B[38;5;241m.\u001B[39maddcmul_(grad, grad\u001B[38;5;241m.\u001B[39mconj(), value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta2)\n\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m capturable \u001B[38;5;129;01mor\u001B[39;00m differentiable:\n\u001B[1;32m    382\u001B[0m     step \u001B[38;5;241m=\u001B[39m step_t\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "id": "9d5e7e325ae07ede",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T15:04:25.153660Z",
     "start_time": "2024-12-09T08:16:31.167825Z"
    }
   },
   "source": [
    "# test the trained policy\n",
    "print(\"Testing the policy...\")\n",
    "test_avg_reward = test_policy(env, policy_network, episodes=100)\n",
    "print(f\"Average Test Reward: {test_avg_reward}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the policy...\n",
      "Test Episode 1, Total Reward: 10000.0\n",
      "Test Episode 2, Total Reward: 10000.0\n",
      "Test Episode 3, Total Reward: 10000.0\n",
      "Test Episode 4, Total Reward: 10000.0\n",
      "Test Episode 5, Total Reward: 10000.0\n",
      "Test Episode 6, Total Reward: 10000.0\n",
      "Test Episode 7, Total Reward: 10000.0\n",
      "Test Episode 8, Total Reward: 10000.0\n",
      "Test Episode 9, Total Reward: 10000.0\n",
      "Test Episode 10, Total Reward: 10000.0\n",
      "Test Episode 11, Total Reward: 10000.0\n",
      "Test Episode 12, Total Reward: 10000.0\n",
      "Test Episode 13, Total Reward: 10000.0\n",
      "Test Episode 14, Total Reward: 10000.0\n",
      "Test Episode 15, Total Reward: 10000.0\n",
      "Test Episode 16, Total Reward: 10000.0\n",
      "Test Episode 17, Total Reward: 10000.0\n",
      "Test Episode 18, Total Reward: 10000.0\n",
      "Test Episode 19, Total Reward: 10000.0\n",
      "Test Episode 20, Total Reward: 10000.0\n",
      "Test Episode 21, Total Reward: 10000.0\n",
      "Test Episode 22, Total Reward: 10000.0\n",
      "Test Episode 23, Total Reward: 10000.0\n",
      "Test Episode 24, Total Reward: 10000.0\n",
      "Test Episode 25, Total Reward: 10000.0\n",
      "Test Episode 26, Total Reward: 10000.0\n",
      "Test Episode 27, Total Reward: 10000.0\n",
      "Test Episode 28, Total Reward: 10000.0\n",
      "Test Episode 29, Total Reward: 10000.0\n",
      "Test Episode 30, Total Reward: 10000.0\n",
      "Test Episode 31, Total Reward: 10000.0\n",
      "Test Episode 32, Total Reward: 10000.0\n",
      "Test Episode 33, Total Reward: 10000.0\n",
      "Test Episode 34, Total Reward: 10000.0\n",
      "Test Episode 35, Total Reward: 10000.0\n",
      "Test Episode 36, Total Reward: 10000.0\n",
      "Test Episode 37, Total Reward: 10000.0\n",
      "Test Episode 38, Total Reward: 10000.0\n",
      "Test Episode 39, Total Reward: 10000.0\n",
      "Test Episode 40, Total Reward: 10000.0\n",
      "Test Episode 41, Total Reward: 10000.0\n",
      "Test Episode 42, Total Reward: 10000.0\n",
      "Test Episode 43, Total Reward: 10000.0\n",
      "Test Episode 44, Total Reward: 10000.0\n",
      "Test Episode 45, Total Reward: 10000.0\n",
      "Test Episode 46, Total Reward: 10000.0\n",
      "Test Episode 47, Total Reward: 10000.0\n",
      "Test Episode 48, Total Reward: 10000.0\n",
      "Test Episode 49, Total Reward: 10000.0\n",
      "Test Episode 50, Total Reward: 10000.0\n",
      "Test Episode 51, Total Reward: 10000.0\n",
      "Test Episode 52, Total Reward: 10000.0\n",
      "Test Episode 53, Total Reward: 10000.0\n",
      "Test Episode 54, Total Reward: 10000.0\n",
      "Test Episode 55, Total Reward: 10000.0\n",
      "Test Episode 56, Total Reward: 10000.0\n",
      "Test Episode 57, Total Reward: 10000.0\n",
      "Test Episode 58, Total Reward: 10000.0\n",
      "Test Episode 59, Total Reward: 10000.0\n",
      "Test Episode 60, Total Reward: 10000.0\n",
      "Test Episode 61, Total Reward: 10000.0\n",
      "Test Episode 62, Total Reward: 10000.0\n",
      "Test Episode 63, Total Reward: 10000.0\n",
      "Test Episode 64, Total Reward: 10000.0\n",
      "Test Episode 65, Total Reward: 10000.0\n",
      "Test Episode 66, Total Reward: 10000.0\n",
      "Test Episode 67, Total Reward: 10000.0\n",
      "Test Episode 68, Total Reward: 10000.0\n",
      "Test Episode 69, Total Reward: 10000.0\n",
      "Test Episode 70, Total Reward: 10000.0\n",
      "Test Episode 71, Total Reward: 10000.0\n",
      "Test Episode 72, Total Reward: 10000.0\n",
      "Test Episode 73, Total Reward: 10000.0\n",
      "Test Episode 74, Total Reward: 10000.0\n",
      "Test Episode 75, Total Reward: 10000.0\n",
      "Test Episode 76, Total Reward: 10000.0\n",
      "Test Episode 77, Total Reward: 10000.0\n",
      "Test Episode 78, Total Reward: 10000.0\n",
      "Test Episode 79, Total Reward: 10000.0\n",
      "Test Episode 80, Total Reward: 10000.0\n",
      "Test Episode 81, Total Reward: 10000.0\n",
      "Test Episode 82, Total Reward: 10000.0\n",
      "Test Episode 83, Total Reward: 10000.0\n",
      "Test Episode 84, Total Reward: 10000.0\n",
      "Test Episode 85, Total Reward: 10000.0\n",
      "Test Episode 86, Total Reward: 10000.0\n",
      "Test Episode 87, Total Reward: 10000.0\n",
      "Test Episode 88, Total Reward: 10000.0\n",
      "Test Episode 89, Total Reward: 10000.0\n",
      "Test Episode 90, Total Reward: 10000.0\n",
      "Test Episode 91, Total Reward: 10000.0\n",
      "Test Episode 92, Total Reward: 10000.0\n",
      "Test Episode 93, Total Reward: 10000.0\n",
      "Test Episode 94, Total Reward: 10000.0\n",
      "Test Episode 95, Total Reward: 10000.0\n",
      "Test Episode 96, Total Reward: 10000.0\n",
      "Test Episode 97, Total Reward: 10000.0\n",
      "Test Episode 98, Total Reward: 10000.0\n",
      "Test Episode 99, Total Reward: 10000.0\n",
      "Test Episode 100, Total Reward: 10000.0\n",
      "Average Test Reward: 10000.0\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "41916778ed6f3e8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T13:55:04.199119Z",
     "start_time": "2024-12-09T08:26:33.243004Z"
    }
   },
   "source": [
    "# save the policy and value networks\n",
    "torch.save(policy_network.state_dict(), 'actor_critic_policy.pth')\n",
    "torch.save(value_network.state_dict(), 'actor_critic_value.pth')"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a314c0840a32a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
