{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-30T09:24:08.085970Z",
     "start_time": "2024-12-30T09:24:07.162217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from assignment3.training_loop import training_loop\n",
    "from assignment3.models import PolicyNetwork, ValueNetwork\n",
    "from assignment3.dim_alignment import ENV_ACT_DIM, max_output_dim, max_input_dim\n",
    "from assignment3.optuna_search import OptunaSearch\n",
    "from assignment3.hyper_params import StudyFloatParamRange, HyperParamsRanges, HyperParams\n",
    "from assignment3.device import get_device\n",
    "from assignment3.action_selector import ContinuousActionSelector, ActionSelector"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T09:24:08.091162Z",
     "start_time": "2024-12-30T09:24:08.088535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generalized_actor_critic(\n",
    "        env_name,\n",
    "        input_dim,\n",
    "        output_dim,\n",
    "        dropout_layers,\n",
    "        episodes,\n",
    "        hyper_params: HyperParams,\n",
    "        log_dir=\"runs/actor_critic\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a policy and value network using Actor-Critic, with padded inputs/outputs.\n",
    "    \"\"\"\n",
    "    device = get_device()\n",
    "    env = gym.make(env_name)\n",
    "    writer = SummaryWriter(log_dir=f\"{log_dir}_{env_name}\")\n",
    "\n",
    "    policy_network = PolicyNetwork(input_dim, hyper_params.hidden_sizes_theta, output_dim, dropout_layers,\n",
    "                                   hyper_params.dropout_p).to(device)\n",
    "    value_network = ValueNetwork(input_dim, hyper_params.hidden_sizes_w).to(device)\n",
    "\n",
    "    policy_optimizer = optim.Adam(policy_network.parameters(), lr=hyper_params.alpha_theta)\n",
    "    value_optimizer = optim.Adam(value_network.parameters(), lr=hyper_params.alpha_w)\n",
    "\n",
    "    action_selector = ContinuousActionSelector(\n",
    "        epsilon=hyper_params.epsilon,\n",
    "        epsilon_decay=hyper_params.epsilon_decay,\n",
    "        min_noise_std=hyper_params.min_noise_std,\n",
    "        max_noise_std=hyper_params.max_noise_std\n",
    "    ) if env_name == \"MountainCarContinuous-v0\" else ActionSelector()\n",
    "\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    # Identify the actual dimensionalities for this env\n",
    "    actual_act_dim = ENV_ACT_DIM[env_name]\n",
    "\n",
    "    train_time = training_loop(\n",
    "        input_dim=input_dim,\n",
    "        actual_act_dim=actual_act_dim,\n",
    "        policy_network=policy_network,\n",
    "        value_network=value_network,\n",
    "        policy_optimizer=policy_optimizer,\n",
    "        value_optimizer=value_optimizer,\n",
    "        env=env,\n",
    "        env_name=env_name,\n",
    "        episodes=episodes,\n",
    "        gamma=hyper_params.gamma,\n",
    "        writer=writer,\n",
    "        rewards_per_episode=rewards_per_episode,\n",
    "        action_selector=action_selector\n",
    "    )\n",
    "\n",
    "    writer.close()\n",
    "    env.close()\n",
    "\n",
    "    return policy_network, value_network, rewards_per_episode, train_time"
   ],
   "id": "39d88d020add6902",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T09:24:08.524407Z",
     "start_time": "2024-12-30T09:24:08.521468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "episodes = 2000\n",
    "n_trials = 10\n",
    "\n",
    "# Common hyperparameters for all environments\n",
    "hyper_params_default_ranges = HyperParamsRanges(\n",
    "    hidden_sizes_theta_values=[\"[16, 32, 16]\", \"[32, 64, 32]\"],\n",
    "    hidden_sizes_w_values=[\"[16, 32, 16]\", \"[32, 64, 32]\"],\n",
    "    alpha_theta_values=StudyFloatParamRange(low=0.0005, high=0.0008, step=0.0001),\n",
    "    alpha_w_values=StudyFloatParamRange(low=0.0005, high=0.0008, step=0.0001),\n",
    "    gamma_values=StudyFloatParamRange(low=0.95, high=0.99, step=0.01),\n",
    "    dropout_p_values=StudyFloatParamRange(low=0.2, high=0.5, step=0.1)\n",
    ")\n",
    "\n",
    "dropout_layers = [1]"
   ],
   "id": "654c9ac89165a89f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T09:24:09.119012Z",
     "start_time": "2024-12-30T09:24:09.115395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_experiment(env_name,\n",
    "                   dropout_layers=dropout_layers,\n",
    "                   episodes=episodes,\n",
    "                   hyper_params_ranges=hyper_params_default_ranges,\n",
    "                   n_trials=n_trials):\n",
    "    optuna_search = OptunaSearch(\n",
    "        train_function=generalized_actor_critic,\n",
    "        env_name=env_name,\n",
    "        max_input_dim=max_input_dim,\n",
    "        max_output_dim=max_output_dim,\n",
    "        dropout_layers=dropout_layers,\n",
    "        episodes=episodes,\n",
    "        hyper_params_ranges=hyper_params_ranges\n",
    "    )\n",
    "    best_policy, best_value, best_params, best_reward, study = optuna_search.optuna_search_for_env(n_trials=n_trials,\n",
    "                                                                                                   study_name=f\"{env_name}_actor_critic_study\")\n",
    "\n",
    "    print(\"\\nDone! Best parameters found by Optuna:\", best_params)\n",
    "    print(\"Best reward from Optuna:\", best_reward)\n",
    "\n",
    "    # save networks to pretrained_models\n",
    "    torch.save(best_policy.state_dict(), f\"pretrained_models/{env_name}_policy.pth\")\n",
    "    torch.save(best_value.state_dict(), f\"pretrained_models/{env_name}_value.pth\")"
   ],
   "id": "3ceaf2c534e0d102",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T09:02:12.820481Z",
     "start_time": "2024-12-30T09:02:04.067460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Launch the search on, say, CartPole-v1\n",
    "run_experiment(\"CartPole-v1\", episodes=1000)"
   ],
   "id": "925b1dfc05860767",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-30 11:02:04,068] A new study created in memory with name: CartPole-v1_actor_critic_study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 0] Env=CartPole-v1\n",
      "\n",
      "        hidden_sizes_theta=[16, 32, 16], hidden_sizes_w=[16, 32, 16],\n",
      "        gamma=0.98, dropout_p=0.4,\n",
      "        alpha_theta=0.0005, alpha_w=0.0005\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 519/1000 [00:07<00:07, 66.04episode/s, Avg Reward(100)=63.86] \n",
      "[W 2024-12-30 11:02:12,449] Trial 0 failed with parameters: {'hidden_sizes_theta': '[16, 32, 16]', 'hidden_sizes_w': '[16, 32, 16]', 'alpha_theta': 0.0005, 'alpha_w': 0.0005, 'gamma': 0.98, 'dropout_p': 0.4} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/Users/nadav/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py\", line 111, in objective_wrapper\n",
      "    return self.objective(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nadav/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py\", line 74, in objective\n",
      "    policy_network, value_network, rewards, train_time = self.train_function(**train_params)\n",
      "                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/q7/94wmcf8921379lkml24q3wbr0000gn/T/ipykernel_77329/3415480692.py\", line 39, in generalized_actor_critic\n",
      "    train_time = training_loop(\n",
      "                 ^^^^^^^^^^^^^^\n",
      "  File \"/Users/nadav/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/training_loop.py\", line 76, in training_loop\n",
      "    policy_optimizer.step()\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 487, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/adam.py\", line 223, in step\n",
      "    adam(\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 154, in maybe_fallback\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/adam.py\", line 784, in adam\n",
      "    func(\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/adam.py\", line 430, in _single_tensor_adam\n",
      "    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-12-30 11:02:12,452] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Launch the search on, say, CartPole-v1\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m run_experiment(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCartPole-v1\u001B[39m\u001B[38;5;124m\"\u001B[39m, episodes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m)\n",
      "Cell \u001B[0;32mIn[4], line 15\u001B[0m, in \u001B[0;36mrun_experiment\u001B[0;34m(env_name, dropout_layers, episodes, hyper_params_ranges, n_trials)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_experiment\u001B[39m(env_name,\n\u001B[1;32m      2\u001B[0m                    dropout_layers\u001B[38;5;241m=\u001B[39mdropout_layers,\n\u001B[1;32m      3\u001B[0m                    episodes\u001B[38;5;241m=\u001B[39mepisodes,\n\u001B[1;32m      4\u001B[0m                    hyper_params_ranges\u001B[38;5;241m=\u001B[39mhyper_params_default_ranges,\n\u001B[1;32m      5\u001B[0m                    n_trials\u001B[38;5;241m=\u001B[39mn_trials):\n\u001B[1;32m      6\u001B[0m     optuna_search \u001B[38;5;241m=\u001B[39m OptunaSearch(\n\u001B[1;32m      7\u001B[0m         train_function\u001B[38;5;241m=\u001B[39mgeneralized_actor_critic,\n\u001B[1;32m      8\u001B[0m         env_name\u001B[38;5;241m=\u001B[39menv_name,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m         hyper_params_ranges\u001B[38;5;241m=\u001B[39mhyper_params_ranges\n\u001B[1;32m     14\u001B[0m     )\n\u001B[0;32m---> 15\u001B[0m     best_policy, best_value, best_params, best_reward, study \u001B[38;5;241m=\u001B[39m optuna_search\u001B[38;5;241m.\u001B[39moptuna_search_for_env(n_trials\u001B[38;5;241m=\u001B[39mn_trials,\n\u001B[1;32m     16\u001B[0m                                                                                                    study_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00menv_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_actor_critic_study\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mDone! Best parameters found by Optuna:\u001B[39m\u001B[38;5;124m\"\u001B[39m, best_params)\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest reward from Optuna:\u001B[39m\u001B[38;5;124m\"\u001B[39m, best_reward)\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py:120\u001B[0m, in \u001B[0;36mOptunaSearch.optuna_search_for_env\u001B[0;34m(self, n_trials, study_name, source_policy_network, source_value_network, fixed_hidden_theta, fixed_hidden_w)\u001B[0m\n\u001B[1;32m    111\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective(\n\u001B[1;32m    112\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[1;32m    113\u001B[0m         source_policy_network\u001B[38;5;241m=\u001B[39msource_policy_network,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    116\u001B[0m         fixed_hidden_w\u001B[38;5;241m=\u001B[39mfixed_hidden_w\n\u001B[1;32m    117\u001B[0m     )\n\u001B[1;32m    119\u001B[0m \u001B[38;5;66;03m# 3. Run the optimization for n_trials\u001B[39;00m\n\u001B[0;32m--> 120\u001B[0m study\u001B[38;5;241m.\u001B[39moptimize(objective_wrapper, n_trials\u001B[38;5;241m=\u001B[39mn_trials)\n\u001B[1;32m    122\u001B[0m \u001B[38;5;66;03m# 4. Print some info about the best trial\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m[OPTUNA] Best trial: trail \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstudy\u001B[38;5;241m.\u001B[39mbest_trial\u001B[38;5;241m.\u001B[39mnumber\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/study.py:475\u001B[0m, in \u001B[0;36mStudy.optimize\u001B[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m    373\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moptimize\u001B[39m(\n\u001B[1;32m    374\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    375\u001B[0m     func: ObjectiveFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    382\u001B[0m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    383\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    384\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[1;32m    385\u001B[0m \n\u001B[1;32m    386\u001B[0m \u001B[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    473\u001B[0m \u001B[38;5;124;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[1;32m    474\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 475\u001B[0m     _optimize(\n\u001B[1;32m    476\u001B[0m         study\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    477\u001B[0m         func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[1;32m    478\u001B[0m         n_trials\u001B[38;5;241m=\u001B[39mn_trials,\n\u001B[1;32m    479\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m    480\u001B[0m         n_jobs\u001B[38;5;241m=\u001B[39mn_jobs,\n\u001B[1;32m    481\u001B[0m         catch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mtuple\u001B[39m(catch) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(catch, Iterable) \u001B[38;5;28;01melse\u001B[39;00m (catch,),\n\u001B[1;32m    482\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mcallbacks,\n\u001B[1;32m    483\u001B[0m         gc_after_trial\u001B[38;5;241m=\u001B[39mgc_after_trial,\n\u001B[1;32m    484\u001B[0m         show_progress_bar\u001B[38;5;241m=\u001B[39mshow_progress_bar,\n\u001B[1;32m    485\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:63\u001B[0m, in \u001B[0;36m_optimize\u001B[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m---> 63\u001B[0m         _optimize_sequential(\n\u001B[1;32m     64\u001B[0m             study,\n\u001B[1;32m     65\u001B[0m             func,\n\u001B[1;32m     66\u001B[0m             n_trials,\n\u001B[1;32m     67\u001B[0m             timeout,\n\u001B[1;32m     68\u001B[0m             catch,\n\u001B[1;32m     69\u001B[0m             callbacks,\n\u001B[1;32m     70\u001B[0m             gc_after_trial,\n\u001B[1;32m     71\u001B[0m             reseed_sampler_rng\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     72\u001B[0m             time_start\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     73\u001B[0m             progress_bar\u001B[38;5;241m=\u001B[39mprogress_bar,\n\u001B[1;32m     74\u001B[0m         )\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     76\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:160\u001B[0m, in \u001B[0;36m_optimize_sequential\u001B[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[1;32m    157\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 160\u001B[0m     frozen_trial \u001B[38;5;241m=\u001B[39m _run_trial(study, func, catch)\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[1;32m    164\u001B[0m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:248\u001B[0m, in \u001B[0;36m_run_trial\u001B[0;34m(study, func, catch)\u001B[0m\n\u001B[1;32m    241\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShould not reach.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    243\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    244\u001B[0m     frozen_trial\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m==\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mFAIL\n\u001B[1;32m    245\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    246\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[1;32m    247\u001B[0m ):\n\u001B[0;32m--> 248\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:197\u001B[0m, in \u001B[0;36m_run_trial\u001B[0;34m(study, func, catch)\u001B[0m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial\u001B[38;5;241m.\u001B[39m_trial_id, study\u001B[38;5;241m.\u001B[39m_storage):\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 197\u001B[0m         value_or_values \u001B[38;5;241m=\u001B[39m func(trial)\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mTrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    199\u001B[0m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[1;32m    200\u001B[0m         state \u001B[38;5;241m=\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mPRUNED\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py:111\u001B[0m, in \u001B[0;36mOptunaSearch.optuna_search_for_env.<locals>.objective_wrapper\u001B[0;34m(trial)\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mobjective_wrapper\u001B[39m(trial):\n\u001B[0;32m--> 111\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective(\n\u001B[1;32m    112\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[1;32m    113\u001B[0m         source_policy_network\u001B[38;5;241m=\u001B[39msource_policy_network,\n\u001B[1;32m    114\u001B[0m         source_value_network\u001B[38;5;241m=\u001B[39msource_value_network,\n\u001B[1;32m    115\u001B[0m         fixed_hidden_theta\u001B[38;5;241m=\u001B[39mfixed_hidden_theta,\n\u001B[1;32m    116\u001B[0m         fixed_hidden_w\u001B[38;5;241m=\u001B[39mfixed_hidden_w\n\u001B[1;32m    117\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py:74\u001B[0m, in \u001B[0;36mOptunaSearch.objective\u001B[0;34m(self, trial, source_policy_network, source_value_network, fixed_hidden_theta, fixed_hidden_w)\u001B[0m\n\u001B[1;32m     71\u001B[0m     train_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource_policy_network\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m source_policy_network\n\u001B[1;32m     72\u001B[0m     train_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource_value_network\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m source_value_network\n\u001B[0;32m---> 74\u001B[0m policy_network, value_network, rewards, train_time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_function(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtrain_params)\n\u001B[1;32m     76\u001B[0m \u001B[38;5;66;03m# 3. Calculate the metric to minimize\u001B[39;00m\n\u001B[1;32m     77\u001B[0m episodes_trained \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(rewards)\n",
      "Cell \u001B[0;32mIn[2], line 39\u001B[0m, in \u001B[0;36mgeneralized_actor_critic\u001B[0;34m(env_name, input_dim, output_dim, dropout_layers, episodes, hyper_params, log_dir)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Identify the actual dimensionalities for this env\u001B[39;00m\n\u001B[1;32m     37\u001B[0m actual_act_dim \u001B[38;5;241m=\u001B[39m ENV_ACT_DIM[env_name]\n\u001B[0;32m---> 39\u001B[0m train_time \u001B[38;5;241m=\u001B[39m training_loop(\n\u001B[1;32m     40\u001B[0m     input_dim\u001B[38;5;241m=\u001B[39minput_dim,\n\u001B[1;32m     41\u001B[0m     actual_act_dim\u001B[38;5;241m=\u001B[39mactual_act_dim,\n\u001B[1;32m     42\u001B[0m     policy_network\u001B[38;5;241m=\u001B[39mpolicy_network,\n\u001B[1;32m     43\u001B[0m     value_network\u001B[38;5;241m=\u001B[39mvalue_network,\n\u001B[1;32m     44\u001B[0m     policy_optimizer\u001B[38;5;241m=\u001B[39mpolicy_optimizer,\n\u001B[1;32m     45\u001B[0m     value_optimizer\u001B[38;5;241m=\u001B[39mvalue_optimizer,\n\u001B[1;32m     46\u001B[0m     env\u001B[38;5;241m=\u001B[39menv,\n\u001B[1;32m     47\u001B[0m     env_name\u001B[38;5;241m=\u001B[39menv_name,\n\u001B[1;32m     48\u001B[0m     episodes\u001B[38;5;241m=\u001B[39mepisodes,\n\u001B[1;32m     49\u001B[0m     gamma\u001B[38;5;241m=\u001B[39mhyper_params\u001B[38;5;241m.\u001B[39mgamma,\n\u001B[1;32m     50\u001B[0m     writer\u001B[38;5;241m=\u001B[39mwriter,\n\u001B[1;32m     51\u001B[0m     rewards_per_episode\u001B[38;5;241m=\u001B[39mrewards_per_episode,\n\u001B[1;32m     52\u001B[0m     action_selector\u001B[38;5;241m=\u001B[39maction_selector\n\u001B[1;32m     53\u001B[0m )\n\u001B[1;32m     55\u001B[0m writer\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m     56\u001B[0m env\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/training_loop.py:76\u001B[0m, in \u001B[0;36mtraining_loop\u001B[0;34m(input_dim, actual_act_dim, policy_network, value_network, policy_optimizer, value_optimizer, env, env_name, episodes, gamma, writer, rewards_per_episode, action_selector)\u001B[0m\n\u001B[1;32m     74\u001B[0m policy_optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     75\u001B[0m policy_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 76\u001B[0m policy_optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     78\u001B[0m \u001B[38;5;66;03m# Update the factor I\u001B[39;00m\n\u001B[1;32m     79\u001B[0m I \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m gamma\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/optimizer.py:487\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    482\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    483\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    484\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    485\u001B[0m             )\n\u001B[0;32m--> 487\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    488\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    490\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/optimizer.py:91\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     89\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     90\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 91\u001B[0m     ret \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     93\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/adam.py:223\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    211\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    213\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    214\u001B[0m         group,\n\u001B[1;32m    215\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    220\u001B[0m         state_steps,\n\u001B[1;32m    221\u001B[0m     )\n\u001B[0;32m--> 223\u001B[0m     adam(\n\u001B[1;32m    224\u001B[0m         params_with_grad,\n\u001B[1;32m    225\u001B[0m         grads,\n\u001B[1;32m    226\u001B[0m         exp_avgs,\n\u001B[1;32m    227\u001B[0m         exp_avg_sqs,\n\u001B[1;32m    228\u001B[0m         max_exp_avg_sqs,\n\u001B[1;32m    229\u001B[0m         state_steps,\n\u001B[1;32m    230\u001B[0m         amsgrad\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mamsgrad\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    231\u001B[0m         has_complex\u001B[38;5;241m=\u001B[39mhas_complex,\n\u001B[1;32m    232\u001B[0m         beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[1;32m    233\u001B[0m         beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[1;32m    234\u001B[0m         lr\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    235\u001B[0m         weight_decay\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweight_decay\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    236\u001B[0m         eps\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meps\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    237\u001B[0m         maximize\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    238\u001B[0m         foreach\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mforeach\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    239\u001B[0m         capturable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcapturable\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    240\u001B[0m         differentiable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    241\u001B[0m         fused\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfused\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    242\u001B[0m         grad_scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad_scale\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    243\u001B[0m         found_inf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    244\u001B[0m     )\n\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/optimizer.py:154\u001B[0m, in \u001B[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 154\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/adam.py:784\u001B[0m, in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    781\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    782\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[0;32m--> 784\u001B[0m func(\n\u001B[1;32m    785\u001B[0m     params,\n\u001B[1;32m    786\u001B[0m     grads,\n\u001B[1;32m    787\u001B[0m     exp_avgs,\n\u001B[1;32m    788\u001B[0m     exp_avg_sqs,\n\u001B[1;32m    789\u001B[0m     max_exp_avg_sqs,\n\u001B[1;32m    790\u001B[0m     state_steps,\n\u001B[1;32m    791\u001B[0m     amsgrad\u001B[38;5;241m=\u001B[39mamsgrad,\n\u001B[1;32m    792\u001B[0m     has_complex\u001B[38;5;241m=\u001B[39mhas_complex,\n\u001B[1;32m    793\u001B[0m     beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[1;32m    794\u001B[0m     beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[1;32m    795\u001B[0m     lr\u001B[38;5;241m=\u001B[39mlr,\n\u001B[1;32m    796\u001B[0m     weight_decay\u001B[38;5;241m=\u001B[39mweight_decay,\n\u001B[1;32m    797\u001B[0m     eps\u001B[38;5;241m=\u001B[39meps,\n\u001B[1;32m    798\u001B[0m     maximize\u001B[38;5;241m=\u001B[39mmaximize,\n\u001B[1;32m    799\u001B[0m     capturable\u001B[38;5;241m=\u001B[39mcapturable,\n\u001B[1;32m    800\u001B[0m     differentiable\u001B[38;5;241m=\u001B[39mdifferentiable,\n\u001B[1;32m    801\u001B[0m     grad_scale\u001B[38;5;241m=\u001B[39mgrad_scale,\n\u001B[1;32m    802\u001B[0m     found_inf\u001B[38;5;241m=\u001B[39mfound_inf,\n\u001B[1;32m    803\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/adam.py:430\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[1;32m    428\u001B[0m         denom \u001B[38;5;241m=\u001B[39m (max_exp_avg_sqs[i]\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[1;32m    429\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 430\u001B[0m         denom \u001B[38;5;241m=\u001B[39m (exp_avg_sq\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[1;32m    432\u001B[0m     param\u001B[38;5;241m.\u001B[39maddcdiv_(exp_avg, denom, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39mstep_size)\n\u001B[1;32m    434\u001B[0m \u001B[38;5;66;03m# Lastly, switch back to complex view\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T16:44:12.367147Z",
     "start_time": "2024-12-29T16:36:21.984772Z"
    }
   },
   "cell_type": "code",
   "source": "run_experiment(\"Acrobot-v1\", episodes=500)",
   "id": "60e3b164384bc6a2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-29 18:36:21,985] A new study created in memory with name: Acrobot-v1_actor_critic_study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 0] Env=Acrobot-v1:\n",
      "        hidden_sizes_theta=[32, 64, 32], hidden_sizes_w=[16, 32, 16],\n",
      "         gamma=0.99, dropout_p=0.30000000000000004,\n",
      "         alpha_theta=0.0006000000000000001, alpha_w=0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 100/500 [00:05<00:20, 19.48episode/s, Avg Reward(100)=-101.64]\n",
      "[I 2024-12-29 18:36:27,128] Trial 0 finished with value: 101.0 and parameters: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[16, 32, 16]', 'gamma': 0.99, 'alpha_theta': 0.0006000000000000001, 'alpha_w': 0.0008, 'dropout_p': 0.30000000000000004}. Best is trial 0 with value: 101.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved Acrobot-v1 in 101 episodes!\n",
      "\n",
      "[OPTUNA Trial 1] Env=Acrobot-v1:\n",
      "        hidden_sizes_theta=[32, 64, 32], hidden_sizes_w=[16, 32, 16],\n",
      "         gamma=0.96, dropout_p=0.4,\n",
      "         alpha_theta=0.0006000000000000001, alpha_w=0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██▏       | 107/500 [00:06<00:23, 16.88episode/s, Avg Reward(100)=-118.61]\n",
      "[I 2024-12-29 18:36:33,471] Trial 1 finished with value: 108.0 and parameters: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[16, 32, 16]', 'gamma': 0.96, 'alpha_theta': 0.0006000000000000001, 'alpha_w': 0.0007, 'dropout_p': 0.4}. Best is trial 0 with value: 101.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved Acrobot-v1 in 108 episodes!\n",
      "\n",
      "[OPTUNA Trial 2] Env=Acrobot-v1:\n",
      "        hidden_sizes_theta=[32, 64, 32], hidden_sizes_w=[16, 32, 16],\n",
      "         gamma=0.98, dropout_p=0.2,\n",
      "         alpha_theta=0.0006000000000000001, alpha_w=0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [02:01<00:00,  4.10episode/s, Avg Reward(100)=-500.00]\n",
      "[I 2024-12-29 18:38:35,313] Trial 2 finished with value: 500.0 and parameters: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[16, 32, 16]', 'gamma': 0.98, 'alpha_theta': 0.0006000000000000001, 'alpha_w': 0.0005, 'dropout_p': 0.2}. Best is trial 0 with value: 101.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 3] Env=Acrobot-v1:\n",
      "        hidden_sizes_theta=[16, 32, 16], hidden_sizes_w=[32, 64, 32],\n",
      "         gamma=0.96, dropout_p=0.2,\n",
      "         alpha_theta=0.0006000000000000001, alpha_w=0.0006000000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 235/500 [00:23<00:26,  9.94episode/s, Avg Reward(100)=-140.40]\n",
      "[I 2024-12-29 18:38:58,959] Trial 3 finished with value: 236.0 and parameters: {'hidden_sizes_theta': '[16, 32, 16]', 'hidden_sizes_w': '[32, 64, 32]', 'gamma': 0.96, 'alpha_theta': 0.0006000000000000001, 'alpha_w': 0.0006000000000000001, 'dropout_p': 0.2}. Best is trial 0 with value: 101.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved Acrobot-v1 in 236 episodes!\n",
      "\n",
      "[OPTUNA Trial 4] Env=Acrobot-v1:\n",
      "        hidden_sizes_theta=[32, 64, 32], hidden_sizes_w=[16, 32, 16],\n",
      "         gamma=0.95, dropout_p=0.5,\n",
      "         alpha_theta=0.0008, alpha_w=0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [02:02<00:00,  4.09episode/s, Avg Reward(100)=-500.00]\n",
      "[I 2024-12-29 18:41:01,128] Trial 4 finished with value: 500.0 and parameters: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[16, 32, 16]', 'gamma': 0.95, 'alpha_theta': 0.0008, 'alpha_w': 0.0008, 'dropout_p': 0.5}. Best is trial 0 with value: 101.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 5] Env=Acrobot-v1:\n",
      "        hidden_sizes_theta=[32, 64, 32], hidden_sizes_w=[32, 64, 32],\n",
      "         gamma=0.98, dropout_p=0.2,\n",
      "         alpha_theta=0.0007, alpha_w=0.0006000000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 106/500 [00:06<00:25, 15.49episode/s, Avg Reward(100)=-128.25]\n",
      "[I 2024-12-29 18:41:07,977] Trial 5 finished with value: 107.0 and parameters: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[32, 64, 32]', 'gamma': 0.98, 'alpha_theta': 0.0007, 'alpha_w': 0.0006000000000000001, 'dropout_p': 0.2}. Best is trial 0 with value: 101.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved Acrobot-v1 in 107 episodes!\n",
      "\n",
      "[OPTUNA Trial 6] Env=Acrobot-v1:\n",
      "        hidden_sizes_theta=[32, 64, 32], hidden_sizes_w=[16, 32, 16],\n",
      "         gamma=0.98, dropout_p=0.30000000000000004,\n",
      "         alpha_theta=0.0005, alpha_w=0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▉     | 245/500 [00:38<00:40,  6.36episode/s, Avg Reward(100)=-254.41]\n",
      "[I 2024-12-29 18:41:46,503] Trial 6 finished with value: 246.0 and parameters: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[16, 32, 16]', 'gamma': 0.98, 'alpha_theta': 0.0005, 'alpha_w': 0.0007, 'dropout_p': 0.30000000000000004}. Best is trial 0 with value: 101.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved Acrobot-v1 in 246 episodes!\n",
      "\n",
      "[OPTUNA Trial 7] Env=Acrobot-v1:\n",
      "        hidden_sizes_theta=[16, 32, 16], hidden_sizes_w=[32, 64, 32],\n",
      "         gamma=0.95, dropout_p=0.30000000000000004,\n",
      "         alpha_theta=0.0008, alpha_w=0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [01:59<00:00,  4.17episode/s, Avg Reward(100)=-500.00]\n",
      "[I 2024-12-29 18:43:46,504] Trial 7 finished with value: 500.0 and parameters: {'hidden_sizes_theta': '[16, 32, 16]', 'hidden_sizes_w': '[32, 64, 32]', 'gamma': 0.95, 'alpha_theta': 0.0008, 'alpha_w': 0.0005, 'dropout_p': 0.30000000000000004}. Best is trial 0 with value: 101.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 8] Env=Acrobot-v1:\n",
      "        hidden_sizes_theta=[16, 32, 16], hidden_sizes_w=[16, 32, 16],\n",
      "         gamma=0.98, dropout_p=0.2,\n",
      "         alpha_theta=0.0008, alpha_w=0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▎       | 118/500 [00:07<00:23, 16.09episode/s, Avg Reward(100)=-133.23]\n",
      "[I 2024-12-29 18:43:53,841] Trial 8 finished with value: 119.0 and parameters: {'hidden_sizes_theta': '[16, 32, 16]', 'hidden_sizes_w': '[16, 32, 16]', 'gamma': 0.98, 'alpha_theta': 0.0008, 'alpha_w': 0.0007, 'dropout_p': 0.2}. Best is trial 0 with value: 101.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved Acrobot-v1 in 119 episodes!\n",
      "\n",
      "[OPTUNA Trial 9] Env=Acrobot-v1:\n",
      "        hidden_sizes_theta=[16, 32, 16], hidden_sizes_w=[32, 64, 32],\n",
      "         gamma=0.99, dropout_p=0.2,\n",
      "         alpha_theta=0.0007, alpha_w=0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 105/500 [00:05<00:22, 17.68episode/s, Avg Reward(100)=-115.45]\n",
      "[I 2024-12-29 18:43:59,781] Trial 9 finished with value: 106.0 and parameters: {'hidden_sizes_theta': '[16, 32, 16]', 'hidden_sizes_w': '[32, 64, 32]', 'gamma': 0.99, 'alpha_theta': 0.0007, 'alpha_w': 0.0008, 'dropout_p': 0.2}. Best is trial 0 with value: 101.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved Acrobot-v1 in 106 episodes!\n",
      "\n",
      "[OPTUNA] Best trial: trail 0\n",
      "  Value (Reward): 101.00\n",
      "  Params: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[16, 32, 16]', 'gamma': 0.99, 'alpha_theta': 0.0006000000000000001, 'alpha_w': 0.0008, 'dropout_p': 0.30000000000000004}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 185/500 [00:12<00:21, 14.71episode/s, Avg Reward(100)=-167.91]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved Acrobot-v1 in 186 episodes!\n",
      "\n",
      "Total Optuna search time for Acrobot-v1: 470.38s\n",
      "\n",
      "Done! Best parameters found by Optuna: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[16, 32, 16]', 'gamma': 0.99, 'alpha_theta': 0.0006000000000000001, 'alpha_w': 0.0008, 'dropout_p': 0.30000000000000004}\n",
      "Best reward from Optuna: 101.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T09:24:27.810016Z",
     "start_time": "2024-12-30T09:24:14.158390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hyper_params_ranges_mountain_car = hyper_params_default_ranges.copy(**{\n",
    "    \"hidden_sizes_theta_values\": [\"[32, 64, 32]\", \"[32, 128, 32]\"],\n",
    "    \"hidden_sizes_w_values\": [\"[32, 64, 32]\", \"[32, 128, 32]\"],\n",
    "    \"epsilon_values\": StudyFloatParamRange(low=0.9995, high=0.9999, step=0.0001),\n",
    "    \"epsilon_decay_values\": StudyFloatParamRange(low=0.9995, high=0.9999, step=0.0001),\n",
    "    \"min_noise_std_values\": StudyFloatParamRange(low=0.05, high=0.1, step=0.01),\n",
    "    \"max_noise_std_values\": StudyFloatParamRange(low=0.2, high=0.4, step=0.1)\n",
    "})\n",
    "run_experiment(\"MountainCarContinuous-v0\", episodes=1500, hyper_params_ranges=hyper_params_ranges_mountain_car)\n"
   ],
   "id": "a717bddf1403f884",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-30 11:24:14,160] A new study created in memory with name: MountainCarContinuous-v0_actor_critic_study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 0] Env=MountainCarContinuous-v0\n",
      "hidden_sizes_theta=[32, 128, 32]  |  hidden_sizes_w=[32, 128, 32]\n",
      "        gamma=0.9900  |  dropout_p=0.4000\n",
      "        alpha_theta=0.0007  |  alpha_w=0.0008\n",
      "            epsilon=0.9998  |  epsilon_decay=0.9995\n",
      "            min_noise_std=0.1000  |  max_noise_std=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 25/1500 [00:12<12:30,  1.97episode/s]\n",
      "[W 2024-12-30 11:24:27,433] Trial 0 failed with parameters: {'hidden_sizes_theta': '[32, 128, 32]', 'hidden_sizes_w': '[32, 128, 32]', 'alpha_theta': 0.0007, 'alpha_w': 0.0008, 'gamma': 0.99, 'dropout_p': 0.4, 'epsilon': 0.9998, 'epsilon_decay': 0.9995, 'min_noise_std': 0.1, 'max_noise_std': 0.30000000000000004} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/Users/nadav/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py\", line 111, in objective_wrapper\n",
      "    return self.objective(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nadav/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py\", line 74, in objective\n",
      "    policy_network, value_network, rewards, train_time = self.train_function(**train_params)\n",
      "                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/q7/94wmcf8921379lkml24q3wbr0000gn/T/ipykernel_77760/3828627348.py\", line 36, in generalized_actor_critic\n",
      "    train_time = training_loop(\n",
      "                 ^^^^^^^^^^^^^^\n",
      "  File \"/Users/nadav/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/training_loop.py\", line 69, in training_loop\n",
      "    value_loss.backward()\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/_tensor.py\", line 581, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 347, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-12-30 11:24:27,437] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 9\u001B[0m\n\u001B[1;32m      1\u001B[0m hyper_params_ranges_mountain_car \u001B[38;5;241m=\u001B[39m hyper_params_default_ranges\u001B[38;5;241m.\u001B[39mcopy(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhidden_sizes_theta_values\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[32, 64, 32]\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[32, 128, 32]\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhidden_sizes_w_values\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[32, 64, 32]\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[32, 128, 32]\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_noise_std_values\u001B[39m\u001B[38;5;124m\"\u001B[39m: StudyFloatParamRange(low\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, high\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.4\u001B[39m, step\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m)\n\u001B[1;32m      8\u001B[0m })\n\u001B[0;32m----> 9\u001B[0m run_experiment(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMountainCarContinuous-v0\u001B[39m\u001B[38;5;124m\"\u001B[39m, episodes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1500\u001B[39m, hyper_params_ranges\u001B[38;5;241m=\u001B[39mhyper_params_ranges_mountain_car)\n",
      "Cell \u001B[0;32mIn[4], line 15\u001B[0m, in \u001B[0;36mrun_experiment\u001B[0;34m(env_name, dropout_layers, episodes, hyper_params_ranges, n_trials)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_experiment\u001B[39m(env_name,\n\u001B[1;32m      2\u001B[0m                    dropout_layers\u001B[38;5;241m=\u001B[39mdropout_layers,\n\u001B[1;32m      3\u001B[0m                    episodes\u001B[38;5;241m=\u001B[39mepisodes,\n\u001B[1;32m      4\u001B[0m                    hyper_params_ranges\u001B[38;5;241m=\u001B[39mhyper_params_default_ranges,\n\u001B[1;32m      5\u001B[0m                    n_trials\u001B[38;5;241m=\u001B[39mn_trials):\n\u001B[1;32m      6\u001B[0m     optuna_search \u001B[38;5;241m=\u001B[39m OptunaSearch(\n\u001B[1;32m      7\u001B[0m         train_function\u001B[38;5;241m=\u001B[39mgeneralized_actor_critic,\n\u001B[1;32m      8\u001B[0m         env_name\u001B[38;5;241m=\u001B[39menv_name,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m         hyper_params_ranges\u001B[38;5;241m=\u001B[39mhyper_params_ranges\n\u001B[1;32m     14\u001B[0m     )\n\u001B[0;32m---> 15\u001B[0m     best_policy, best_value, best_params, best_reward, study \u001B[38;5;241m=\u001B[39m optuna_search\u001B[38;5;241m.\u001B[39moptuna_search_for_env(n_trials\u001B[38;5;241m=\u001B[39mn_trials,\n\u001B[1;32m     16\u001B[0m                                                                                                    study_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00menv_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_actor_critic_study\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mDone! Best parameters found by Optuna:\u001B[39m\u001B[38;5;124m\"\u001B[39m, best_params)\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest reward from Optuna:\u001B[39m\u001B[38;5;124m\"\u001B[39m, best_reward)\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py:120\u001B[0m, in \u001B[0;36mOptunaSearch.optuna_search_for_env\u001B[0;34m(self, n_trials, study_name, source_policy_network, source_value_network, fixed_hidden_theta, fixed_hidden_w)\u001B[0m\n\u001B[1;32m    111\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective(\n\u001B[1;32m    112\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[1;32m    113\u001B[0m         source_policy_network\u001B[38;5;241m=\u001B[39msource_policy_network,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    116\u001B[0m         fixed_hidden_w\u001B[38;5;241m=\u001B[39mfixed_hidden_w\n\u001B[1;32m    117\u001B[0m     )\n\u001B[1;32m    119\u001B[0m \u001B[38;5;66;03m# 3. Run the optimization for n_trials\u001B[39;00m\n\u001B[0;32m--> 120\u001B[0m study\u001B[38;5;241m.\u001B[39moptimize(objective_wrapper, n_trials\u001B[38;5;241m=\u001B[39mn_trials)\n\u001B[1;32m    122\u001B[0m \u001B[38;5;66;03m# 4. Print some info about the best trial\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m[OPTUNA] Best trial: trail \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstudy\u001B[38;5;241m.\u001B[39mbest_trial\u001B[38;5;241m.\u001B[39mnumber\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/study.py:475\u001B[0m, in \u001B[0;36mStudy.optimize\u001B[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m    373\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moptimize\u001B[39m(\n\u001B[1;32m    374\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    375\u001B[0m     func: ObjectiveFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    382\u001B[0m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    383\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    384\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[1;32m    385\u001B[0m \n\u001B[1;32m    386\u001B[0m \u001B[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    473\u001B[0m \u001B[38;5;124;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[1;32m    474\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 475\u001B[0m     _optimize(\n\u001B[1;32m    476\u001B[0m         study\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    477\u001B[0m         func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[1;32m    478\u001B[0m         n_trials\u001B[38;5;241m=\u001B[39mn_trials,\n\u001B[1;32m    479\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m    480\u001B[0m         n_jobs\u001B[38;5;241m=\u001B[39mn_jobs,\n\u001B[1;32m    481\u001B[0m         catch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mtuple\u001B[39m(catch) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(catch, Iterable) \u001B[38;5;28;01melse\u001B[39;00m (catch,),\n\u001B[1;32m    482\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mcallbacks,\n\u001B[1;32m    483\u001B[0m         gc_after_trial\u001B[38;5;241m=\u001B[39mgc_after_trial,\n\u001B[1;32m    484\u001B[0m         show_progress_bar\u001B[38;5;241m=\u001B[39mshow_progress_bar,\n\u001B[1;32m    485\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:63\u001B[0m, in \u001B[0;36m_optimize\u001B[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m---> 63\u001B[0m         _optimize_sequential(\n\u001B[1;32m     64\u001B[0m             study,\n\u001B[1;32m     65\u001B[0m             func,\n\u001B[1;32m     66\u001B[0m             n_trials,\n\u001B[1;32m     67\u001B[0m             timeout,\n\u001B[1;32m     68\u001B[0m             catch,\n\u001B[1;32m     69\u001B[0m             callbacks,\n\u001B[1;32m     70\u001B[0m             gc_after_trial,\n\u001B[1;32m     71\u001B[0m             reseed_sampler_rng\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     72\u001B[0m             time_start\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     73\u001B[0m             progress_bar\u001B[38;5;241m=\u001B[39mprogress_bar,\n\u001B[1;32m     74\u001B[0m         )\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     76\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:160\u001B[0m, in \u001B[0;36m_optimize_sequential\u001B[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[1;32m    157\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 160\u001B[0m     frozen_trial \u001B[38;5;241m=\u001B[39m _run_trial(study, func, catch)\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[1;32m    164\u001B[0m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:248\u001B[0m, in \u001B[0;36m_run_trial\u001B[0;34m(study, func, catch)\u001B[0m\n\u001B[1;32m    241\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShould not reach.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    243\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    244\u001B[0m     frozen_trial\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m==\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mFAIL\n\u001B[1;32m    245\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    246\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[1;32m    247\u001B[0m ):\n\u001B[0;32m--> 248\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:197\u001B[0m, in \u001B[0;36m_run_trial\u001B[0;34m(study, func, catch)\u001B[0m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial\u001B[38;5;241m.\u001B[39m_trial_id, study\u001B[38;5;241m.\u001B[39m_storage):\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 197\u001B[0m         value_or_values \u001B[38;5;241m=\u001B[39m func(trial)\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mTrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    199\u001B[0m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[1;32m    200\u001B[0m         state \u001B[38;5;241m=\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mPRUNED\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py:111\u001B[0m, in \u001B[0;36mOptunaSearch.optuna_search_for_env.<locals>.objective_wrapper\u001B[0;34m(trial)\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mobjective_wrapper\u001B[39m(trial):\n\u001B[0;32m--> 111\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective(\n\u001B[1;32m    112\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[1;32m    113\u001B[0m         source_policy_network\u001B[38;5;241m=\u001B[39msource_policy_network,\n\u001B[1;32m    114\u001B[0m         source_value_network\u001B[38;5;241m=\u001B[39msource_value_network,\n\u001B[1;32m    115\u001B[0m         fixed_hidden_theta\u001B[38;5;241m=\u001B[39mfixed_hidden_theta,\n\u001B[1;32m    116\u001B[0m         fixed_hidden_w\u001B[38;5;241m=\u001B[39mfixed_hidden_w\n\u001B[1;32m    117\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py:74\u001B[0m, in \u001B[0;36mOptunaSearch.objective\u001B[0;34m(self, trial, source_policy_network, source_value_network, fixed_hidden_theta, fixed_hidden_w)\u001B[0m\n\u001B[1;32m     71\u001B[0m     train_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource_policy_network\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m source_policy_network\n\u001B[1;32m     72\u001B[0m     train_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource_value_network\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m source_value_network\n\u001B[0;32m---> 74\u001B[0m policy_network, value_network, rewards, train_time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_function(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtrain_params)\n\u001B[1;32m     76\u001B[0m \u001B[38;5;66;03m# 3. Calculate the metric to minimize\u001B[39;00m\n\u001B[1;32m     77\u001B[0m episodes_trained \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(rewards)\n",
      "Cell \u001B[0;32mIn[2], line 36\u001B[0m, in \u001B[0;36mgeneralized_actor_critic\u001B[0;34m(env_name, input_dim, output_dim, dropout_layers, episodes, hyper_params, log_dir)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# Identify the actual dimensionalities for this env\u001B[39;00m\n\u001B[1;32m     34\u001B[0m actual_act_dim \u001B[38;5;241m=\u001B[39m ENV_ACT_DIM[env_name]\n\u001B[0;32m---> 36\u001B[0m train_time \u001B[38;5;241m=\u001B[39m training_loop(\n\u001B[1;32m     37\u001B[0m     input_dim\u001B[38;5;241m=\u001B[39minput_dim,\n\u001B[1;32m     38\u001B[0m     actual_act_dim\u001B[38;5;241m=\u001B[39mactual_act_dim,\n\u001B[1;32m     39\u001B[0m     policy_network\u001B[38;5;241m=\u001B[39mpolicy_network,\n\u001B[1;32m     40\u001B[0m     value_network\u001B[38;5;241m=\u001B[39mvalue_network,\n\u001B[1;32m     41\u001B[0m     policy_optimizer\u001B[38;5;241m=\u001B[39mpolicy_optimizer,\n\u001B[1;32m     42\u001B[0m     value_optimizer\u001B[38;5;241m=\u001B[39mvalue_optimizer,\n\u001B[1;32m     43\u001B[0m     env\u001B[38;5;241m=\u001B[39menv,\n\u001B[1;32m     44\u001B[0m     env_name\u001B[38;5;241m=\u001B[39menv_name,\n\u001B[1;32m     45\u001B[0m     episodes\u001B[38;5;241m=\u001B[39mepisodes,\n\u001B[1;32m     46\u001B[0m     gamma\u001B[38;5;241m=\u001B[39mhyper_params\u001B[38;5;241m.\u001B[39mgamma,\n\u001B[1;32m     47\u001B[0m     writer\u001B[38;5;241m=\u001B[39mwriter,\n\u001B[1;32m     48\u001B[0m     rewards_per_episode\u001B[38;5;241m=\u001B[39mrewards_per_episode,\n\u001B[1;32m     49\u001B[0m     action_selector\u001B[38;5;241m=\u001B[39maction_selector\n\u001B[1;32m     50\u001B[0m )\n\u001B[1;32m     52\u001B[0m writer\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m     53\u001B[0m env\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/training_loop.py:69\u001B[0m, in \u001B[0;36mtraining_loop\u001B[0;34m(input_dim, actual_act_dim, policy_network, value_network, policy_optimizer, value_optimizer, env, env_name, episodes, gamma, writer, rewards_per_episode, action_selector)\u001B[0m\n\u001B[1;32m     67\u001B[0m value_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mvalue \u001B[38;5;241m*\u001B[39m delta\u001B[38;5;241m.\u001B[39mdetach() \u001B[38;5;241m*\u001B[39m I\n\u001B[1;32m     68\u001B[0m value_optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 69\u001B[0m value_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     70\u001B[0m value_optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     72\u001B[0m \u001B[38;5;66;03m# Policy update\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    580\u001B[0m     )\n\u001B[0;32m--> 581\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[1;32m    582\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[1;32m    583\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 347\u001B[0m _engine_run_backward(\n\u001B[1;32m    348\u001B[0m     tensors,\n\u001B[1;32m    349\u001B[0m     grad_tensors_,\n\u001B[1;32m    350\u001B[0m     retain_graph,\n\u001B[1;32m    351\u001B[0m     create_graph,\n\u001B[1;32m    352\u001B[0m     inputs,\n\u001B[1;32m    353\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    354\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    355\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    826\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    827\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T19:05:32.905116Z",
     "start_time": "2024-12-29T19:05:32.903630Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "116c610e1b8afab0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
