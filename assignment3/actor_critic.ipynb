{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-28T14:37:54.282256Z",
     "start_time": "2024-12-28T14:37:53.475199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from assignment3.training_loop import training_loop\n",
    "from assignment3.models import PolicyNetwork, ValueNetwork\n",
    "from assignment3.dim_alignment import ENV_ACT_DIM, max_output_dim, max_input_dim\n",
    "from assignment3.optuna_search import OptunaSearch"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T14:37:54.288625Z",
     "start_time": "2024-12-28T14:37:54.285804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generalized_actor_critic(\n",
    "        env_name,\n",
    "        input_dim,\n",
    "        output_dim,\n",
    "        hidden_sizes_theta,\n",
    "        hidden_sizes_w,\n",
    "        alpha_theta=0.001,\n",
    "        alpha_w=0.001,\n",
    "        episodes=500,\n",
    "        gamma=0.99,\n",
    "        log_dir=\"runs/actor_critic\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a policy and value network using Actor-Critic, with padded inputs/outputs.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    env = gym.make(env_name)\n",
    "    writer = SummaryWriter(log_dir=f\"{log_dir}_{env_name}\")\n",
    "\n",
    "    policy_network = PolicyNetwork(input_dim, hidden_sizes_theta, output_dim).to(device)\n",
    "    value_network = ValueNetwork(input_dim, hidden_sizes_w).to(device)\n",
    "\n",
    "    policy_optimizer = optim.Adam(policy_network.parameters(), lr=alpha_theta)\n",
    "    value_optimizer = optim.Adam(value_network.parameters(), lr=alpha_w)\n",
    "\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    # Identify the actual dimensionalities for this env\n",
    "    actual_act_dim = ENV_ACT_DIM[env_name] \n",
    "\n",
    "    train_time = training_loop(\n",
    "        input_dim=input_dim,\n",
    "        actual_act_dim=actual_act_dim,\n",
    "        policy_network=policy_network,\n",
    "        value_network=value_network,\n",
    "        policy_optimizer=policy_optimizer,\n",
    "        value_optimizer=value_optimizer,\n",
    "        env=env,\n",
    "        env_name=env_name,\n",
    "        episodes=episodes,\n",
    "        gamma=gamma,\n",
    "        device=device,\n",
    "        writer=writer,\n",
    "        rewards_per_episode=rewards_per_episode,\n",
    "    )\n",
    "\n",
    "    writer.close()\n",
    "    env.close()\n",
    "\n",
    "    return policy_network, value_network, rewards_per_episode, train_time"
   ],
   "id": "39d88d020add6902",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T14:37:54.297998Z",
     "start_time": "2024-12-28T14:37:54.295854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: to find the best hyperparameters for each environment, initialize different ranges and params for each env separately\n",
    "\n",
    "# Common hidden sizes\n",
    "hidden_sizes_theta = [16, 32, 16]\n",
    "hidden_sizes_w = [16, 32, 16]\n",
    "episodes = 2000\n",
    "n_trials = 10\n",
    "overall_results = {}\n",
    "\n",
    "# Define your search ranges\n",
    "gamma_values = [0.95, 0.99]\n",
    "alpha_theta_values = [0.001, 0.0005]\n",
    "alpha_w_values = [0.001, 0.0005]"
   ],
   "id": "654c9ac89165a89f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T14:37:54.569498Z",
     "start_time": "2024-12-28T14:37:54.565873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_experiment(env_name):\n",
    "    optuna_search = OptunaSearch(\n",
    "        train_function=generalized_actor_critic,\n",
    "        env_name=env_name,\n",
    "        max_input_dim=max_input_dim,\n",
    "        max_output_dim=max_output_dim,\n",
    "        hidden_sizes_theta=hidden_sizes_theta,\n",
    "        hidden_sizes_w=hidden_sizes_w,\n",
    "        gamma_values=gamma_values,\n",
    "        alpha_theta_values=alpha_theta_values,\n",
    "        alpha_w_values=alpha_w_values,\n",
    "        episodes=episodes,\n",
    "    )\n",
    "    best_policy, best_value, best_params, best_reward, study = optuna_search.optuna_search_for_env(n_trials=n_trials)\n",
    "\n",
    "    print(\"\\nDone! Best parameters found by Optuna:\", best_params)\n",
    "    print(\"Best reward from Optuna:\", best_reward)\n",
    "\n",
    "\n",
    "# save networks to pretrained_models\n",
    "    torch.save(best_policy.state_dict(), f\"pretrained_models/{env_name}_policy.pth\")\n",
    "    torch.save(best_value.state_dict(), f\"pretrained_models/{env_name}_value.pth\")"
   ],
   "id": "3ceaf2c534e0d102",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T13:30:51.701813Z",
     "start_time": "2024-12-28T13:30:13.130571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Launch the search on, say, CartPole-v1\n",
    "# TODO: experiment fails with Nan values somehow, need to investigate\n",
    "run_experiment(\"CartPole-v1\")"
   ],
   "id": "925b1dfc05860767",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-28 15:30:13,131] A new study created in memory with name: no-name-a30ec3c4-ab68-4446-b418-1243aab85191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial] Env=CartPole-v1 | gamma=0.95, alpha_theta=0.0005, alpha_w=0.001\n",
      "Episode 100: Reward=10.00, Avg(100)=15.23\n",
      "Episode 200: Reward=16.00, Avg(100)=26.41\n",
      "Episode 300: Reward=44.00, Avg(100)=56.46\n",
      "Episode 400: Reward=64.00, Avg(100)=72.98\n",
      "Episode 500: Reward=155.00, Avg(100)=126.16\n",
      "Episode 600: Reward=124.00, Avg(100)=146.36\n",
      "Episode 700: Reward=175.00, Avg(100)=141.31\n",
      "Episode 800: Reward=65.00, Avg(100)=80.91\n",
      "Episode 900: Reward=128.00, Avg(100)=101.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-12-28 15:30:51,428] Trial 0 failed with parameters: {'gamma': 0.95, 'alpha_theta': 0.0005, 'alpha_w': 0.001} because of the following error: ValueError('Expected parameter probs (Tensor of shape (2,)) of distribution Categorical(probs: torch.Size([2])) to satisfy the constraint Simplex(), but found invalid values:\\ntensor([nan, nan], grad_fn=<DivBackward0>)').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/Users/nadav/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py\", line 98, in objective_wrapper\n",
      "    return self.objective(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nadav/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py\", line 56, in objective\n",
      "    policy_network, value_network, rewards, train_time = self.train_function(\n",
      "                                                         ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/q7/94wmcf8921379lkml24q3wbr0000gn/T/ipykernel_58522/956835820.py\", line 31, in generalized_actor_critic\n",
      "    train_time = training_loop(\n",
      "                 ^^^^^^^^^^^^^^\n",
      "  File \"/Users/nadav/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/training_loop.py\", line 43, in training_loop\n",
      "    action = sample_valid_action(action_probs, valid_action_dim=actual_act_dim)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nadav/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/dim_alignment.py\", line 38, in sample_valid_action\n",
      "    dist = torch.distributions.Categorical(action_probs[0, :valid_action_dim])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/distributions/categorical.py\", line 72, in __init__\n",
      "    super().__init__(batch_shape, validate_args=validate_args)\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/distributions/distribution.py\", line 71, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Expected parameter probs (Tensor of shape (2,)) of distribution Categorical(probs: torch.Size([2])) to satisfy the constraint Simplex(), but found invalid values:\n",
      "tensor([nan, nan], grad_fn=<DivBackward0>)\n",
      "[W 2024-12-28 15:30:51,429] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter probs (Tensor of shape (2,)) of distribution Categorical(probs: torch.Size([2])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([nan, nan], grad_fn=<DivBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Launch the search on, say, CartPole-v1\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m run_experiment(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCartPole-v1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[4], line 14\u001B[0m, in \u001B[0;36mrun_experiment\u001B[0;34m(env_name)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_experiment\u001B[39m(env_name):\n\u001B[1;32m      2\u001B[0m     optuna_search \u001B[38;5;241m=\u001B[39m OptunaSearch(\n\u001B[1;32m      3\u001B[0m         train_function\u001B[38;5;241m=\u001B[39mgeneralized_actor_critic,\n\u001B[1;32m      4\u001B[0m         env_name\u001B[38;5;241m=\u001B[39menv_name,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m         episodes\u001B[38;5;241m=\u001B[39mepisodes,\n\u001B[1;32m     13\u001B[0m     )\n\u001B[0;32m---> 14\u001B[0m     best_policy, best_value, best_params, best_reward, study \u001B[38;5;241m=\u001B[39m optuna_search\u001B[38;5;241m.\u001B[39moptuna_search_for_env(n_trials\u001B[38;5;241m=\u001B[39mn_trials)\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mDone! Best parameters found by Optuna:\u001B[39m\u001B[38;5;124m\"\u001B[39m, best_params)\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest reward from Optuna:\u001B[39m\u001B[38;5;124m\"\u001B[39m, best_reward)\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py:103\u001B[0m, in \u001B[0;36mOptunaSearch.optuna_search_for_env\u001B[0;34m(self, n_trials)\u001B[0m\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective(\n\u001B[1;32m     99\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[1;32m    100\u001B[0m     )\n\u001B[1;32m    102\u001B[0m \u001B[38;5;66;03m# 3. Run the optimization for n_trials\u001B[39;00m\n\u001B[0;32m--> 103\u001B[0m study\u001B[38;5;241m.\u001B[39moptimize(objective_wrapper, n_trials\u001B[38;5;241m=\u001B[39mn_trials)\n\u001B[1;32m    105\u001B[0m \u001B[38;5;66;03m# 4. Print some info about the best trial\u001B[39;00m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m[OPTUNA] Best trial:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/study.py:475\u001B[0m, in \u001B[0;36mStudy.optimize\u001B[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m    373\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moptimize\u001B[39m(\n\u001B[1;32m    374\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    375\u001B[0m     func: ObjectiveFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    382\u001B[0m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    383\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    384\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[1;32m    385\u001B[0m \n\u001B[1;32m    386\u001B[0m \u001B[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    473\u001B[0m \u001B[38;5;124;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[1;32m    474\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 475\u001B[0m     _optimize(\n\u001B[1;32m    476\u001B[0m         study\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    477\u001B[0m         func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[1;32m    478\u001B[0m         n_trials\u001B[38;5;241m=\u001B[39mn_trials,\n\u001B[1;32m    479\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m    480\u001B[0m         n_jobs\u001B[38;5;241m=\u001B[39mn_jobs,\n\u001B[1;32m    481\u001B[0m         catch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mtuple\u001B[39m(catch) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(catch, Iterable) \u001B[38;5;28;01melse\u001B[39;00m (catch,),\n\u001B[1;32m    482\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mcallbacks,\n\u001B[1;32m    483\u001B[0m         gc_after_trial\u001B[38;5;241m=\u001B[39mgc_after_trial,\n\u001B[1;32m    484\u001B[0m         show_progress_bar\u001B[38;5;241m=\u001B[39mshow_progress_bar,\n\u001B[1;32m    485\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:63\u001B[0m, in \u001B[0;36m_optimize\u001B[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m---> 63\u001B[0m         _optimize_sequential(\n\u001B[1;32m     64\u001B[0m             study,\n\u001B[1;32m     65\u001B[0m             func,\n\u001B[1;32m     66\u001B[0m             n_trials,\n\u001B[1;32m     67\u001B[0m             timeout,\n\u001B[1;32m     68\u001B[0m             catch,\n\u001B[1;32m     69\u001B[0m             callbacks,\n\u001B[1;32m     70\u001B[0m             gc_after_trial,\n\u001B[1;32m     71\u001B[0m             reseed_sampler_rng\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     72\u001B[0m             time_start\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     73\u001B[0m             progress_bar\u001B[38;5;241m=\u001B[39mprogress_bar,\n\u001B[1;32m     74\u001B[0m         )\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     76\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:160\u001B[0m, in \u001B[0;36m_optimize_sequential\u001B[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[1;32m    157\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 160\u001B[0m     frozen_trial \u001B[38;5;241m=\u001B[39m _run_trial(study, func, catch)\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[1;32m    164\u001B[0m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:248\u001B[0m, in \u001B[0;36m_run_trial\u001B[0;34m(study, func, catch)\u001B[0m\n\u001B[1;32m    241\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShould not reach.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    243\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    244\u001B[0m     frozen_trial\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m==\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mFAIL\n\u001B[1;32m    245\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    246\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[1;32m    247\u001B[0m ):\n\u001B[0;32m--> 248\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:197\u001B[0m, in \u001B[0;36m_run_trial\u001B[0;34m(study, func, catch)\u001B[0m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial\u001B[38;5;241m.\u001B[39m_trial_id, study\u001B[38;5;241m.\u001B[39m_storage):\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 197\u001B[0m         value_or_values \u001B[38;5;241m=\u001B[39m func(trial)\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mTrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    199\u001B[0m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[1;32m    200\u001B[0m         state \u001B[38;5;241m=\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mPRUNED\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py:98\u001B[0m, in \u001B[0;36mOptunaSearch.optuna_search_for_env.<locals>.objective_wrapper\u001B[0;34m(trial)\u001B[0m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mobjective_wrapper\u001B[39m(trial):\n\u001B[0;32m---> 98\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective(\n\u001B[1;32m     99\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[1;32m    100\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py:56\u001B[0m, in \u001B[0;36mOptunaSearch.objective\u001B[0;34m(self, trial)\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m[OPTUNA Trial] Env=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | gamma=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgamma\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, alpha_theta=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00malpha_theta\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, alpha_w=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00malpha_w\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# 2. Train actor-critic with these parameters\u001B[39;00m\n\u001B[0;32m---> 56\u001B[0m policy_network, value_network, rewards, train_time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_function(\n\u001B[1;32m     57\u001B[0m     env_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv_name,\n\u001B[1;32m     58\u001B[0m     input_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_input_dim,\n\u001B[1;32m     59\u001B[0m     output_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_output_dim,\n\u001B[1;32m     60\u001B[0m     hidden_sizes_theta\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden_sizes_theta,\n\u001B[1;32m     61\u001B[0m     hidden_sizes_w\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden_sizes_w,\n\u001B[1;32m     62\u001B[0m     alpha_theta\u001B[38;5;241m=\u001B[39malpha_theta,\n\u001B[1;32m     63\u001B[0m     alpha_w\u001B[38;5;241m=\u001B[39malpha_w,\n\u001B[1;32m     64\u001B[0m     episodes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepisodes,\n\u001B[1;32m     65\u001B[0m     gamma\u001B[38;5;241m=\u001B[39mgamma,\n\u001B[1;32m     66\u001B[0m     log_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_g\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgamma\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_at\u001B[39m\u001B[38;5;132;01m{\u001B[39;00malpha_theta\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_aw\u001B[39m\u001B[38;5;132;01m{\u001B[39;00malpha_w\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     67\u001B[0m )\n\u001B[1;32m     69\u001B[0m \u001B[38;5;66;03m# 3. Compute the metric — e.g. average of the last 100 rewards\u001B[39;00m\n\u001B[1;32m     70\u001B[0m avg_reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(np\u001B[38;5;241m.\u001B[39mmean(rewards[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m100\u001B[39m:]))\n",
      "Cell \u001B[0;32mIn[2], line 31\u001B[0m, in \u001B[0;36mgeneralized_actor_critic\u001B[0;34m(env_name, input_dim, output_dim, hidden_sizes_theta, hidden_sizes_w, alpha_theta, alpha_w, episodes, gamma, log_dir)\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Identify the actual dimensionalities for this env\u001B[39;00m\n\u001B[1;32m     29\u001B[0m actual_act_dim \u001B[38;5;241m=\u001B[39m ENV_ACT_DIM[env_name] \n\u001B[0;32m---> 31\u001B[0m train_time \u001B[38;5;241m=\u001B[39m training_loop(\n\u001B[1;32m     32\u001B[0m     input_dim\u001B[38;5;241m=\u001B[39minput_dim,\n\u001B[1;32m     33\u001B[0m     actual_act_dim\u001B[38;5;241m=\u001B[39mactual_act_dim,\n\u001B[1;32m     34\u001B[0m     policy_network\u001B[38;5;241m=\u001B[39mpolicy_network,\n\u001B[1;32m     35\u001B[0m     value_network\u001B[38;5;241m=\u001B[39mvalue_network,\n\u001B[1;32m     36\u001B[0m     policy_optimizer\u001B[38;5;241m=\u001B[39mpolicy_optimizer,\n\u001B[1;32m     37\u001B[0m     value_optimizer\u001B[38;5;241m=\u001B[39mvalue_optimizer,\n\u001B[1;32m     38\u001B[0m     env\u001B[38;5;241m=\u001B[39menv,\n\u001B[1;32m     39\u001B[0m     env_name\u001B[38;5;241m=\u001B[39menv_name,\n\u001B[1;32m     40\u001B[0m     episodes\u001B[38;5;241m=\u001B[39mepisodes,\n\u001B[1;32m     41\u001B[0m     gamma\u001B[38;5;241m=\u001B[39mgamma,\n\u001B[1;32m     42\u001B[0m     device\u001B[38;5;241m=\u001B[39mdevice,\n\u001B[1;32m     43\u001B[0m     writer\u001B[38;5;241m=\u001B[39mwriter,\n\u001B[1;32m     44\u001B[0m     rewards_per_episode\u001B[38;5;241m=\u001B[39mrewards_per_episode,\n\u001B[1;32m     45\u001B[0m )\n\u001B[1;32m     47\u001B[0m writer\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m     48\u001B[0m env\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/training_loop.py:43\u001B[0m, in \u001B[0;36mtraining_loop\u001B[0;34m(input_dim, actual_act_dim, policy_network, value_network, policy_optimizer, value_optimizer, env, env_name, episodes, gamma, device, writer, rewards_per_episode)\u001B[0m\n\u001B[1;32m     40\u001B[0m action_probs \u001B[38;5;241m=\u001B[39m policy_network(state_tensor)\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# Sample valid action from first 'actual_act_dim' entries\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m action \u001B[38;5;241m=\u001B[39m sample_valid_action(action_probs, valid_action_dim\u001B[38;5;241m=\u001B[39mactual_act_dim)\n\u001B[1;32m     44\u001B[0m log_prob_action \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mlog(action_probs[\u001B[38;5;241m0\u001B[39m, action])\n\u001B[1;32m     46\u001B[0m \u001B[38;5;66;03m# Step in the environment\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/dim_alignment.py:38\u001B[0m, in \u001B[0;36msample_valid_action\u001B[0;34m(action_probs, valid_action_dim)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msample_valid_action\u001B[39m(action_probs, valid_action_dim):\n\u001B[1;32m     31\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;124;03m    Samples an action from the first `valid_action_dim` entries of `action_probs`.\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;124;03m    If an \"empty\" action (>= valid_action_dim) is chosen, resample.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;124;03m    unused action probabilities.\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 38\u001B[0m     dist \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdistributions\u001B[38;5;241m.\u001B[39mCategorical(action_probs[\u001B[38;5;241m0\u001B[39m, :valid_action_dim])\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m dist\u001B[38;5;241m.\u001B[39msample()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/distributions/categorical.py:72\u001B[0m, in \u001B[0;36mCategorical.__init__\u001B[0;34m(self, probs, logits, validate_args)\u001B[0m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_events \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_param\u001B[38;5;241m.\u001B[39msize()[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     69\u001B[0m batch_shape \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_param\u001B[38;5;241m.\u001B[39msize()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_param\u001B[38;5;241m.\u001B[39mndimension() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mSize()\n\u001B[1;32m     71\u001B[0m )\n\u001B[0;32m---> 72\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(batch_shape, validate_args\u001B[38;5;241m=\u001B[39mvalidate_args)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/distributions/distribution.py:71\u001B[0m, in \u001B[0;36mDistribution.__init__\u001B[0;34m(self, batch_shape, event_shape, validate_args)\u001B[0m\n\u001B[1;32m     69\u001B[0m         valid \u001B[38;5;241m=\u001B[39m constraint\u001B[38;5;241m.\u001B[39mcheck(value)\n\u001B[1;32m     70\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m valid\u001B[38;5;241m.\u001B[39mall():\n\u001B[0;32m---> 71\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     72\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected parameter \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     73\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(value)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m of shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtuple\u001B[39m(value\u001B[38;5;241m.\u001B[39mshape)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     74\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mof distribution \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     75\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto satisfy the constraint \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(constraint)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     76\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut found invalid values:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mvalue\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     77\u001B[0m             )\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "\u001B[0;31mValueError\u001B[0m: Expected parameter probs (Tensor of shape (2,)) of distribution Categorical(probs: torch.Size([2])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([nan, nan], grad_fn=<DivBackward0>)"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T13:53:21.409863Z",
     "start_time": "2024-12-28T13:31:35.343618Z"
    }
   },
   "cell_type": "code",
   "source": "run_experiment(\"Acrobot-v1\")",
   "id": "60e3b164384bc6a2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-28 15:31:35,344] A new study created in memory with name: no-name-37523063-bac5-4a73-92ff-f3e358741da1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial] Env=Acrobot-v1 | gamma=0.95, alpha_theta=0.001, alpha_w=0.001\n",
      "Episode 100: Reward=-77.00, Avg(100)=-138.74\n",
      "Episode 200: Reward=-500.00, Avg(100)=-280.03\n",
      "Episode 300: Reward=-121.00, Avg(100)=-245.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-28 15:32:08,504] Trial 0 finished with value: -98.38 and parameters: {'gamma': 0.95, 'alpha_theta': 0.001, 'alpha_w': 0.001}. Best is trial 0 with value: -98.38.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved Acrobot-v1 in 338 episodes!\n",
      "\n",
      "[OPTUNA Trial] Env=Acrobot-v1 | gamma=0.95, alpha_theta=0.0005, alpha_w=0.0005\n",
      "Episode 100: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 200: Reward=-108.00, Avg(100)=-276.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-28 15:32:47,037] Trial 1 finished with value: -99.9 and parameters: {'gamma': 0.95, 'alpha_theta': 0.0005, 'alpha_w': 0.0005}. Best is trial 0 with value: -98.38.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved Acrobot-v1 in 247 episodes!\n",
      "\n",
      "[OPTUNA Trial] Env=Acrobot-v1 | gamma=0.99, alpha_theta=0.001, alpha_w=0.0005\n",
      "Episode 100: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 200: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 300: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 400: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 500: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 600: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 700: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 800: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 900: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 1000: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 1100: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 1200: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 1300: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 1400: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 1500: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 1600: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 1700: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 1800: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 1900: Reward=-500.00, Avg(100)=-500.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-28 15:40:37,751] Trial 2 finished with value: -500.0 and parameters: {'gamma': 0.99, 'alpha_theta': 0.001, 'alpha_w': 0.0005}. Best is trial 0 with value: -98.38.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2000: Reward=-500.00, Avg(100)=-500.00\n",
      "\n",
      "[OPTUNA Trial] Env=Acrobot-v1 | gamma=0.99, alpha_theta=0.001, alpha_w=0.0005\n",
      "Episode 100: Reward=-81.00, Avg(100)=-125.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-28 15:40:44,220] Trial 3 finished with value: -99.69 and parameters: {'gamma': 0.99, 'alpha_theta': 0.001, 'alpha_w': 0.0005}. Best is trial 0 with value: -98.38.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved Acrobot-v1 in 111 episodes!\n",
      "\n",
      "[OPTUNA Trial] Env=Acrobot-v1 | gamma=0.95, alpha_theta=0.0005, alpha_w=0.0005\n",
      "Episode 100: Reward=-76.00, Avg(100)=-190.68\n",
      "Episode 200: Reward=-96.00, Avg(100)=-103.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-28 15:40:59,565] Trial 4 finished with value: -99.77 and parameters: {'gamma': 0.95, 'alpha_theta': 0.0005, 'alpha_w': 0.0005}. Best is trial 0 with value: -98.38.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved Acrobot-v1 in 224 episodes!\n",
      "\n",
      "[OPTUNA Trial] Env=Acrobot-v1 | gamma=0.95, alpha_theta=0.0005, alpha_w=0.001\n",
      "Episode 100: Reward=-500.00, Avg(100)=-175.16\n",
      "Episode 200: Reward=-500.00, Avg(100)=-495.42\n",
      "Episode 300: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 400: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 500: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 600: Reward=-122.00, Avg(100)=-404.76\n",
      "Episode 700: Reward=-72.00, Avg(100)=-160.81\n",
      "Episode 800: Reward=-107.00, Avg(100)=-381.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-28 15:43:29,812] Trial 5 finished with value: -98.96 and parameters: {'gamma': 0.95, 'alpha_theta': 0.0005, 'alpha_w': 0.001}. Best is trial 0 with value: -98.38.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved Acrobot-v1 in 881 episodes!\n",
      "\n",
      "[OPTUNA Trial] Env=Acrobot-v1 | gamma=0.95, alpha_theta=0.0005, alpha_w=0.001\n",
      "Episode 100: Reward=-89.00, Avg(100)=-221.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-28 15:43:43,979] Trial 6 finished with value: -99.26 and parameters: {'gamma': 0.95, 'alpha_theta': 0.0005, 'alpha_w': 0.001}. Best is trial 0 with value: -98.38.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved Acrobot-v1 in 184 episodes!\n",
      "\n",
      "[OPTUNA Trial] Env=Acrobot-v1 | gamma=0.99, alpha_theta=0.0005, alpha_w=0.001\n",
      "Episode 100: Reward=-97.00, Avg(100)=-124.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-28 15:43:50,447] Trial 7 finished with value: -99.71 and parameters: {'gamma': 0.99, 'alpha_theta': 0.0005, 'alpha_w': 0.001}. Best is trial 0 with value: -98.38.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved Acrobot-v1 in 112 episodes!\n",
      "\n",
      "[OPTUNA Trial] Env=Acrobot-v1 | gamma=0.99, alpha_theta=0.001, alpha_w=0.0005\n",
      "Episode 100: Reward=-155.00, Avg(100)=-422.46\n",
      "Episode 200: Reward=-500.00, Avg(100)=-485.44\n",
      "Episode 300: Reward=-500.00, Avg(100)=-481.27\n",
      "Episode 400: Reward=-97.00, Avg(100)=-250.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-28 15:45:09,783] Trial 8 finished with value: -97.64 and parameters: {'gamma': 0.99, 'alpha_theta': 0.001, 'alpha_w': 0.0005}. Best is trial 8 with value: -97.64.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved Acrobot-v1 in 440 episodes!\n",
      "\n",
      "[OPTUNA Trial] Env=Acrobot-v1 | gamma=0.99, alpha_theta=0.001, alpha_w=0.0005\n",
      "Episode 100: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 200: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 300: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 400: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 500: Reward=-500.00, Avg(100)=-499.20\n",
      "Episode 600: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 700: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 800: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 900: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 1000: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 1100: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 1200: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 1300: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 1400: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 1500: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 1600: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 1700: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 1800: Reward=-500.00, Avg(100)=-500.00\n",
      "Episode 1900: Reward=-500.00, Avg(100)=-500.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-28 15:53:02,569] Trial 9 finished with value: -500.0 and parameters: {'gamma': 0.99, 'alpha_theta': 0.001, 'alpha_w': 0.0005}. Best is trial 8 with value: -97.64.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2000: Reward=-500.00, Avg(100)=-500.00\n",
      "\n",
      "[OPTUNA] Best trial:\n",
      "  Value (Reward): -97.64\n",
      "  Params: {'gamma': 0.99, 'alpha_theta': 0.001, 'alpha_w': 0.0005}\n",
      "Episode 100: Reward=-94.00, Avg(100)=-141.07\n",
      "Episode 200: Reward=-95.00, Avg(100)=-101.69\n",
      "Episode 300: Reward=-80.00, Avg(100)=-108.48\n",
      "Solved Acrobot-v1 in 345 episodes!\n",
      "\n",
      "Total Optuna search time for Acrobot-v1: 1306.06s\n",
      "\n",
      "Done! Best parameters found by Optuna: {'gamma': 0.99, 'alpha_theta': 0.001, 'alpha_w': 0.0005}\n",
      "Best reward from Optuna: -97.64\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T14:38:03.243939Z",
     "start_time": "2024-12-28T14:38:02.595721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "run_experiment(\"MountainCarContinuous-v0\")\n",
    "# TODO: experiment fails with 'int' object is not subscriptable, need to investigate\n"
   ],
   "id": "a717bddf1403f884",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-28 16:38:02,597] A new study created in memory with name: no-name-e2761128-682d-4f3c-83f1-0e455df0e4e5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial] Env=MountainCarContinuous-v0 | gamma=0.99, alpha_theta=0.0005, alpha_w=0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-12-28 16:38:03,059] Trial 0 failed with parameters: {'gamma': 0.99, 'alpha_theta': 0.0005, 'alpha_w': 0.0005} because of the following error: TypeError(\"'int' object is not subscriptable\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/Users/nadav/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py\", line 108, in objective_wrapper\n",
      "    return self.objective(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nadav/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py\", line 77, in objective\n",
      "    policy_network, value_network, rewards, train_time = self.train_function(**train_params)\n",
      "                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/q7/94wmcf8921379lkml24q3wbr0000gn/T/ipykernel_59399/956835820.py\", line 31, in generalized_actor_critic\n",
      "    train_time = training_loop(\n",
      "                 ^^^^^^^^^^^^^^\n",
      "  File \"/Users/nadav/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/training_loop.py\", line 47, in training_loop\n",
      "    next_state, reward, done, truncated, _info = env.step(action.item())\n",
      "                                                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/gymnasium/wrappers/time_limit.py\", line 57, in step\n",
      "    observation, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py\", line 56, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/gymnasium/wrappers/env_checker.py\", line 47, in step\n",
      "    return env_step_passive_checker(self.env, action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py\", line 237, in env_step_passive_checker\n",
      "    result = env.step(action)\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/gymnasium/envs/classic_control/continuous_mountain_car.py\", line 149, in step\n",
      "    force = min(max(action[0], self.min_action), self.max_action)\n",
      "                    ~~~~~~^^^\n",
      "TypeError: 'int' object is not subscriptable\n",
      "[W 2024-12-28 16:38:03,061] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m run_experiment(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMountainCarContinuous-v0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[5], line 14\u001B[0m, in \u001B[0;36mrun_experiment\u001B[0;34m(env_name)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_experiment\u001B[39m(env_name):\n\u001B[1;32m      2\u001B[0m     optuna_search \u001B[38;5;241m=\u001B[39m OptunaSearch(\n\u001B[1;32m      3\u001B[0m         train_function\u001B[38;5;241m=\u001B[39mgeneralized_actor_critic,\n\u001B[1;32m      4\u001B[0m         env_name\u001B[38;5;241m=\u001B[39menv_name,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m         episodes\u001B[38;5;241m=\u001B[39mepisodes,\n\u001B[1;32m     13\u001B[0m     )\n\u001B[0;32m---> 14\u001B[0m     best_policy, best_value, best_params, best_reward, study \u001B[38;5;241m=\u001B[39m optuna_search\u001B[38;5;241m.\u001B[39moptuna_search_for_env(n_trials\u001B[38;5;241m=\u001B[39mn_trials)\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mDone! Best parameters found by Optuna:\u001B[39m\u001B[38;5;124m\"\u001B[39m, best_params)\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest reward from Optuna:\u001B[39m\u001B[38;5;124m\"\u001B[39m, best_reward)\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py:113\u001B[0m, in \u001B[0;36mOptunaSearch.optuna_search_for_env\u001B[0;34m(self, n_trials)\u001B[0m\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective(\n\u001B[1;32m    109\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[1;32m    110\u001B[0m     )\n\u001B[1;32m    112\u001B[0m \u001B[38;5;66;03m# 3. Run the optimization for n_trials\u001B[39;00m\n\u001B[0;32m--> 113\u001B[0m study\u001B[38;5;241m.\u001B[39moptimize(objective_wrapper, n_trials\u001B[38;5;241m=\u001B[39mn_trials)\n\u001B[1;32m    115\u001B[0m \u001B[38;5;66;03m# 4. Print some info about the best trial\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m[OPTUNA] Best trial:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/study.py:475\u001B[0m, in \u001B[0;36mStudy.optimize\u001B[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m    373\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moptimize\u001B[39m(\n\u001B[1;32m    374\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    375\u001B[0m     func: ObjectiveFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    382\u001B[0m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    383\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    384\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[1;32m    385\u001B[0m \n\u001B[1;32m    386\u001B[0m \u001B[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    473\u001B[0m \u001B[38;5;124;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[1;32m    474\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 475\u001B[0m     _optimize(\n\u001B[1;32m    476\u001B[0m         study\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    477\u001B[0m         func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[1;32m    478\u001B[0m         n_trials\u001B[38;5;241m=\u001B[39mn_trials,\n\u001B[1;32m    479\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m    480\u001B[0m         n_jobs\u001B[38;5;241m=\u001B[39mn_jobs,\n\u001B[1;32m    481\u001B[0m         catch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mtuple\u001B[39m(catch) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(catch, Iterable) \u001B[38;5;28;01melse\u001B[39;00m (catch,),\n\u001B[1;32m    482\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mcallbacks,\n\u001B[1;32m    483\u001B[0m         gc_after_trial\u001B[38;5;241m=\u001B[39mgc_after_trial,\n\u001B[1;32m    484\u001B[0m         show_progress_bar\u001B[38;5;241m=\u001B[39mshow_progress_bar,\n\u001B[1;32m    485\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:63\u001B[0m, in \u001B[0;36m_optimize\u001B[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m---> 63\u001B[0m         _optimize_sequential(\n\u001B[1;32m     64\u001B[0m             study,\n\u001B[1;32m     65\u001B[0m             func,\n\u001B[1;32m     66\u001B[0m             n_trials,\n\u001B[1;32m     67\u001B[0m             timeout,\n\u001B[1;32m     68\u001B[0m             catch,\n\u001B[1;32m     69\u001B[0m             callbacks,\n\u001B[1;32m     70\u001B[0m             gc_after_trial,\n\u001B[1;32m     71\u001B[0m             reseed_sampler_rng\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     72\u001B[0m             time_start\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     73\u001B[0m             progress_bar\u001B[38;5;241m=\u001B[39mprogress_bar,\n\u001B[1;32m     74\u001B[0m         )\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     76\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:160\u001B[0m, in \u001B[0;36m_optimize_sequential\u001B[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[1;32m    157\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 160\u001B[0m     frozen_trial \u001B[38;5;241m=\u001B[39m _run_trial(study, func, catch)\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[1;32m    164\u001B[0m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:248\u001B[0m, in \u001B[0;36m_run_trial\u001B[0;34m(study, func, catch)\u001B[0m\n\u001B[1;32m    241\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShould not reach.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    243\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    244\u001B[0m     frozen_trial\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m==\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mFAIL\n\u001B[1;32m    245\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    246\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[1;32m    247\u001B[0m ):\n\u001B[0;32m--> 248\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:197\u001B[0m, in \u001B[0;36m_run_trial\u001B[0;34m(study, func, catch)\u001B[0m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial\u001B[38;5;241m.\u001B[39m_trial_id, study\u001B[38;5;241m.\u001B[39m_storage):\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 197\u001B[0m         value_or_values \u001B[38;5;241m=\u001B[39m func(trial)\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mTrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    199\u001B[0m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[1;32m    200\u001B[0m         state \u001B[38;5;241m=\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mPRUNED\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py:108\u001B[0m, in \u001B[0;36mOptunaSearch.optuna_search_for_env.<locals>.objective_wrapper\u001B[0;34m(trial)\u001B[0m\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mobjective_wrapper\u001B[39m(trial):\n\u001B[0;32m--> 108\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective(\n\u001B[1;32m    109\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[1;32m    110\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py:77\u001B[0m, in \u001B[0;36mOptunaSearch.objective\u001B[0;34m(self, trial)\u001B[0m\n\u001B[1;32m     74\u001B[0m     train_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource_policy_network\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_policy_network\n\u001B[1;32m     75\u001B[0m     train_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource_value_network\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_policy_network\n\u001B[0;32m---> 77\u001B[0m policy_network, value_network, rewards, train_time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_function(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtrain_params)\n\u001B[1;32m     79\u001B[0m \u001B[38;5;66;03m# 3. Compute the metric — e.g. average of the last 100 rewards\u001B[39;00m\n\u001B[1;32m     80\u001B[0m avg_reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(np\u001B[38;5;241m.\u001B[39mmean(rewards[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m100\u001B[39m:]))\n",
      "Cell \u001B[0;32mIn[3], line 31\u001B[0m, in \u001B[0;36mgeneralized_actor_critic\u001B[0;34m(env_name, input_dim, output_dim, hidden_sizes_theta, hidden_sizes_w, alpha_theta, alpha_w, episodes, gamma, log_dir)\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Identify the actual dimensionalities for this env\u001B[39;00m\n\u001B[1;32m     29\u001B[0m actual_act_dim \u001B[38;5;241m=\u001B[39m ENV_ACT_DIM[env_name] \n\u001B[0;32m---> 31\u001B[0m train_time \u001B[38;5;241m=\u001B[39m training_loop(\n\u001B[1;32m     32\u001B[0m     input_dim\u001B[38;5;241m=\u001B[39minput_dim,\n\u001B[1;32m     33\u001B[0m     actual_act_dim\u001B[38;5;241m=\u001B[39mactual_act_dim,\n\u001B[1;32m     34\u001B[0m     policy_network\u001B[38;5;241m=\u001B[39mpolicy_network,\n\u001B[1;32m     35\u001B[0m     value_network\u001B[38;5;241m=\u001B[39mvalue_network,\n\u001B[1;32m     36\u001B[0m     policy_optimizer\u001B[38;5;241m=\u001B[39mpolicy_optimizer,\n\u001B[1;32m     37\u001B[0m     value_optimizer\u001B[38;5;241m=\u001B[39mvalue_optimizer,\n\u001B[1;32m     38\u001B[0m     env\u001B[38;5;241m=\u001B[39menv,\n\u001B[1;32m     39\u001B[0m     env_name\u001B[38;5;241m=\u001B[39menv_name,\n\u001B[1;32m     40\u001B[0m     episodes\u001B[38;5;241m=\u001B[39mepisodes,\n\u001B[1;32m     41\u001B[0m     gamma\u001B[38;5;241m=\u001B[39mgamma,\n\u001B[1;32m     42\u001B[0m     device\u001B[38;5;241m=\u001B[39mdevice,\n\u001B[1;32m     43\u001B[0m     writer\u001B[38;5;241m=\u001B[39mwriter,\n\u001B[1;32m     44\u001B[0m     rewards_per_episode\u001B[38;5;241m=\u001B[39mrewards_per_episode,\n\u001B[1;32m     45\u001B[0m )\n\u001B[1;32m     47\u001B[0m writer\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m     48\u001B[0m env\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/training_loop.py:47\u001B[0m, in \u001B[0;36mtraining_loop\u001B[0;34m(input_dim, actual_act_dim, policy_network, value_network, policy_optimizer, value_optimizer, env, env_name, episodes, gamma, device, writer, rewards_per_episode)\u001B[0m\n\u001B[1;32m     44\u001B[0m log_prob_action \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mlog(action_probs[\u001B[38;5;241m0\u001B[39m, action])\n\u001B[1;32m     46\u001B[0m \u001B[38;5;66;03m# Step in the environment\u001B[39;00m\n\u001B[0;32m---> 47\u001B[0m next_state, reward, done, truncated, _info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action\u001B[38;5;241m.\u001B[39mitem())\n\u001B[1;32m     48\u001B[0m total_reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m reward\n\u001B[1;32m     50\u001B[0m \u001B[38;5;66;03m# Pad next state\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/gymnasium/wrappers/time_limit.py:57\u001B[0m, in \u001B[0;36mTimeLimit.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m     47\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001B[39;00m\n\u001B[1;32m     48\u001B[0m \n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     55\u001B[0m \n\u001B[1;32m     56\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 57\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_episode_steps:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/gymnasium/wrappers/env_checker.py:47\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchecked_step \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchecked_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, action)\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:237\u001B[0m, in \u001B[0;36menv_step_passive_checker\u001B[0;34m(env, action)\u001B[0m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001B[39;00m\n\u001B[1;32m    236\u001B[0m \u001B[38;5;66;03m# We don't check the action as for some environments then out-of-bounds values can be given\u001B[39;00m\n\u001B[0;32m--> 237\u001B[0m result \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[1;32m    239\u001B[0m     result, \u001B[38;5;28mtuple\u001B[39m\n\u001B[1;32m    240\u001B[0m ), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpects step result to be a tuple, actual type: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(result)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    241\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(result) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m4\u001B[39m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/gymnasium/envs/classic_control/continuous_mountain_car.py:149\u001B[0m, in \u001B[0;36mContinuous_MountainCarEnv.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    147\u001B[0m position \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    148\u001B[0m velocity \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m--> 149\u001B[0m force \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(\u001B[38;5;28mmax\u001B[39m(action[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_action), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_action)\n\u001B[1;32m    151\u001B[0m velocity \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m force \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpower \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m0.0025\u001B[39m \u001B[38;5;241m*\u001B[39m math\u001B[38;5;241m.\u001B[39mcos(\u001B[38;5;241m3\u001B[39m \u001B[38;5;241m*\u001B[39m position)\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m velocity \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_speed:\n",
      "\u001B[0;31mTypeError\u001B[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T14:38:49.936985Z",
     "start_time": "2024-12-28T14:38:49.933721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print final summary\n",
    "# TODO: show statistics?\n",
    "for env in overall_results:\n",
    "    print(f\"\\n=== {env} Final Results ===\")\n",
    "    print(f\"  Best Params: {overall_results[env]['best_params']}\")\n",
    "    print(f\"  Best Episodes: {overall_results[env]['best_episodes']}\")\n",
    "    print(f\"  Best Avg Reward: {overall_results[env]['best_reward']:.2f}\")\n",
    "    print(\"  Grid Search Results (gamma, alpha_theta, alpha_w, episodes, avg_reward, train_time_s):\")\n",
    "    for res in overall_results[env][\"grid_results\"]:\n",
    "        print(f\"    {res}\")"
   ],
   "id": "b500c38f38645b13",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "116c610e1b8afab0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
