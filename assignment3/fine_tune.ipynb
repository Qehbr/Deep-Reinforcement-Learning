{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Section 2: Fine-Tuning an Existing Model\n",
   "id": "b2de4d132c27f28f"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-28T14:24:06.682038Z",
     "start_time": "2024-12-28T14:24:06.007947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from assignment3.training_loop import training_loop\n",
    "from models import PolicyNetwork, ValueNetwork\n",
    "from assignment3.dim_alignment import ENV_ACT_DIM, max_input_dim, max_output_dim\n",
    "from assignment3.optuna_search import OptunaSearch"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T14:24:08.118532Z",
     "start_time": "2024-12-28T14:24:08.114957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reinitialize_output_layer(model):\n",
    "    \"\"\"\n",
    "    Re-initialize the final layer's weights and biases of a given nn.Sequential model.\n",
    "    Expects the final layer to be `nn.Linear(..., output_dim)`.\n",
    "    \"\"\"\n",
    "    # model is typically `nn.Sequential([..., nn.Linear(prev_size, output_dim)])`\n",
    "    # So we can directly access the last layer by indexing.\n",
    "    last_layer = model[-1]\n",
    "    if isinstance(last_layer, nn.Linear):\n",
    "        # Re-initialize\n",
    "        nn.init.xavier_uniform_(last_layer.weight)\n",
    "        if last_layer.bias is not None:\n",
    "            nn.init.zeros_(last_layer.bias)\n",
    "    else:\n",
    "        raise ValueError(\"The last layer of the model is not a Linear layer.\")\n"
   ],
   "id": "ec3686f122ba2aa",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T14:24:08.833352Z",
     "start_time": "2024-12-28T14:24:08.828405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fine_tune_actor_critic(\n",
    "    source_policy_network,\n",
    "    source_value_network,\n",
    "    env_name,\n",
    "    input_dim,\n",
    "    output_dim,\n",
    "    hidden_sizes_theta,\n",
    "    hidden_sizes_w,\n",
    "    alpha_theta=0.001,\n",
    "    alpha_w=0.001,\n",
    "    episodes=500,\n",
    "    gamma=0.99,\n",
    "    log_dir=\"runs/fine_tune\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Fine-tune a policy/value network that was trained on another task.\n",
    "    Steps:\n",
    "      1) Re-initialize the final output layer.\n",
    "      2) Train on the target environment.\n",
    "\n",
    "    Returns:\n",
    "      fine_tuned_policy_network, fine_tuned_value_network, rewards_per_episode, train_time\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    env = gym.make(env_name)\n",
    "    writer = SummaryWriter(log_dir=f\"{log_dir}_{env_name}\")\n",
    "\n",
    "    # -- POLICY NETWORK --\n",
    "    fine_tuned_policy_network = PolicyNetwork(input_dim, hidden_sizes_theta, output_dim).to(device)\n",
    "    fine_tuned_policy_network.load_state_dict(source_policy_network.state_dict())  # copy all weights\n",
    "    reinitialize_output_layer(fine_tuned_policy_network.model) # Re-initialize the final layer of the policy network\n",
    "\n",
    "    # -- VALUE NETWORK --\n",
    "    fine_tuned_value_network = ValueNetwork(input_dim, hidden_sizes_w).to(device)\n",
    "    fine_tuned_value_network.load_state_dict(source_value_network.state_dict())  # copy all weights\n",
    "    reinitialize_output_layer(fine_tuned_value_network.model) # Re-initialize the final layer of the value network\n",
    "\n",
    "    policy_optimizer = optim.Adam(fine_tuned_policy_network.parameters(), lr=alpha_theta)\n",
    "    value_optimizer = optim.Adam(fine_tuned_value_network.parameters(), lr=alpha_w)\n",
    "\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    actual_act_dim = ENV_ACT_DIM[env_name]\n",
    "    \n",
    "    train_time = training_loop(\n",
    "        input_dim=input_dim,\n",
    "        actual_act_dim=actual_act_dim,\n",
    "        policy_network=fine_tuned_policy_network,\n",
    "        value_network=fine_tuned_value_network,\n",
    "        policy_optimizer=policy_optimizer,\n",
    "        value_optimizer=value_optimizer,\n",
    "        env=env,\n",
    "        env_name=env_name,\n",
    "        gamma=gamma,\n",
    "        episodes=episodes,\n",
    "        device=device,\n",
    "        writer=writer,\n",
    "        rewards_per_episode=rewards_per_episode,\n",
    "    )\n",
    "    \n",
    "    writer.close()\n",
    "    env.close()\n",
    "\n",
    "    return fine_tuned_policy_network, fine_tuned_value_network, rewards_per_episode, train_time\n"
   ],
   "id": "6b89c50558aaae1a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T14:24:09.454316Z",
     "start_time": "2024-12-28T14:24:09.451706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: to find the best hyperparameters for each environment, initialize different ranges and params for each env separately\n",
    "\n",
    "# Common hidden sizes\n",
    "hidden_sizes_theta = [16, 32, 16]\n",
    "hidden_sizes_w = [16, 32, 16]\n",
    "episodes = 2000\n",
    "n_trials = 10\n",
    "overall_results = {}\n",
    "\n",
    "# Define your search ranges\n",
    "gamma_values = [0.95, 0.99]\n",
    "alpha_theta_values = [0.001, 0.0005]\n",
    "alpha_w_values = [0.001, 0.0005]"
   ],
   "id": "fdea0780511df8e1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T14:24:10.234175Z",
     "start_time": "2024-12-28T14:24:10.231678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_experiment(env_name, source_policy_network, source_value_network):\n",
    "    optuna_search = OptunaSearch(\n",
    "        train_function=fine_tune_actor_critic,\n",
    "        env_name=env_name,\n",
    "        max_input_dim=max_input_dim,\n",
    "        max_output_dim=max_output_dim,\n",
    "        hidden_sizes_theta=hidden_sizes_theta,\n",
    "        hidden_sizes_w=hidden_sizes_w,\n",
    "        gamma_values=gamma_values,\n",
    "        alpha_theta_values=alpha_theta_values,\n",
    "        alpha_w_values=alpha_w_values,\n",
    "        episodes=episodes,\n",
    "        source_policy_network=source_policy_network,\n",
    "        source_value_network=source_value_network,\n",
    "    )\n",
    "    best_policy, best_value, best_params, best_reward, study = optuna_search.optuna_search_for_env(n_trials=n_trials)\n",
    "\n",
    "    print(\"\\nDone! Best parameters found by Optuna:\", best_params)\n",
    "    print(\"Best reward from Optuna:\", best_reward)\n",
    "\n",
    "\n",
    "# save networks to pretrained_models\n",
    "    torch.save(best_policy.state_dict(), f\"pretrained_models/fine_tuned_{env_name}_policy.pth\")\n",
    "    torch.save(best_value.state_dict(), f\"pretrained_models/fine_tuned_{env_name}_value.pth\")"
   ],
   "id": "57ba25b6fedb569",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T14:24:11.428032Z",
     "start_time": "2024-12-28T14:24:11.419892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: annoying future warnings, fix them?\n",
    "# Load the pre-trained models\n",
    "policy_acrobot = PolicyNetwork(max_input_dim, hidden_sizes_theta, max_output_dim)\n",
    "policy_acrobot.load_state_dict(torch.load(\"pretrained_models/acrobot-v1_policy.pth\"))\n",
    "value_acrobot = ValueNetwork(max_input_dim, hidden_sizes_w)\n",
    "value_acrobot.load_state_dict(torch.load(\"pretrained_models/acrobot-v1_value.pth\"))\n"
   ],
   "id": "b34af181521e6965",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/94wmcf8921379lkml24q3wbr0000gn/T/ipykernel_59272/2335984660.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy_acrobot.load_state_dict(torch.load(\"pretrained_models/acrobot-v1_policy.pth\"))\n",
      "/var/folders/q7/94wmcf8921379lkml24q3wbr0000gn/T/ipykernel_59272/2335984660.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  value_acrobot.load_state_dict(torch.load(\"pretrained_models/acrobot-v1_value.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T14:24:13.624859Z",
     "start_time": "2024-12-28T14:24:13.279175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "run_experiment(\"Acrobot-v1\", policy_acrobot, value_acrobot)\n",
    "# TODO: load_state_dict() is not working because of dim mismatch"
   ],
   "id": "b3d9221e35d41482",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-28 16:24:13,280] A new study created in memory with name: no-name-6371784a-c3f5-4500-abf1-b964ed4e5df3\n",
      "[W 2024-12-28 16:24:13,287] Trial 0 failed with parameters: {'gamma': 0.99, 'alpha_theta': 0.001, 'alpha_w': 0.001} because of the following error: RuntimeError('Error(s) in loading state_dict for ValueNetwork:\\n\\tsize mismatch for model.6.weight: copying a param with shape torch.Size([3, 16]) from checkpoint, the shape in current model is torch.Size([1, 16]).\\n\\tsize mismatch for model.6.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([1]).').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/Users/nadav/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py\", line 108, in objective_wrapper\n",
      "    return self.objective(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nadav/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py\", line 77, in objective\n",
      "    policy_network, value_network, rewards, train_time = self.train_function(**train_params)\n",
      "                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/q7/94wmcf8921379lkml24q3wbr0000gn/T/ipykernel_59272/3201584544.py\", line 35, in fine_tune_actor_critic\n",
      "    fine_tuned_value_network.load_state_dict(source_value_network.state_dict())  # copy all weights\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 2584, in load_state_dict\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Error(s) in loading state_dict for ValueNetwork:\n",
      "\tsize mismatch for model.6.weight: copying a param with shape torch.Size([3, 16]) from checkpoint, the shape in current model is torch.Size([1, 16]).\n",
      "\tsize mismatch for model.6.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([1]).\n",
      "[W 2024-12-28 16:24:13,288] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial] Env=Acrobot-v1 | gamma=0.99, alpha_theta=0.001, alpha_w=0.001\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ValueNetwork:\n\tsize mismatch for model.6.weight: copying a param with shape torch.Size([3, 16]) from checkpoint, the shape in current model is torch.Size([1, 16]).\n\tsize mismatch for model.6.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([1]).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m run_experiment(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAcrobot-v1\u001B[39m\u001B[38;5;124m\"\u001B[39m, policy_acrobot, value_acrobot)\n",
      "Cell \u001B[0;32mIn[5], line 16\u001B[0m, in \u001B[0;36mrun_experiment\u001B[0;34m(env_name, source_policy_network, source_value_network)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_experiment\u001B[39m(env_name, source_policy_network, source_value_network):\n\u001B[1;32m      2\u001B[0m     optuna_search \u001B[38;5;241m=\u001B[39m OptunaSearch(\n\u001B[1;32m      3\u001B[0m         train_function\u001B[38;5;241m=\u001B[39mfine_tune_actor_critic,\n\u001B[1;32m      4\u001B[0m         env_name\u001B[38;5;241m=\u001B[39menv_name,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     14\u001B[0m         source_value_network\u001B[38;5;241m=\u001B[39msource_value_network,\n\u001B[1;32m     15\u001B[0m     )\n\u001B[0;32m---> 16\u001B[0m     best_policy, best_value, best_params, best_reward, study \u001B[38;5;241m=\u001B[39m optuna_search\u001B[38;5;241m.\u001B[39moptuna_search_for_env(n_trials\u001B[38;5;241m=\u001B[39mn_trials)\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mDone! Best parameters found by Optuna:\u001B[39m\u001B[38;5;124m\"\u001B[39m, best_params)\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest reward from Optuna:\u001B[39m\u001B[38;5;124m\"\u001B[39m, best_reward)\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py:113\u001B[0m, in \u001B[0;36mOptunaSearch.optuna_search_for_env\u001B[0;34m(self, n_trials)\u001B[0m\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective(\n\u001B[1;32m    109\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[1;32m    110\u001B[0m     )\n\u001B[1;32m    112\u001B[0m \u001B[38;5;66;03m# 3. Run the optimization for n_trials\u001B[39;00m\n\u001B[0;32m--> 113\u001B[0m study\u001B[38;5;241m.\u001B[39moptimize(objective_wrapper, n_trials\u001B[38;5;241m=\u001B[39mn_trials)\n\u001B[1;32m    115\u001B[0m \u001B[38;5;66;03m# 4. Print some info about the best trial\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m[OPTUNA] Best trial:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/study.py:475\u001B[0m, in \u001B[0;36mStudy.optimize\u001B[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m    373\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moptimize\u001B[39m(\n\u001B[1;32m    374\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    375\u001B[0m     func: ObjectiveFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    382\u001B[0m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    383\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    384\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[1;32m    385\u001B[0m \n\u001B[1;32m    386\u001B[0m \u001B[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    473\u001B[0m \u001B[38;5;124;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[1;32m    474\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 475\u001B[0m     _optimize(\n\u001B[1;32m    476\u001B[0m         study\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    477\u001B[0m         func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[1;32m    478\u001B[0m         n_trials\u001B[38;5;241m=\u001B[39mn_trials,\n\u001B[1;32m    479\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m    480\u001B[0m         n_jobs\u001B[38;5;241m=\u001B[39mn_jobs,\n\u001B[1;32m    481\u001B[0m         catch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mtuple\u001B[39m(catch) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(catch, Iterable) \u001B[38;5;28;01melse\u001B[39;00m (catch,),\n\u001B[1;32m    482\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mcallbacks,\n\u001B[1;32m    483\u001B[0m         gc_after_trial\u001B[38;5;241m=\u001B[39mgc_after_trial,\n\u001B[1;32m    484\u001B[0m         show_progress_bar\u001B[38;5;241m=\u001B[39mshow_progress_bar,\n\u001B[1;32m    485\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:63\u001B[0m, in \u001B[0;36m_optimize\u001B[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m---> 63\u001B[0m         _optimize_sequential(\n\u001B[1;32m     64\u001B[0m             study,\n\u001B[1;32m     65\u001B[0m             func,\n\u001B[1;32m     66\u001B[0m             n_trials,\n\u001B[1;32m     67\u001B[0m             timeout,\n\u001B[1;32m     68\u001B[0m             catch,\n\u001B[1;32m     69\u001B[0m             callbacks,\n\u001B[1;32m     70\u001B[0m             gc_after_trial,\n\u001B[1;32m     71\u001B[0m             reseed_sampler_rng\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     72\u001B[0m             time_start\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     73\u001B[0m             progress_bar\u001B[38;5;241m=\u001B[39mprogress_bar,\n\u001B[1;32m     74\u001B[0m         )\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     76\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:160\u001B[0m, in \u001B[0;36m_optimize_sequential\u001B[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[1;32m    157\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 160\u001B[0m     frozen_trial \u001B[38;5;241m=\u001B[39m _run_trial(study, func, catch)\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[1;32m    164\u001B[0m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:248\u001B[0m, in \u001B[0;36m_run_trial\u001B[0;34m(study, func, catch)\u001B[0m\n\u001B[1;32m    241\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShould not reach.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    243\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    244\u001B[0m     frozen_trial\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m==\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mFAIL\n\u001B[1;32m    245\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    246\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[1;32m    247\u001B[0m ):\n\u001B[0;32m--> 248\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:197\u001B[0m, in \u001B[0;36m_run_trial\u001B[0;34m(study, func, catch)\u001B[0m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial\u001B[38;5;241m.\u001B[39m_trial_id, study\u001B[38;5;241m.\u001B[39m_storage):\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 197\u001B[0m         value_or_values \u001B[38;5;241m=\u001B[39m func(trial)\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mTrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    199\u001B[0m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[1;32m    200\u001B[0m         state \u001B[38;5;241m=\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mPRUNED\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py:108\u001B[0m, in \u001B[0;36mOptunaSearch.optuna_search_for_env.<locals>.objective_wrapper\u001B[0;34m(trial)\u001B[0m\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mobjective_wrapper\u001B[39m(trial):\n\u001B[0;32m--> 108\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective(\n\u001B[1;32m    109\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[1;32m    110\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py:77\u001B[0m, in \u001B[0;36mOptunaSearch.objective\u001B[0;34m(self, trial)\u001B[0m\n\u001B[1;32m     74\u001B[0m     train_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource_policy_network\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_policy_network\n\u001B[1;32m     75\u001B[0m     train_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource_value_network\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_policy_network\n\u001B[0;32m---> 77\u001B[0m policy_network, value_network, rewards, train_time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_function(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtrain_params)\n\u001B[1;32m     79\u001B[0m \u001B[38;5;66;03m# 3. Compute the metric â€” e.g. average of the last 100 rewards\u001B[39;00m\n\u001B[1;32m     80\u001B[0m avg_reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(np\u001B[38;5;241m.\u001B[39mmean(rewards[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m100\u001B[39m:]))\n",
      "Cell \u001B[0;32mIn[3], line 35\u001B[0m, in \u001B[0;36mfine_tune_actor_critic\u001B[0;34m(source_policy_network, source_value_network, env_name, input_dim, output_dim, hidden_sizes_theta, hidden_sizes_w, alpha_theta, alpha_w, episodes, gamma, log_dir)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# -- VALUE NETWORK --\u001B[39;00m\n\u001B[1;32m     34\u001B[0m fine_tuned_value_network \u001B[38;5;241m=\u001B[39m ValueNetwork(input_dim, hidden_sizes_w)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 35\u001B[0m fine_tuned_value_network\u001B[38;5;241m.\u001B[39mload_state_dict(source_value_network\u001B[38;5;241m.\u001B[39mstate_dict())  \u001B[38;5;66;03m# copy all weights\u001B[39;00m\n\u001B[1;32m     36\u001B[0m reinitialize_output_layer(fine_tuned_value_network\u001B[38;5;241m.\u001B[39mmodel) \u001B[38;5;66;03m# Re-initialize the final layer of the value network\u001B[39;00m\n\u001B[1;32m     38\u001B[0m policy_optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(fine_tuned_policy_network\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39malpha_theta)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/nn/modules/module.py:2584\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[0;34m(self, state_dict, strict, assign)\u001B[0m\n\u001B[1;32m   2576\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[1;32m   2577\u001B[0m             \u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m   2578\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2579\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)\n\u001B[1;32m   2580\u001B[0m             ),\n\u001B[1;32m   2581\u001B[0m         )\n\u001B[1;32m   2583\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 2584\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   2585\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2586\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)\n\u001B[1;32m   2587\u001B[0m         )\n\u001B[1;32m   2588\u001B[0m     )\n\u001B[1;32m   2589\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for ValueNetwork:\n\tsize mismatch for model.6.weight: copying a param with shape torch.Size([3, 16]) from checkpoint, the shape in current model is torch.Size([1, 16]).\n\tsize mismatch for model.6.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([1])."
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# TODO: cant load model, fix it\n",
    "policy_cartpole = PolicyNetwork(max_input_dim, hidden_sizes_theta, max_output_dim)\n",
    "policy_cartpole.load_state_dict(torch.load(\"pretrained_models/cartpole-v1_policy.pth\"))\n",
    "value_cartpole = ValueNetwork(max_input_dim, hidden_sizes_w)\n",
    "value_cartpole.load_state_dict(torch.load(\"pretrained_models/cartpole-v1_value.pth\"))"
   ],
   "id": "15cf55690fdbf5e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T14:09:43.789183Z",
     "start_time": "2024-12-28T14:09:43.774496Z"
    }
   },
   "cell_type": "code",
   "source": "run_experiment(\"CartPole-v1\", policy_cartpole, value_cartpole)",
   "id": "dd7f93685cb96dc1",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'value_cartpole' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# CartPole -> MountainCar\u001B[39;00m\n\u001B[1;32m      2\u001B[0m fine_tuned_mountain_car_policy, fine_tuned_mountain_car_value, fine_tune_mountain_car_rewards, fine_tune_mountain_car_time \u001B[38;5;241m=\u001B[39m fine_tune_actor_critic(\n\u001B[1;32m      3\u001B[0m     source_policy_network\u001B[38;5;241m=\u001B[39mpolicy_cartpole,\n\u001B[0;32m----> 4\u001B[0m     source_value_network\u001B[38;5;241m=\u001B[39mvalue_cartpole,\n\u001B[1;32m      5\u001B[0m     target_env_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMountainCarContinuous-v0\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      6\u001B[0m     input_dim\u001B[38;5;241m=\u001B[39mmax_input_dim,\n\u001B[1;32m      7\u001B[0m     output_dim\u001B[38;5;241m=\u001B[39mmax_output_dim,\n\u001B[1;32m      8\u001B[0m     hidden_sizes_theta\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m64\u001B[39m],\n\u001B[1;32m      9\u001B[0m     hidden_sizes_w\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m64\u001B[39m],\n\u001B[1;32m     10\u001B[0m     alpha_theta\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m,\n\u001B[1;32m     11\u001B[0m     alpha_w\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m,\n\u001B[1;32m     12\u001B[0m     episodes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m,\n\u001B[1;32m     13\u001B[0m     gamma\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.99\u001B[39m,\n\u001B[1;32m     14\u001B[0m     log_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mruns/fine_tune_cartpole_mountaincar\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     15\u001B[0m )\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCartPole -> MountainCar fine-tuning done.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumber of episodes trained:\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mlen\u001B[39m(fine_tune_mountain_car_rewards))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'value_cartpole' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8e31d7236d02796e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
