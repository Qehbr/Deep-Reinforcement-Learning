{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Section 2: Fine-Tuning an Existing Model\n",
   "id": "b2de4d132c27f28f"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-29T19:53:49.374292Z",
     "start_time": "2024-12-29T19:53:48.402768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import json\n",
    "from assignment3.device import get_device\n",
    "from assignment3.training_loop import training_loop\n",
    "from models import PolicyNetwork, ValueNetwork\n",
    "from assignment3.dim_alignment import ENV_ACT_DIM, max_input_dim, max_output_dim\n",
    "from assignment3.optuna_search import OptunaSearch, StudyFloatParamRange"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T19:53:49.782417Z",
     "start_time": "2024-12-29T19:53:49.779556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reinitialize_output_layer(model):\n",
    "    \"\"\"\n",
    "    Re-initialize the final layer's weights and biases of a given nn.Sequential model.\n",
    "    Expects the final layer to be `nn.Linear(..., output_dim)`.\n",
    "    \"\"\"\n",
    "    # model is typically `nn.Sequential([..., nn.Linear(prev_size, output_dim)])`\n",
    "    # So we can directly access the last layer by indexing.\n",
    "    last_layer = model[-1]\n",
    "    if isinstance(last_layer, nn.Linear):\n",
    "        # Re-initialize\n",
    "        nn.init.xavier_uniform_(last_layer.weight)\n",
    "        if last_layer.bias is not None:\n",
    "            nn.init.zeros_(last_layer.bias)\n",
    "    else:\n",
    "        raise ValueError(\"The last layer of the model is not a Linear layer.\")\n"
   ],
   "id": "ec3686f122ba2aa",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T19:53:50.262305Z",
     "start_time": "2024-12-29T19:53:50.257522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fine_tune_actor_critic(\n",
    "    source_policy_network,\n",
    "    source_value_network,\n",
    "    env_name,\n",
    "    input_dim,\n",
    "    output_dim,\n",
    "    hidden_sizes_theta,\n",
    "    hidden_sizes_w,\n",
    "    dropout_layers = None,\n",
    "    alpha_theta=0.001,\n",
    "    alpha_w=0.001,\n",
    "    episodes=500,\n",
    "    gamma=0.99,\n",
    "    dropout_p=0.7,\n",
    "    log_dir=\"runs/fine_tune\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Fine-tune a policy/value network that was trained on another task.\n",
    "    Steps:\n",
    "      1) Re-initialize the final output layer.\n",
    "      2) Train on the target environment.\n",
    "\n",
    "    Returns:\n",
    "      fine_tuned_policy_network, fine_tuned_value_network, rewards_per_episode, train_time\n",
    "    \"\"\"\n",
    "    device = get_device()\n",
    "    env = gym.make(env_name)\n",
    "    writer = SummaryWriter(log_dir=f\"{log_dir}_{env_name}\")\n",
    "\n",
    "    # -- POLICY NETWORK --\n",
    "    fine_tuned_policy_network = PolicyNetwork(input_dim, hidden_sizes_theta, output_dim, dropout_layers, dropout_p).to(device)\n",
    "    fine_tuned_policy_network.load_state_dict(source_policy_network.state_dict())  # copy all weights\n",
    "    reinitialize_output_layer(fine_tuned_policy_network.model) # Re-initialize the final layer of the policy network\n",
    "\n",
    "    # -- VALUE NETWORK --\n",
    "    fine_tuned_value_network = ValueNetwork(input_dim, hidden_sizes_w).to(device)\n",
    "    fine_tuned_value_network.load_state_dict(source_value_network.state_dict())  # copy all weights\n",
    "    reinitialize_output_layer(fine_tuned_value_network.model) # Re-initialize the final layer of the value network\n",
    "\n",
    "    policy_optimizer = optim.Adam(fine_tuned_policy_network.parameters(), lr=alpha_theta)\n",
    "    value_optimizer = optim.Adam(fine_tuned_value_network.parameters(), lr=alpha_w)\n",
    "\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    actual_act_dim = ENV_ACT_DIM[env_name]\n",
    "    \n",
    "    train_time = training_loop(\n",
    "        input_dim=input_dim,\n",
    "        actual_act_dim=actual_act_dim,\n",
    "        policy_network=fine_tuned_policy_network,\n",
    "        value_network=fine_tuned_value_network,\n",
    "        policy_optimizer=policy_optimizer,\n",
    "        value_optimizer=value_optimizer,\n",
    "        env=env,\n",
    "        env_name=env_name,\n",
    "        gamma=gamma,\n",
    "        episodes=episodes,\n",
    "        writer=writer,\n",
    "        rewards_per_episode=rewards_per_episode,\n",
    "    )\n",
    "    \n",
    "    writer.close()\n",
    "    env.close()\n",
    "\n",
    "    return fine_tuned_policy_network, fine_tuned_value_network, rewards_per_episode, train_time\n"
   ],
   "id": "6b89c50558aaae1a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T19:53:50.928400Z",
     "start_time": "2024-12-29T19:53:50.924929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: to find the best hyperparameters for each environment, initialize different ranges and params for each env separately\n",
    "\n",
    "episodes = 2000\n",
    "n_trials = 10\n",
    "overall_results = {}\n",
    "\n",
    "# Common hidden sizes options\n",
    "hidden_sizes_theta_values = [\"[16, 32, 16]\", \"[32, 64, 32]\"]\n",
    "hidden_sizes_w_values = [\"[16, 32, 16]\", \"[32, 64, 32]\"]\n",
    "dropout_layers = [1]\n",
    "\n",
    "# Define your search ranges\n",
    "gamma_values = StudyFloatParamRange(low=0.95, high=0.99, step=0.01)\n",
    "alpha_theta_values = StudyFloatParamRange(low=0.0005, high=0.0008, step=0.0001)\n",
    "alpha_w_values = StudyFloatParamRange(low=0.0005, high=0.0008, step=0.0001)\n",
    "dropout_p_values = StudyFloatParamRange(low=0.2, high=0.5, step=0.1)"
   ],
   "id": "fdea0780511df8e1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T19:53:51.874370Z",
     "start_time": "2024-12-29T19:53:51.871216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_experiment(env_name, source_policy_network, source_value_network, fixed_hidden_theta, fixed_hidden_w):\n",
    "    optuna_search = OptunaSearch(\n",
    "        train_function=fine_tune_actor_critic,\n",
    "        env_name=env_name,\n",
    "        max_input_dim=max_input_dim,\n",
    "        max_output_dim=max_output_dim,\n",
    "        hidden_sizes_theta_values=hidden_sizes_theta_values,\n",
    "        hidden_sizes_w_values=hidden_sizes_theta_values,\n",
    "        dropout_layers=dropout_layers,\n",
    "        gamma_values=gamma_values,\n",
    "        alpha_theta_values=alpha_theta_values,\n",
    "        alpha_w_values=alpha_w_values,\n",
    "        dropout_p_values=dropout_p_values,\n",
    "        episodes=episodes,\n",
    "    )\n",
    "    best_policy, best_value, best_params, best_reward, study = optuna_search.optuna_search_for_env(\n",
    "        n_trials=n_trials,\n",
    "        source_policy_network=source_policy_network,\n",
    "        source_value_network=source_value_network,\n",
    "        fixed_hidden_theta=fixed_hidden_theta,\n",
    "        fixed_hidden_w=fixed_hidden_w,\n",
    "    )\n",
    "\n",
    "    print(\"\\nDone! Best parameters found by Optuna:\", best_params)\n",
    "    print(\"Best reward from Optuna:\", best_reward)\n",
    "\n",
    "\n",
    "# save networks to pretrained_models\n",
    "    torch.save(best_policy.state_dict(), f\"pretrained_models/fine_tuned_{env_name}_policy.pth\")\n",
    "    torch.save(best_value.state_dict(), f\"pretrained_models/fine_tuned_{env_name}_value.pth\")"
   ],
   "id": "57ba25b6fedb569",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T19:58:32.239462Z",
     "start_time": "2024-12-29T19:58:32.233005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read hidden sizes from the best hyperparameters found by Optuna from json file\n",
    "with open(\"best_params/acrobot-v1_actor_critic_study.json\", \"r\") as f:\n",
    "    acrobot_hyperparameters = json.load(f)\n",
    "    acrobot_hidden_sizes_theta = eval(acrobot_hyperparameters[\"hidden_sizes_theta\"])\n",
    "    acrobot_hidden_sizes_w = eval(acrobot_hyperparameters[\"hidden_sizes_w\"])\n",
    "    acrobot_dropout_p = acrobot_hyperparameters[\"dropout_p\"]\n",
    "    print(acrobot_hidden_sizes_theta, acrobot_hidden_sizes_w)"
   ],
   "id": "73b683ec38512e97",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 64, 32] [16, 32, 16]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T19:58:38.062096Z",
     "start_time": "2024-12-29T19:58:38.050409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: annoying future warnings, fix them?\n",
    "# Load the pre-trained models\n",
    "policy_acrobot = PolicyNetwork(max_input_dim, acrobot_hidden_sizes_theta, max_output_dim, dropout_layers, acrobot_dropout_p)\n",
    "policy_acrobot.load_state_dict(torch.load(\"pretrained_models/acrobot-v1_policy.pth\"))\n",
    "value_acrobot = ValueNetwork(max_input_dim, acrobot_hidden_sizes_w)\n",
    "value_acrobot.load_state_dict(torch.load(\"pretrained_models/acrobot-v1_value.pth\"))\n"
   ],
   "id": "b34af181521e6965",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/94wmcf8921379lkml24q3wbr0000gn/T/ipykernel_70257/1758889256.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy_acrobot.load_state_dict(torch.load(\"pretrained_models/acrobot-v1_policy.pth\"))\n",
      "/var/folders/q7/94wmcf8921379lkml24q3wbr0000gn/T/ipykernel_70257/1758889256.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  value_acrobot.load_state_dict(torch.load(\"pretrained_models/acrobot-v1_value.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T20:06:37.037634Z",
     "start_time": "2024-12-29T19:59:40.749232Z"
    }
   },
   "cell_type": "code",
   "source": "run_experiment(\"CartPole-v1\", policy_acrobot, value_acrobot, acrobot_hidden_sizes_theta, acrobot_hidden_sizes_w)",
   "id": "b3d9221e35d41482",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-29 21:59:40,750] A new study created in memory with name: CartPole-v1_study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 0] Env=CartPole-v1:\n",
      "        hidden_sizes_theta=[32, 64, 32], hidden_sizes_w=[16, 32, 16],\n",
      "         gamma=0.97, dropout_p=0.30000000000000004,\n",
      "         alpha_theta=0.0008, alpha_w=0.0006000000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2000/2000 [00:08<00:00, 233.05episode/s, Avg Reward(100)=9.37]\n",
      "[I 2024-12-29 21:59:49,340] Trial 0 finished with value: 2000.0 and parameters: {'hidden_sizes_theta': '[16, 32, 16]', 'hidden_sizes_w': '[32, 64, 32]', 'gamma': 0.97, 'alpha_theta': 0.0008, 'alpha_w': 0.0006000000000000001, 'dropout_p': 0.30000000000000004}. Best is trial 0 with value: 2000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 1] Env=CartPole-v1:\n",
      "        hidden_sizes_theta=[32, 64, 32], hidden_sizes_w=[16, 32, 16],\n",
      "         gamma=0.95, dropout_p=0.30000000000000004,\n",
      "         alpha_theta=0.0006000000000000001, alpha_w=0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2000/2000 [02:14<00:00, 14.92episode/s, Avg Reward(100)=236.10]\n",
      "[I 2024-12-29 22:02:03,406] Trial 1 finished with value: 2000.0 and parameters: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[16, 32, 16]', 'gamma': 0.95, 'alpha_theta': 0.0006000000000000001, 'alpha_w': 0.0008, 'dropout_p': 0.30000000000000004}. Best is trial 0 with value: 2000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 2] Env=CartPole-v1:\n",
      "        hidden_sizes_theta=[32, 64, 32], hidden_sizes_w=[16, 32, 16],\n",
      "         gamma=0.97, dropout_p=0.2,\n",
      "         alpha_theta=0.0005, alpha_w=0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2000/2000 [00:38<00:00, 52.02episode/s, Avg Reward(100)=9.20]  \n",
      "[I 2024-12-29 22:02:41,854] Trial 2 finished with value: 2000.0 and parameters: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[32, 64, 32]', 'gamma': 0.97, 'alpha_theta': 0.0005, 'alpha_w': 0.0007, 'dropout_p': 0.2}. Best is trial 0 with value: 2000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 3] Env=CartPole-v1:\n",
      "        hidden_sizes_theta=[32, 64, 32], hidden_sizes_w=[16, 32, 16],\n",
      "         gamma=0.96, dropout_p=0.4,\n",
      "         alpha_theta=0.0008, alpha_w=0.0006000000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2000/2000 [00:14<00:00, 134.10episode/s, Avg Reward(100)=9.37]\n",
      "[I 2024-12-29 22:02:56,771] Trial 3 finished with value: 2000.0 and parameters: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[16, 32, 16]', 'gamma': 0.96, 'alpha_theta': 0.0008, 'alpha_w': 0.0006000000000000001, 'dropout_p': 0.4}. Best is trial 0 with value: 2000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 4] Env=CartPole-v1:\n",
      "        hidden_sizes_theta=[32, 64, 32], hidden_sizes_w=[16, 32, 16],\n",
      "         gamma=0.97, dropout_p=0.30000000000000004,\n",
      "         alpha_theta=0.0008, alpha_w=0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2000/2000 [00:10<00:00, 197.45episode/s, Avg Reward(100)=9.30]\n",
      "[I 2024-12-29 22:03:06,903] Trial 4 finished with value: 2000.0 and parameters: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[32, 64, 32]', 'gamma': 0.97, 'alpha_theta': 0.0008, 'alpha_w': 0.0005, 'dropout_p': 0.30000000000000004}. Best is trial 0 with value: 2000.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 5] Env=CartPole-v1:\n",
      "        hidden_sizes_theta=[32, 64, 32], hidden_sizes_w=[16, 32, 16],\n",
      "         gamma=0.98, dropout_p=0.30000000000000004,\n",
      "         alpha_theta=0.0005, alpha_w=0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▌| 1900/2000 [01:31<00:04, 20.73episode/s, Avg Reward(100)=474.45]\n",
      "[I 2024-12-29 22:04:38,571] Trial 5 finished with value: 1901.0 and parameters: {'hidden_sizes_theta': '[16, 32, 16]', 'hidden_sizes_w': '[16, 32, 16]', 'gamma': 0.98, 'alpha_theta': 0.0005, 'alpha_w': 0.0005, 'dropout_p': 0.30000000000000004}. Best is trial 5 with value: 1901.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved CartPole-v1 in 1901 episodes!\n",
      "\n",
      "[OPTUNA Trial 6] Env=CartPole-v1:\n",
      "        hidden_sizes_theta=[32, 64, 32], hidden_sizes_w=[16, 32, 16],\n",
      "         gamma=0.98, dropout_p=0.5,\n",
      "         alpha_theta=0.0006000000000000001, alpha_w=0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▊| 1974/2000 [00:57<00:00, 34.29episode/s, Avg Reward(100)=316.02]\n",
      "[I 2024-12-29 22:05:36,144] Trial 6 finished with value: 1975.0 and parameters: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[16, 32, 16]', 'gamma': 0.98, 'alpha_theta': 0.0006000000000000001, 'alpha_w': 0.0008, 'dropout_p': 0.5}. Best is trial 5 with value: 1901.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved CartPole-v1 in 1975 episodes!\n",
      "\n",
      "[OPTUNA Trial 7] Env=CartPole-v1:\n",
      "        hidden_sizes_theta=[32, 64, 32], hidden_sizes_w=[16, 32, 16],\n",
      "         gamma=0.98, dropout_p=0.4,\n",
      "         alpha_theta=0.0007, alpha_w=0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2000/2000 [00:10<00:00, 189.33episode/s, Avg Reward(100)=48.27]\n",
      "[I 2024-12-29 22:05:46,711] Trial 7 finished with value: 2000.0 and parameters: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[32, 64, 32]', 'gamma': 0.98, 'alpha_theta': 0.0007, 'alpha_w': 0.0008, 'dropout_p': 0.4}. Best is trial 5 with value: 1901.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 8] Env=CartPole-v1:\n",
      "        hidden_sizes_theta=[32, 64, 32], hidden_sizes_w=[16, 32, 16],\n",
      "         gamma=0.96, dropout_p=0.30000000000000004,\n",
      "         alpha_theta=0.0008, alpha_w=0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2000/2000 [00:08<00:00, 228.11episode/s, Avg Reward(100)=9.40]\n",
      "[I 2024-12-29 22:05:55,482] Trial 8 finished with value: 2000.0 and parameters: {'hidden_sizes_theta': '[16, 32, 16]', 'hidden_sizes_w': '[32, 64, 32]', 'gamma': 0.96, 'alpha_theta': 0.0008, 'alpha_w': 0.0008, 'dropout_p': 0.30000000000000004}. Best is trial 5 with value: 1901.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 9] Env=CartPole-v1:\n",
      "        hidden_sizes_theta=[32, 64, 32], hidden_sizes_w=[16, 32, 16],\n",
      "         gamma=0.96, dropout_p=0.5,\n",
      "         alpha_theta=0.0008, alpha_w=0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2000/2000 [00:41<00:00, 48.17episode/s, Avg Reward(100)=9.44]   \n",
      "[I 2024-12-29 22:06:37,004] Trial 9 finished with value: 2000.0 and parameters: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[32, 64, 32]', 'gamma': 0.96, 'alpha_theta': 0.0008, 'alpha_w': 0.0008, 'dropout_p': 0.5}. Best is trial 5 with value: 1901.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA] Best trial: trail 5\n",
      "  Value (Reward): 1901.00\n",
      "  Params: {'hidden_sizes_theta': '[16, 32, 16]', 'hidden_sizes_w': '[16, 32, 16]', 'gamma': 0.98, 'alpha_theta': 0.0005, 'alpha_w': 0.0005, 'dropout_p': 0.30000000000000004}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "eval() arg 1 must be a string, bytes or code object",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m run_experiment(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCartPole-v1\u001B[39m\u001B[38;5;124m\"\u001B[39m, policy_acrobot, value_acrobot, acrobot_hidden_sizes_theta, acrobot_hidden_sizes_w)\n",
      "Cell \u001B[0;32mIn[5], line 16\u001B[0m, in \u001B[0;36mrun_experiment\u001B[0;34m(env_name, source_policy_network, source_value_network, fixed_hidden_theta, fixed_hidden_w)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_experiment\u001B[39m(env_name, source_policy_network, source_value_network, fixed_hidden_theta, fixed_hidden_w):\n\u001B[1;32m      2\u001B[0m     optuna_search \u001B[38;5;241m=\u001B[39m OptunaSearch(\n\u001B[1;32m      3\u001B[0m         train_function\u001B[38;5;241m=\u001B[39mfine_tune_actor_critic,\n\u001B[1;32m      4\u001B[0m         env_name\u001B[38;5;241m=\u001B[39menv_name,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     14\u001B[0m         episodes\u001B[38;5;241m=\u001B[39mepisodes,\n\u001B[1;32m     15\u001B[0m     )\n\u001B[0;32m---> 16\u001B[0m     best_policy, best_value, best_params, best_reward, study \u001B[38;5;241m=\u001B[39m optuna_search\u001B[38;5;241m.\u001B[39moptuna_search_for_env(\n\u001B[1;32m     17\u001B[0m         n_trials\u001B[38;5;241m=\u001B[39mn_trials,\n\u001B[1;32m     18\u001B[0m         source_policy_network\u001B[38;5;241m=\u001B[39msource_policy_network,\n\u001B[1;32m     19\u001B[0m         source_value_network\u001B[38;5;241m=\u001B[39msource_value_network,\n\u001B[1;32m     20\u001B[0m         fixed_hidden_theta\u001B[38;5;241m=\u001B[39mfixed_hidden_theta,\n\u001B[1;32m     21\u001B[0m         fixed_hidden_w\u001B[38;5;241m=\u001B[39mfixed_hidden_w,\n\u001B[1;32m     22\u001B[0m     )\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mDone! Best parameters found by Optuna:\u001B[39m\u001B[38;5;124m\"\u001B[39m, best_params)\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest reward from Optuna:\u001B[39m\u001B[38;5;124m\"\u001B[39m, best_reward)\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py:179\u001B[0m, in \u001B[0;36mOptunaSearch.optuna_search_for_env\u001B[0;34m(self, n_trials, study_name, source_policy_network, source_value_network, fixed_hidden_theta, fixed_hidden_w)\u001B[0m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fixed_hidden_w \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    177\u001B[0m     best_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhidden_sizes_w\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m fixed_hidden_w\n\u001B[0;32m--> 179\u001B[0m hidden_sizes_theta \u001B[38;5;241m=\u001B[39m \u001B[38;5;28meval\u001B[39m(best_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhidden_sizes_theta\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    180\u001B[0m hidden_sizes_w \u001B[38;5;241m=\u001B[39m \u001B[38;5;28meval\u001B[39m(best_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhidden_sizes_w\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    181\u001B[0m gamma_opt \u001B[38;5;241m=\u001B[39m best_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgamma\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "\u001B[0;31mTypeError\u001B[0m: eval() arg 1 must be a string, bytes or code object"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T20:10:30.226906Z",
     "start_time": "2024-12-29T20:10:30.217330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read hidden sizes from the best hyperparameters found by Optuna from json file\n",
    "with open(\"best_params/cartpole-v1_actor_critic_study.json\", \"r\") as f:\n",
    "    cartpole_hyperparameters = json.load(f)\n",
    "    cartpole_hidden_sizes_theta = eval(cartpole_hyperparameters[\"hidden_sizes_theta\"])\n",
    "    cartpole_hidden_sizes_w = eval(cartpole_hyperparameters[\"hidden_sizes_w\"])\n",
    "    cartpole_dropout_p = cartpole_hyperparameters[\"dropout_p\"]\n",
    "    print(cartpole_hidden_sizes_theta, cartpole_hidden_sizes_w)"
   ],
   "id": "798040202237be28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 32, 16] [32, 64, 32]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T20:10:31.216259Z",
     "start_time": "2024-12-29T20:10:31.207186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "policy_cartpole = PolicyNetwork(max_input_dim, cartpole_hidden_sizes_theta, max_output_dim, dropout_layers, cartpole_dropout_p)\n",
    "policy_cartpole.load_state_dict(torch.load(\"pretrained_models/cartpole-v1_policy.pth\"))\n",
    "value_cartpole = ValueNetwork(max_input_dim, cartpole_hidden_sizes_w)\n",
    "value_cartpole.load_state_dict(torch.load(\"pretrained_models/cartpole-v1_value.pth\"))"
   ],
   "id": "15cf55690fdbf5e6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/94wmcf8921379lkml24q3wbr0000gn/T/ipykernel_70257/4156715337.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy_cartpole.load_state_dict(torch.load(\"pretrained_models/cartpole-v1_policy.pth\"))\n",
      "/var/folders/q7/94wmcf8921379lkml24q3wbr0000gn/T/ipykernel_70257/4156715337.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  value_cartpole.load_state_dict(torch.load(\"pretrained_models/cartpole-v1_value.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T20:10:47.714142Z",
     "start_time": "2024-12-29T20:10:32.498371Z"
    }
   },
   "cell_type": "code",
   "source": "run_experiment(\"MountainCarContinuous-v0\", policy_cartpole, value_cartpole, cartpole_hidden_sizes_theta, cartpole_hidden_sizes_w)",
   "id": "dd7f93685cb96dc1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-29 22:10:32,499] A new study created in memory with name: MountainCarContinuous-v0_study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 0] Env=MountainCarContinuous-v0:\n",
      "        hidden_sizes_theta=[16, 32, 16], hidden_sizes_w=[32, 64, 32],\n",
      "         gamma=0.95, dropout_p=0.30000000000000004,\n",
      "         alpha_theta=0.0007, alpha_w=0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 36/2000 [00:14<13:30,  2.42episode/s]\n",
      "[W 2024-12-29 22:10:47,358] Trial 0 failed with parameters: {'hidden_sizes_theta': '[16, 32, 16]', 'hidden_sizes_w': '[32, 64, 32]', 'gamma': 0.95, 'alpha_theta': 0.0007, 'alpha_w': 0.0007, 'dropout_p': 0.30000000000000004} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/Users/nadav/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py\", line 153, in objective_wrapper\n",
      "    return self.objective(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nadav/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py\", line 116, in objective\n",
      "    policy_network, value_network, rewards, train_time = self.train_function(**train_params)\n",
      "                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/q7/94wmcf8921379lkml24q3wbr0000gn/T/ipykernel_70257/1671091987.py\", line 47, in fine_tune_actor_critic\n",
      "    train_time = training_loop(\n",
      "                 ^^^^^^^^^^^^^^\n",
      "  File \"/Users/nadav/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/training_loop.py\", line 86, in training_loop\n",
      "    policy_optimizer.step()\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 487, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/adam.py\", line 223, in step\n",
      "    adam(\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 154, in maybe_fallback\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/adam.py\", line 784, in adam\n",
      "    func(\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/adam.py\", line 356, in _single_tensor_adam\n",
      "    if not torch._utils.is_compiling() and capturable:\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/_utils.py\", line 873, in is_compiling\n",
      "    return torch.compiler.is_compiling()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/compiler/__init__.py\", line 308, in is_compiling\n",
      "    if torch.jit.is_scripting():\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/_jit_internal.py\", line 103, in is_scripting\n",
      "    def is_scripting() -> bool:\n",
      "    \n",
      "KeyboardInterrupt\n",
      "[W 2024-12-29 22:10:47,365] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m run_experiment(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMountainCarContinuous-v0\u001B[39m\u001B[38;5;124m\"\u001B[39m, policy_cartpole, value_cartpole, cartpole_hidden_sizes_theta, cartpole_hidden_sizes_w)\n",
      "Cell \u001B[0;32mIn[5], line 16\u001B[0m, in \u001B[0;36mrun_experiment\u001B[0;34m(env_name, source_policy_network, source_value_network, fixed_hidden_theta, fixed_hidden_w)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_experiment\u001B[39m(env_name, source_policy_network, source_value_network, fixed_hidden_theta, fixed_hidden_w):\n\u001B[1;32m      2\u001B[0m     optuna_search \u001B[38;5;241m=\u001B[39m OptunaSearch(\n\u001B[1;32m      3\u001B[0m         train_function\u001B[38;5;241m=\u001B[39mfine_tune_actor_critic,\n\u001B[1;32m      4\u001B[0m         env_name\u001B[38;5;241m=\u001B[39menv_name,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     14\u001B[0m         episodes\u001B[38;5;241m=\u001B[39mepisodes,\n\u001B[1;32m     15\u001B[0m     )\n\u001B[0;32m---> 16\u001B[0m     best_policy, best_value, best_params, best_reward, study \u001B[38;5;241m=\u001B[39m optuna_search\u001B[38;5;241m.\u001B[39moptuna_search_for_env(\n\u001B[1;32m     17\u001B[0m         n_trials\u001B[38;5;241m=\u001B[39mn_trials,\n\u001B[1;32m     18\u001B[0m         source_policy_network\u001B[38;5;241m=\u001B[39msource_policy_network,\n\u001B[1;32m     19\u001B[0m         source_value_network\u001B[38;5;241m=\u001B[39msource_value_network,\n\u001B[1;32m     20\u001B[0m         fixed_hidden_theta\u001B[38;5;241m=\u001B[39mfixed_hidden_theta,\n\u001B[1;32m     21\u001B[0m         fixed_hidden_w\u001B[38;5;241m=\u001B[39mfixed_hidden_w,\n\u001B[1;32m     22\u001B[0m     )\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mDone! Best parameters found by Optuna:\u001B[39m\u001B[38;5;124m\"\u001B[39m, best_params)\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest reward from Optuna:\u001B[39m\u001B[38;5;124m\"\u001B[39m, best_reward)\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py:162\u001B[0m, in \u001B[0;36mOptunaSearch.optuna_search_for_env\u001B[0;34m(self, n_trials, study_name, source_policy_network, source_value_network, fixed_hidden_theta, fixed_hidden_w)\u001B[0m\n\u001B[1;32m    153\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective(\n\u001B[1;32m    154\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[1;32m    155\u001B[0m         source_policy_network\u001B[38;5;241m=\u001B[39msource_policy_network,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    158\u001B[0m         fixed_hidden_w\u001B[38;5;241m=\u001B[39mfixed_hidden_w\n\u001B[1;32m    159\u001B[0m     )\n\u001B[1;32m    161\u001B[0m \u001B[38;5;66;03m# 3. Run the optimization for n_trials\u001B[39;00m\n\u001B[0;32m--> 162\u001B[0m study\u001B[38;5;241m.\u001B[39moptimize(objective_wrapper, n_trials\u001B[38;5;241m=\u001B[39mn_trials)\n\u001B[1;32m    164\u001B[0m \u001B[38;5;66;03m# 4. Print some info about the best trial\u001B[39;00m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m[OPTUNA] Best trial: trail \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstudy\u001B[38;5;241m.\u001B[39mbest_trial\u001B[38;5;241m.\u001B[39mnumber\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/study.py:475\u001B[0m, in \u001B[0;36mStudy.optimize\u001B[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m    373\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moptimize\u001B[39m(\n\u001B[1;32m    374\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    375\u001B[0m     func: ObjectiveFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    382\u001B[0m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    383\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    384\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[1;32m    385\u001B[0m \n\u001B[1;32m    386\u001B[0m \u001B[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    473\u001B[0m \u001B[38;5;124;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[1;32m    474\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 475\u001B[0m     _optimize(\n\u001B[1;32m    476\u001B[0m         study\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    477\u001B[0m         func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[1;32m    478\u001B[0m         n_trials\u001B[38;5;241m=\u001B[39mn_trials,\n\u001B[1;32m    479\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m    480\u001B[0m         n_jobs\u001B[38;5;241m=\u001B[39mn_jobs,\n\u001B[1;32m    481\u001B[0m         catch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mtuple\u001B[39m(catch) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(catch, Iterable) \u001B[38;5;28;01melse\u001B[39;00m (catch,),\n\u001B[1;32m    482\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mcallbacks,\n\u001B[1;32m    483\u001B[0m         gc_after_trial\u001B[38;5;241m=\u001B[39mgc_after_trial,\n\u001B[1;32m    484\u001B[0m         show_progress_bar\u001B[38;5;241m=\u001B[39mshow_progress_bar,\n\u001B[1;32m    485\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:63\u001B[0m, in \u001B[0;36m_optimize\u001B[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m---> 63\u001B[0m         _optimize_sequential(\n\u001B[1;32m     64\u001B[0m             study,\n\u001B[1;32m     65\u001B[0m             func,\n\u001B[1;32m     66\u001B[0m             n_trials,\n\u001B[1;32m     67\u001B[0m             timeout,\n\u001B[1;32m     68\u001B[0m             catch,\n\u001B[1;32m     69\u001B[0m             callbacks,\n\u001B[1;32m     70\u001B[0m             gc_after_trial,\n\u001B[1;32m     71\u001B[0m             reseed_sampler_rng\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     72\u001B[0m             time_start\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     73\u001B[0m             progress_bar\u001B[38;5;241m=\u001B[39mprogress_bar,\n\u001B[1;32m     74\u001B[0m         )\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     76\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:160\u001B[0m, in \u001B[0;36m_optimize_sequential\u001B[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[1;32m    157\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 160\u001B[0m     frozen_trial \u001B[38;5;241m=\u001B[39m _run_trial(study, func, catch)\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[1;32m    164\u001B[0m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:248\u001B[0m, in \u001B[0;36m_run_trial\u001B[0;34m(study, func, catch)\u001B[0m\n\u001B[1;32m    241\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShould not reach.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    243\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    244\u001B[0m     frozen_trial\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m==\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mFAIL\n\u001B[1;32m    245\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    246\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[1;32m    247\u001B[0m ):\n\u001B[0;32m--> 248\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/optuna/study/_optimize.py:197\u001B[0m, in \u001B[0;36m_run_trial\u001B[0;34m(study, func, catch)\u001B[0m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial\u001B[38;5;241m.\u001B[39m_trial_id, study\u001B[38;5;241m.\u001B[39m_storage):\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 197\u001B[0m         value_or_values \u001B[38;5;241m=\u001B[39m func(trial)\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mTrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    199\u001B[0m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[1;32m    200\u001B[0m         state \u001B[38;5;241m=\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mPRUNED\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py:153\u001B[0m, in \u001B[0;36mOptunaSearch.optuna_search_for_env.<locals>.objective_wrapper\u001B[0;34m(trial)\u001B[0m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mobjective_wrapper\u001B[39m(trial):\n\u001B[0;32m--> 153\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective(\n\u001B[1;32m    154\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[1;32m    155\u001B[0m         source_policy_network\u001B[38;5;241m=\u001B[39msource_policy_network,\n\u001B[1;32m    156\u001B[0m         source_value_network\u001B[38;5;241m=\u001B[39msource_value_network,\n\u001B[1;32m    157\u001B[0m         fixed_hidden_theta\u001B[38;5;241m=\u001B[39mfixed_hidden_theta,\n\u001B[1;32m    158\u001B[0m         fixed_hidden_w\u001B[38;5;241m=\u001B[39mfixed_hidden_w\n\u001B[1;32m    159\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/optuna_search.py:116\u001B[0m, in \u001B[0;36mOptunaSearch.objective\u001B[0;34m(self, trial, source_policy_network, source_value_network, fixed_hidden_theta, fixed_hidden_w)\u001B[0m\n\u001B[1;32m    113\u001B[0m     train_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource_policy_network\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m source_policy_network\n\u001B[1;32m    114\u001B[0m     train_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource_value_network\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m source_value_network\n\u001B[0;32m--> 116\u001B[0m policy_network, value_network, rewards, train_time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_function(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtrain_params)\n\u001B[1;32m    118\u001B[0m \u001B[38;5;66;03m# 3. Calculate the metric to minimize\u001B[39;00m\n\u001B[1;32m    119\u001B[0m episodes_trained \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(rewards)\n",
      "Cell \u001B[0;32mIn[3], line 47\u001B[0m, in \u001B[0;36mfine_tune_actor_critic\u001B[0;34m(source_policy_network, source_value_network, env_name, input_dim, output_dim, hidden_sizes_theta, hidden_sizes_w, dropout_layers, alpha_theta, alpha_w, episodes, gamma, dropout_p, log_dir)\u001B[0m\n\u001B[1;32m     43\u001B[0m rewards_per_episode \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     45\u001B[0m actual_act_dim \u001B[38;5;241m=\u001B[39m ENV_ACT_DIM[env_name]\n\u001B[0;32m---> 47\u001B[0m train_time \u001B[38;5;241m=\u001B[39m training_loop(\n\u001B[1;32m     48\u001B[0m     input_dim\u001B[38;5;241m=\u001B[39minput_dim,\n\u001B[1;32m     49\u001B[0m     actual_act_dim\u001B[38;5;241m=\u001B[39mactual_act_dim,\n\u001B[1;32m     50\u001B[0m     policy_network\u001B[38;5;241m=\u001B[39mfine_tuned_policy_network,\n\u001B[1;32m     51\u001B[0m     value_network\u001B[38;5;241m=\u001B[39mfine_tuned_value_network,\n\u001B[1;32m     52\u001B[0m     policy_optimizer\u001B[38;5;241m=\u001B[39mpolicy_optimizer,\n\u001B[1;32m     53\u001B[0m     value_optimizer\u001B[38;5;241m=\u001B[39mvalue_optimizer,\n\u001B[1;32m     54\u001B[0m     env\u001B[38;5;241m=\u001B[39menv,\n\u001B[1;32m     55\u001B[0m     env_name\u001B[38;5;241m=\u001B[39menv_name,\n\u001B[1;32m     56\u001B[0m     gamma\u001B[38;5;241m=\u001B[39mgamma,\n\u001B[1;32m     57\u001B[0m     episodes\u001B[38;5;241m=\u001B[39mepisodes,\n\u001B[1;32m     58\u001B[0m     writer\u001B[38;5;241m=\u001B[39mwriter,\n\u001B[1;32m     59\u001B[0m     rewards_per_episode\u001B[38;5;241m=\u001B[39mrewards_per_episode,\n\u001B[1;32m     60\u001B[0m )\n\u001B[1;32m     62\u001B[0m writer\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m     63\u001B[0m env\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/PycharmProjects/Deep-Reinforcement-Learning-Policy-Gradient-Methods/assignment3/training_loop.py:86\u001B[0m, in \u001B[0;36mtraining_loop\u001B[0;34m(input_dim, actual_act_dim, policy_network, value_network, policy_optimizer, value_optimizer, env, env_name, episodes, gamma, writer, rewards_per_episode)\u001B[0m\n\u001B[1;32m     84\u001B[0m policy_optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     85\u001B[0m policy_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 86\u001B[0m policy_optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     88\u001B[0m \u001B[38;5;66;03m# Update the factor I\u001B[39;00m\n\u001B[1;32m     89\u001B[0m I \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m gamma\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/optimizer.py:487\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    482\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    483\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    484\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    485\u001B[0m             )\n\u001B[0;32m--> 487\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    488\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    490\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/optimizer.py:91\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     89\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     90\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 91\u001B[0m     ret \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     93\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/adam.py:223\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    211\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    213\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    214\u001B[0m         group,\n\u001B[1;32m    215\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    220\u001B[0m         state_steps,\n\u001B[1;32m    221\u001B[0m     )\n\u001B[0;32m--> 223\u001B[0m     adam(\n\u001B[1;32m    224\u001B[0m         params_with_grad,\n\u001B[1;32m    225\u001B[0m         grads,\n\u001B[1;32m    226\u001B[0m         exp_avgs,\n\u001B[1;32m    227\u001B[0m         exp_avg_sqs,\n\u001B[1;32m    228\u001B[0m         max_exp_avg_sqs,\n\u001B[1;32m    229\u001B[0m         state_steps,\n\u001B[1;32m    230\u001B[0m         amsgrad\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mamsgrad\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    231\u001B[0m         has_complex\u001B[38;5;241m=\u001B[39mhas_complex,\n\u001B[1;32m    232\u001B[0m         beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[1;32m    233\u001B[0m         beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[1;32m    234\u001B[0m         lr\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    235\u001B[0m         weight_decay\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweight_decay\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    236\u001B[0m         eps\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meps\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    237\u001B[0m         maximize\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    238\u001B[0m         foreach\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mforeach\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    239\u001B[0m         capturable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcapturable\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    240\u001B[0m         differentiable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    241\u001B[0m         fused\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfused\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    242\u001B[0m         grad_scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad_scale\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    243\u001B[0m         found_inf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    244\u001B[0m     )\n\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/optimizer.py:154\u001B[0m, in \u001B[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 154\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/adam.py:784\u001B[0m, in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    781\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    782\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[0;32m--> 784\u001B[0m func(\n\u001B[1;32m    785\u001B[0m     params,\n\u001B[1;32m    786\u001B[0m     grads,\n\u001B[1;32m    787\u001B[0m     exp_avgs,\n\u001B[1;32m    788\u001B[0m     exp_avg_sqs,\n\u001B[1;32m    789\u001B[0m     max_exp_avg_sqs,\n\u001B[1;32m    790\u001B[0m     state_steps,\n\u001B[1;32m    791\u001B[0m     amsgrad\u001B[38;5;241m=\u001B[39mamsgrad,\n\u001B[1;32m    792\u001B[0m     has_complex\u001B[38;5;241m=\u001B[39mhas_complex,\n\u001B[1;32m    793\u001B[0m     beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[1;32m    794\u001B[0m     beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[1;32m    795\u001B[0m     lr\u001B[38;5;241m=\u001B[39mlr,\n\u001B[1;32m    796\u001B[0m     weight_decay\u001B[38;5;241m=\u001B[39mweight_decay,\n\u001B[1;32m    797\u001B[0m     eps\u001B[38;5;241m=\u001B[39meps,\n\u001B[1;32m    798\u001B[0m     maximize\u001B[38;5;241m=\u001B[39mmaximize,\n\u001B[1;32m    799\u001B[0m     capturable\u001B[38;5;241m=\u001B[39mcapturable,\n\u001B[1;32m    800\u001B[0m     differentiable\u001B[38;5;241m=\u001B[39mdifferentiable,\n\u001B[1;32m    801\u001B[0m     grad_scale\u001B[38;5;241m=\u001B[39mgrad_scale,\n\u001B[1;32m    802\u001B[0m     found_inf\u001B[38;5;241m=\u001B[39mfound_inf,\n\u001B[1;32m    803\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/optim/adam.py:356\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[1;32m    353\u001B[0m step_t \u001B[38;5;241m=\u001B[39m state_steps[i]\n\u001B[1;32m    355\u001B[0m \u001B[38;5;66;03m# If compiling, the compiler will handle cudagraph checks, see note [torch.compile x capturable]\u001B[39;00m\n\u001B[0;32m--> 356\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_utils\u001B[38;5;241m.\u001B[39mis_compiling() \u001B[38;5;129;01mand\u001B[39;00m capturable:\n\u001B[1;32m    357\u001B[0m     capturable_supported_devices \u001B[38;5;241m=\u001B[39m _get_capturable_supported_devices()\n\u001B[1;32m    358\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m (\n\u001B[1;32m    359\u001B[0m         param\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m step_t\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype\n\u001B[1;32m    360\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m param\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;129;01min\u001B[39;00m capturable_supported_devices\n\u001B[1;32m    361\u001B[0m     ), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf capturable=True, params and state_steps must be on supported devices: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcapturable_supported_devices\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/_utils.py:873\u001B[0m, in \u001B[0;36mis_compiling\u001B[0;34m()\u001B[0m\n\u001B[1;32m    867\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_compiling\u001B[39m() \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[1;32m    868\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    869\u001B[0m \u001B[38;5;124;03m    Indicates whether we are tracing/compiling with torch.compile() or torch.export().\u001B[39;00m\n\u001B[1;32m    870\u001B[0m \n\u001B[1;32m    871\u001B[0m \u001B[38;5;124;03m    TODO(khabinov): we should deprecate this function and use torch.compiler.is_compiling().\u001B[39;00m\n\u001B[1;32m    872\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 873\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcompiler\u001B[38;5;241m.\u001B[39mis_compiling()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/compiler/__init__.py:308\u001B[0m, in \u001B[0;36mis_compiling\u001B[0;34m()\u001B[0m\n\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_compiling\u001B[39m() \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[1;32m    293\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    294\u001B[0m \u001B[38;5;124;03m    Indicates whether a graph is executed/traced as part of torch.compile() or torch.export().\u001B[39;00m\n\u001B[1;32m    295\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    306\u001B[0m \u001B[38;5;124;03m        >>>     # ...rest of the function...\u001B[39;00m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 308\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mis_scripting():\n\u001B[1;32m    309\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    310\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/DRLCourse/lib/python3.11/site-packages/torch/_jit_internal.py:103\u001B[0m, in \u001B[0;36mis_scripting\u001B[0;34m()\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m7\u001B[39m):\n\u001B[1;32m    100\u001B[0m     \u001B[38;5;28mglobals\u001B[39m()[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBroadcastingList\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m BroadcastingList1\n\u001B[0;32m--> 103\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_scripting\u001B[39m() \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[1;32m    104\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;124;03m    Function that returns True when in compilation and False otherwise. This\u001B[39;00m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;124;03m    is useful especially with the @unused decorator to leave code in your\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;124;03m                return unsupported_linear_op(x)\u001B[39;00m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m    122\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8e31d7236d02796e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
