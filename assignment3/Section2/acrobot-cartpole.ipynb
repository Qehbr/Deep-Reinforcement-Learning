{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:24:21.017694Z",
     "start_time": "2025-01-09T07:24:10.848199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "import gymnasium\n",
    "import optuna\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from assignment3.Section1.CartPole_AcroBot.dim_alignment import max_input_dim, max_output_dim\n",
    "from assignment3.Section1.CartPole_AcroBot.models import PolicyNetwork, ValueNetwork\n",
    "from assignment3.Section1.CartPole_AcroBot.action_selector import ActionSelector\n",
    "from assignment3.Section1.CartPole_AcroBot.device import get_device\n",
    "from assignment3.Section1.CartPole_AcroBot.training_loop import training_loop"
   ],
   "id": "1984e03547cdb349",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rusanov\\.conda\\envs\\DRL\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:24:21.054309Z",
     "start_time": "2025-01-09T07:24:21.020823Z"
    }
   },
   "cell_type": "code",
   "source": "device = get_device()",
   "id": "49dc143a91b16119",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:24:21.407900Z",
     "start_time": "2025-01-09T07:24:21.404290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reinitialize_output_layer(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear) and module.out_features == 3:\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "    return model"
   ],
   "id": "30a687a7a55160a0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:24:21.417962Z",
     "start_time": "2025-01-09T07:24:21.411396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# best hyperparameters\n",
    "# hidden_sizes_theta = [32, 64, 32]\n",
    "hidden_sizes_w = [16, 32, 16]\n",
    "alpha_theta = 0.0007\n",
    "alpha_w = 0.0006000000000000001\n",
    "gamma = 0.98\n",
    "\n",
    "episodes = 1000"
   ],
   "id": "adc2f5e5bfa430b2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T08:59:19.698802Z",
     "start_time": "2025-01-09T08:59:19.693762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna hyperparameter optimization.\n",
    "    Fine-tunes the Acrobot policy network on the CartPole environment.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define hyperparameter search space\n",
    "    alpha_theta = trial.suggest_loguniform('alpha_theta', 1e-5, 1e-2)\n",
    "    alpha_w = trial.suggest_loguniform('alpha_w', 1e-5, 1e-2)\n",
    "    gamma = trial.suggest_uniform('gamma', 0.90, 0.999)\n",
    "\n",
    "    episodes = 2000  # You can also make this a hyperparameter if desired\n",
    "\n",
    "    # Define the unique log directory for this trial\n",
    "    log_dir = f\"runs/fine_tuning_acrobot_to_cartpole_trial_{trial.number}\"\n",
    "\n",
    "    # Print the trial number and hyperparameters\n",
    "    hyperparams = {\n",
    "        'alpha_theta': alpha_theta,\n",
    "        'alpha_w': alpha_w,\n",
    "        'gamma': gamma,\n",
    "    }\n",
    "    hyperparams_str = ', '.join([f\"{key}={value:.6f}\" for key, value in hyperparams.items()])\n",
    "    print(f\"Starting Trial {trial.number}: {hyperparams_str}\")\n",
    "\n",
    "    # Initialize networks\n",
    "\n",
    "    # Load the pre-trained Acrobot model\n",
    "    acrobot_policy_network = PolicyNetwork(max_input_dim, [32, 64, 32], max_output_dim).to(device)\n",
    "    acrobot_policy_network.load_state_dict(torch.load(\n",
    "        '../Section1/CartPole_AcroBot/models/Acrobot-v1/best/policy.pth',\n",
    "        map_location=device\n",
    "    ))\n",
    "\n",
    "    acrobot_policy_network = reinitialize_output_layer(acrobot_policy_network)\n",
    "    acrobot_policy_network.train()\n",
    "\n",
    "        # Initialize the Value Network\n",
    "    value_network = ValueNetwork(max_input_dim, hidden_sizes_w).to(device)\n",
    "    value_network.train()\n",
    "\n",
    "    # Initialize optimizers\n",
    "    policy_optimizer = Adam(acrobot_policy_network.parameters(), lr=alpha_theta)\n",
    "    value_optimizer = Adam(value_network.parameters(), lr=alpha_w)\n",
    "\n",
    "\n",
    "    # Initialize rewards tracking\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    # Initialize the action selector\n",
    "    action_selector = ActionSelector()\n",
    "\n",
    "    # Set up the CartPole environment\n",
    "    env = gymnasium.make(\"CartPole-v1\")\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    # Start fine-tuning\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_time, avg_reward = training_loop(\n",
    "        input_dim=max_input_dim,\n",
    "        actual_act_dim=env.action_space.n,  # CartPole has 2 actions\n",
    "        policy_network=acrobot_policy_network,\n",
    "        value_network=value_network,\n",
    "        policy_optimizer=policy_optimizer,\n",
    "        value_optimizer=value_optimizer,\n",
    "        env=env,\n",
    "        env_name=\"CartPole-v1\",\n",
    "        episodes=episodes,\n",
    "        gamma=gamma,\n",
    "        writer=writer,\n",
    "        rewards_per_episode=rewards_per_episode,\n",
    "        action_selector=action_selector\n",
    "    )\n",
    "\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Trial {trial.number} completed in {elapsed_time:.2f} seconds with Average Reward {avg_reward:.2f}.\")\n",
    "\n",
    "    # Optionally, save the model for the best trial\n",
    "    # This can be handled outside the objective function if preferred\n",
    "\n",
    "    # Close the writer to free resources\n",
    "    writer.close()\n",
    "\n",
    "    # Return the metric to be maximized\n",
    "    return avg_reward"
   ],
   "id": "d50c1db999dcab93",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T08:59:21.112856Z",
     "start_time": "2025-01-09T08:59:21.108856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_optuna_study(n_trials=50):\n",
    "    \"\"\"\n",
    "    Runs an Optuna study to optimize hyperparameters for fine-tuning the Acrobot policy network.\n",
    "    \"\"\"\n",
    "    # Create a study object\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',  # We aim to maximize the average reward\n",
    "        sampler=optuna.samplers.TPESampler(seed=42)  # Set a seed for reproducibility\n",
    "    )\n",
    "\n",
    "    # Optimize the objective function\n",
    "    study.optimize(objective, n_trials=n_trials, timeout=3600)  # e.g., 50 trials or 1 hour\n",
    "\n",
    "    # Print study statistics\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(f\"  Value (Average Reward): {trial.value:.2f}\")\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "\n",
    "    return study"
   ],
   "id": "d2ace794f4e886b9",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T10:13:41.135268Z",
     "start_time": "2025-01-09T08:59:23.043829Z"
    }
   },
   "cell_type": "code",
   "source": "study = run_optuna_study(n_trials=30)",
   "id": "711cd9b2aca31a14",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-09 10:59:23,044] A new study created in memory with name: no-name-1ab27c33-41e7-458b-9944-dad90ff12eca\n",
      "C:\\Users\\rusanov\\AppData\\Local\\Temp\\ipykernel_41620\\178151069.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha_theta = trial.suggest_loguniform('alpha_theta', 1e-5, 1e-2)\n",
      "C:\\Users\\rusanov\\AppData\\Local\\Temp\\ipykernel_41620\\178151069.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha_w = trial.suggest_loguniform('alpha_w', 1e-5, 1e-2)\n",
      "C:\\Users\\rusanov\\AppData\\Local\\Temp\\ipykernel_41620\\178151069.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  gamma = trial.suggest_uniform('gamma', 0.90, 0.999)\n",
      "C:\\Users\\rusanov\\AppData\\Local\\Temp\\ipykernel_41620\\178151069.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  acrobot_policy_network.load_state_dict(torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Trial 0: alpha_theta=0.000133, alpha_w=0.007114, gamma=0.972467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2000/2000 [17:02<00:00,  1.96episode/s, Avg Reward(100)=389.24]\n",
      "[I 2025-01-09 11:16:25,248] Trial 0 finished with value: 389.24 and parameters: {'alpha_theta': 0.0001329291894316216, 'alpha_w': 0.0071144760093434225, 'gamma': 0.9724674002393291}. Best is trial 0 with value: 389.24.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 completed in 1022.19 seconds with Average Reward 389.24.\n",
      "Starting Trial 1: alpha_theta=0.000625, alpha_w=0.000029, gamma=0.915443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2000/2000 [01:17<00:00, 25.69episode/s, Avg Reward(100)=9.36]\n",
      "[I 2025-01-09 11:17:43,110] Trial 1 finished with value: 9.36 and parameters: {'alpha_theta': 0.0006251373574521745, 'alpha_w': 2.9380279387035334e-05, 'gamma': 0.915443457513284}. Best is trial 0 with value: 389.24.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 completed in 77.86 seconds with Average Reward 9.36.\n",
      "Starting Trial 2: alpha_theta=0.000015, alpha_w=0.003968, gamma=0.959510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2000/2000 [21:12<00:00,  1.57episode/s, Avg Reward(100)=197.15]\n",
      "[I 2025-01-09 11:38:55,958] Trial 2 finished with value: 197.15 and parameters: {'alpha_theta': 1.493656855461762e-05, 'alpha_w': 0.003967605077052989, 'gamma': 0.9595103861625777}. Best is trial 0 with value: 389.24.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 completed in 1272.84 seconds with Average Reward 197.15.\n",
      "Starting Trial 3: alpha_theta=0.001331, alpha_w=0.000012, gamma=0.996021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2000/2000 [01:17<00:00, 25.92episode/s, Avg Reward(100)=9.48]\n",
      "[I 2025-01-09 11:40:13,131] Trial 3 finished with value: 9.48 and parameters: {'alpha_theta': 0.001331121608073689, 'alpha_w': 1.1527987128232396e-05, 'gamma': 0.9960210753640374}. Best is trial 0 with value: 389.24.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 completed in 77.16 seconds with Average Reward 9.48.\n",
      "Starting Trial 4: alpha_theta=0.003143, alpha_w=0.000043, gamma=0.918001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2000/2000 [01:15<00:00, 26.44episode/s, Avg Reward(100)=9.28]\n",
      "[I 2025-01-09 11:41:28,788] Trial 4 finished with value: 9.28 and parameters: {'alpha_theta': 0.00314288089084011, 'alpha_w': 4.335281794951564e-05, 'gamma': 0.918000671753503}. Best is trial 0 with value: 389.24.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 completed in 75.64 seconds with Average Reward 9.28.\n",
      "Starting Trial 5: alpha_theta=0.000035, alpha_w=0.000082, gamma=0.951951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2000/2000 [01:13<00:00, 27.03episode/s, Avg Reward(100)=9.35]\n",
      "[I 2025-01-09 11:42:42,780] Trial 5 finished with value: 9.35 and parameters: {'alpha_theta': 3.5498788321965036e-05, 'alpha_w': 8.17949947521167e-05, 'gamma': 0.9519508867315916}. Best is trial 0 with value: 389.24.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 completed in 73.98 seconds with Average Reward 9.35.\n",
      "Starting Trial 6: alpha_theta=0.000198, alpha_w=0.000075, gamma=0.960573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2000/2000 [01:11<00:00, 28.08episode/s, Avg Reward(100)=9.34]\n",
      "[I 2025-01-09 11:43:54,013] Trial 6 finished with value: 9.34 and parameters: {'alpha_theta': 0.00019762189340280086, 'alpha_w': 7.476312062252303e-05, 'gamma': 0.9605734365775156}. Best is trial 0 with value: 389.24.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 completed in 71.22 seconds with Average Reward 9.34.\n",
      "Starting Trial 7: alpha_theta=0.000026, alpha_w=0.000075, gamma=0.936270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2000/2000 [01:17<00:00, 25.93episode/s, Avg Reward(100)=9.37]\n",
      "[I 2025-01-09 11:45:11,164] Trial 7 finished with value: 9.37 and parameters: {'alpha_theta': 2.621087878265438e-05, 'alpha_w': 7.52374288453485e-05, 'gamma': 0.9362698224860755}. Best is trial 0 with value: 389.24.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 completed in 77.14 seconds with Average Reward 9.37.\n",
      "Starting Trial 8: alpha_theta=0.000233, alpha_w=0.002267, gamma=0.919768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2000/2000 [13:19<00:00,  2.50episode/s, Avg Reward(100)=9.40]  \n",
      "[I 2025-01-09 11:58:30,527] Trial 8 finished with value: 9.4 and parameters: {'alpha_theta': 0.00023345864076016249, 'alpha_w': 0.0022673986523780395, 'gamma': 0.9197677044336776}. Best is trial 0 with value: 389.24.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 completed in 799.35 seconds with Average Reward 9.40.\n",
      "Starting Trial 9: alpha_theta=0.000349, alpha_w=0.000599, gamma=0.904599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  82%|████████▏ | 1645/2000 [15:10<03:16,  1.81episode/s, Avg Reward(100)=366.85]\n",
      "[I 2025-01-09 12:13:41,131] Trial 9 finished with value: 475.44 and parameters: {'alpha_theta': 0.0003489018845491386, 'alpha_w': 0.0005987474910461401, 'gamma': 0.9045985908592798}. Best is trial 9 with value: 475.44.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved CartPole-v1 in 1646 episodes!\n",
      "Trial 9 completed in 910.59 seconds with Average Reward 475.44.\n",
      "Number of finished trials:  10\n",
      "Best trial:\n",
      "  Value (Average Reward): 475.44\n",
      "  Params: \n",
      "    alpha_theta: 0.0003489018845491386\n",
      "    alpha_w: 0.0005987474910461401\n",
      "    gamma: 0.9045985908592798\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T07:57:15.873924Z",
     "start_time": "2025-01-09T07:57:15.870925Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "287e300c106e4b65",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
