{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-04T11:44:01.087137Z",
     "start_time": "2025-01-04T11:43:59.646736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from training_loop import training_loop\n",
    "from models import PolicyNetwork, ValueNetwork\n",
    "from dim_alignment import ENV_ACT_DIM, max_output_dim, max_input_dim\n",
    "from optuna_search import OptunaSearch\n",
    "from hyper_params import StudyFloatParamRange, HyperParamsRanges, HyperParams\n",
    "from device import get_device\n",
    "from action_selector import ActionSelector"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T11:44:01.102689Z",
     "start_time": "2025-01-04T11:44:01.097137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generalized_actor_critic(\n",
    "        env_name,\n",
    "        input_dim,\n",
    "        output_dim,\n",
    "        episodes,\n",
    "        hyper_params: HyperParams,\n",
    "        log_dir=\"runs/actor_critic\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a policy and value network using Actor-Critic, with padded inputs/outputs.\n",
    "    \"\"\"\n",
    "    device = get_device()\n",
    "    env = gym.make(env_name)\n",
    "    writer = SummaryWriter(log_dir=f\"{log_dir}_{env_name}\")\n",
    "\n",
    "    policy_network = PolicyNetwork(input_dim, hyper_params.hidden_sizes_theta, output_dim).to(device)\n",
    "    value_network = ValueNetwork(input_dim, hyper_params.hidden_sizes_w).to(device)\n",
    "\n",
    "    policy_optimizer = optim.Adam(policy_network.parameters(), lr=hyper_params.alpha_theta)\n",
    "    value_optimizer = optim.Adam(value_network.parameters(), lr=hyper_params.alpha_w)\n",
    "\n",
    "    action_selector = ActionSelector()\n",
    "\n",
    "    rewards_per_episode = []\n",
    "\n",
    "    # Identify the actual dimensionality for this env\n",
    "    actual_act_dim = ENV_ACT_DIM[env_name]\n",
    "\n",
    "    train_time = training_loop(\n",
    "        input_dim=input_dim,\n",
    "        actual_act_dim=actual_act_dim,\n",
    "        policy_network=policy_network,\n",
    "        value_network=value_network,\n",
    "        policy_optimizer=policy_optimizer,\n",
    "        value_optimizer=value_optimizer,\n",
    "        env=env,\n",
    "        env_name=env_name,\n",
    "        episodes=episodes,\n",
    "        gamma=hyper_params.gamma,\n",
    "        writer=writer,\n",
    "        rewards_per_episode=rewards_per_episode,\n",
    "        action_selector=action_selector\n",
    "    )\n",
    "\n",
    "    writer.close()\n",
    "    env.close()\n",
    "\n",
    "    return policy_network, value_network, rewards_per_episode, train_time"
   ],
   "id": "39d88d020add6902",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T12:20:42.441849Z",
     "start_time": "2025-01-04T12:20:42.427850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "episodes = 2000\n",
    "n_trials = 10\n",
    "\n",
    "# Common hyperparameters for all environments\n",
    "hyper_params_default_ranges = HyperParamsRanges(\n",
    "    hidden_sizes_theta_values=[\"[16, 32, 16]\", \"[32, 64, 32]\"],\n",
    "    hidden_sizes_w_values=[\"[16, 32, 16]\", \"[32, 64, 32]\"],\n",
    "    alpha_theta_values=StudyFloatParamRange(low=0.0005, high=0.0008, step=0.0001),\n",
    "    alpha_w_values=StudyFloatParamRange(low=0.0005, high=0.0008, step=0.0001),\n",
    "    gamma_values=StudyFloatParamRange(low=0.95, high=0.99, step=0.01),\n",
    ")"
   ],
   "id": "654c9ac89165a89f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T12:20:42.534028Z",
     "start_time": "2025-01-04T12:20:42.519777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_experiment(env_name,\n",
    "                   episodes=episodes,\n",
    "                   hyper_params_ranges=hyper_params_default_ranges,\n",
    "                   n_trials=n_trials):\n",
    "    optuna_search = OptunaSearch(\n",
    "        train_function=generalized_actor_critic,\n",
    "        env_name=env_name,\n",
    "        max_input_dim=max_input_dim,\n",
    "        max_output_dim=max_output_dim,\n",
    "        episodes=episodes,\n",
    "        hyper_params_ranges=hyper_params_ranges\n",
    "    )\n",
    "    best_policy, best_value, best_params, best_reward, study = optuna_search.optuna_search_for_env(n_trials=n_trials,\n",
    "                                                                                                   study_name=f\"{env_name}_actor_critic_study\")\n",
    "\n",
    "    print(\"\\nDone! Best parameters found by Optuna:\", best_params)\n",
    "    print(\"Best reward from Optuna:\", best_reward)"
   ],
   "id": "3ceaf2c534e0d102",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T13:15:34.044049Z",
     "start_time": "2025-01-04T12:20:42.998679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Launch the search on, say, CartPole-v1\n",
    "run_experiment(\"CartPole-v1\", episodes=1000)"
   ],
   "id": "925b1dfc05860767",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-04 14:20:42,999] A new study created in memory with name: CartPole-v1_actor_critic_study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 0] Env=CartPole-v1\n",
      "hidden_sizes_theta=[32, 64, 32]  |  hidden_sizes_w=[16, 32, 16]\n",
      "        gamma=0.9900\n",
      "        alpha_theta=0.0008  |  alpha_w=0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [06:25<00:00,  2.59episode/s, Avg Reward(100)=345.58]\n",
      "[I 2025-01-04 14:27:08,618] Trial 0 finished with value: 345.58 and parameters: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[16, 32, 16]', 'alpha_theta': 0.0008, 'alpha_w': 0.0007, 'gamma': 0.99}. Best is trial 0 with value: 345.58.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 1] Env=CartPole-v1\n",
      "hidden_sizes_theta=[16, 32, 16]  |  hidden_sizes_w=[32, 64, 32]\n",
      "        gamma=0.9700\n",
      "        alpha_theta=0.0005  |  alpha_w=0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [06:35<00:00,  2.53episode/s, Avg Reward(100)=287.83]\n",
      "[I 2025-01-04 14:33:43,998] Trial 1 finished with value: 287.83 and parameters: {'hidden_sizes_theta': '[16, 32, 16]', 'hidden_sizes_w': '[32, 64, 32]', 'alpha_theta': 0.0005, 'alpha_w': 0.0005, 'gamma': 0.97}. Best is trial 0 with value: 345.58.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 2] Env=CartPole-v1\n",
      "hidden_sizes_theta=[32, 64, 32]  |  hidden_sizes_w=[16, 32, 16]\n",
      "        gamma=0.9600\n",
      "        alpha_theta=0.0007  |  alpha_w=0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 502/1000 [03:56<03:54,  2.13episode/s, Avg Reward(100)=471.44]\n",
      "[I 2025-01-04 14:37:40,060] Trial 2 finished with value: 475.35 and parameters: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[16, 32, 16]', 'alpha_theta': 0.0007, 'alpha_w': 0.0008, 'gamma': 0.96}. Best is trial 2 with value: 475.35.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved CartPole-v1 in 503 episodes!\n",
      "\n",
      "[OPTUNA Trial 3] Env=CartPole-v1\n",
      "hidden_sizes_theta=[16, 32, 16]  |  hidden_sizes_w=[16, 32, 16]\n",
      "        gamma=0.9900\n",
      "        alpha_theta=0.0007  |  alpha_w=0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [09:08<00:00,  1.82episode/s, Avg Reward(100)=419.57]\n",
      "[I 2025-01-04 14:46:48,689] Trial 3 finished with value: 419.57 and parameters: {'hidden_sizes_theta': '[16, 32, 16]', 'hidden_sizes_w': '[16, 32, 16]', 'alpha_theta': 0.0007, 'alpha_w': 0.0008, 'gamma': 0.99}. Best is trial 2 with value: 475.35.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 4] Env=CartPole-v1\n",
      "hidden_sizes_theta=[32, 64, 32]  |  hidden_sizes_w=[16, 32, 16]\n",
      "        gamma=0.9600\n",
      "        alpha_theta=0.0008  |  alpha_w=0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:27<00:00, 35.87episode/s, Avg Reward(100)=9.45]\n",
      "[I 2025-01-04 14:47:16,572] Trial 4 finished with value: 9.45 and parameters: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[16, 32, 16]', 'alpha_theta': 0.0008, 'alpha_w': 0.0005, 'gamma': 0.96}. Best is trial 2 with value: 475.35.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 5] Env=CartPole-v1\n",
      "hidden_sizes_theta=[16, 32, 16]  |  hidden_sizes_w=[32, 64, 32]\n",
      "        gamma=0.9500\n",
      "        alpha_theta=0.0007  |  alpha_w=0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▍  | 748/1000 [06:24<02:09,  1.94episode/s, Avg Reward(100)=418.21]\n",
      "[I 2025-01-04 14:53:41,415] Trial 5 finished with value: 475.83 and parameters: {'hidden_sizes_theta': '[16, 32, 16]', 'hidden_sizes_w': '[32, 64, 32]', 'alpha_theta': 0.0007, 'alpha_w': 0.0006000000000000001, 'gamma': 0.95}. Best is trial 5 with value: 475.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved CartPole-v1 in 749 episodes!\n",
      "\n",
      "[OPTUNA Trial 6] Env=CartPole-v1\n",
      "hidden_sizes_theta=[16, 32, 16]  |  hidden_sizes_w=[32, 64, 32]\n",
      "        gamma=0.9800\n",
      "        alpha_theta=0.0005  |  alpha_w=0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [09:44<00:00,  1.71episode/s, Avg Reward(100)=407.74]\n",
      "[I 2025-01-04 15:03:26,075] Trial 6 finished with value: 407.74 and parameters: {'hidden_sizes_theta': '[16, 32, 16]', 'hidden_sizes_w': '[32, 64, 32]', 'alpha_theta': 0.0005, 'alpha_w': 0.0007, 'gamma': 0.98}. Best is trial 5 with value: 475.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 7] Env=CartPole-v1\n",
      "hidden_sizes_theta=[32, 64, 32]  |  hidden_sizes_w=[32, 64, 32]\n",
      "        gamma=0.9800\n",
      "        alpha_theta=0.0007  |  alpha_w=0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [01:36<00:00, 10.35episode/s, Avg Reward(100)=24.60]\n",
      "[I 2025-01-04 15:05:02,717] Trial 7 finished with value: 24.6 and parameters: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[32, 64, 32]', 'alpha_theta': 0.0007, 'alpha_w': 0.0008, 'gamma': 0.98}. Best is trial 5 with value: 475.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 8] Env=CartPole-v1\n",
      "hidden_sizes_theta=[32, 64, 32]  |  hidden_sizes_w=[16, 32, 16]\n",
      "        gamma=0.9600\n",
      "        alpha_theta=0.0008  |  alpha_w=0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [03:43<00:00,  4.48episode/s, Avg Reward(100)=9.35] \n",
      "[I 2025-01-04 15:08:45,775] Trial 8 finished with value: 9.35 and parameters: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[16, 32, 16]', 'alpha_theta': 0.0008, 'alpha_w': 0.0008, 'gamma': 0.96}. Best is trial 5 with value: 475.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 9] Env=CartPole-v1\n",
      "hidden_sizes_theta=[16, 32, 16]  |  hidden_sizes_w=[16, 32, 16]\n",
      "        gamma=0.9500\n",
      "        alpha_theta=0.0007  |  alpha_w=0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [01:49<00:00,  9.11episode/s, Avg Reward(100)=20.62]\n",
      "[I 2025-01-04 15:10:35,515] Trial 9 finished with value: 20.62 and parameters: {'hidden_sizes_theta': '[16, 32, 16]', 'hidden_sizes_w': '[16, 32, 16]', 'alpha_theta': 0.0007, 'alpha_w': 0.0007, 'gamma': 0.95}. Best is trial 5 with value: 475.83.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA] Best trial: trail 5\n",
      "  Value (Reward): 475.83\n",
      "  Params: {'hidden_sizes_theta': '[16, 32, 16]', 'hidden_sizes_w': '[32, 64, 32]', 'alpha_theta': 0.0007, 'alpha_w': 0.0006000000000000001, 'gamma': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|██████▊   | 680/1000 [04:58<02:20,  2.28episode/s, Avg Reward(100)=269.32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved CartPole-v1 in 681 episodes!\n",
      "\n",
      "Total Optuna search time for CartPole-v1: 3291.04s\n",
      "\n",
      "Done! Best parameters found by Optuna: {'hidden_sizes_theta': '[16, 32, 16]', 'hidden_sizes_w': '[32, 64, 32]', 'alpha_theta': 0.0007, 'alpha_w': 0.0006000000000000001, 'gamma': 0.95}\n",
      "Best reward from Optuna: 475.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T14:14:37.134032Z",
     "start_time": "2025-01-04T13:25:51.067131Z"
    }
   },
   "cell_type": "code",
   "source": "run_experiment(\"Acrobot-v1\", episodes=500, n_trials=3)",
   "id": "60e3b164384bc6a2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-04 15:25:51,069] A new study created in memory with name: Acrobot-v1_actor_critic_study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 0] Env=Acrobot-v1\n",
      "hidden_sizes_theta=[32, 64, 32]  |  hidden_sizes_w=[16, 32, 16]\n",
      "        gamma=0.9500\n",
      "        alpha_theta=0.0007  |  alpha_w=0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [14:48<00:00,  1.78s/episode, Avg Reward(100)=-500.00]\n",
      "[I 2025-01-04 15:40:39,712] Trial 0 finished with value: -500.0 and parameters: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[16, 32, 16]', 'alpha_theta': 0.0007, 'alpha_w': 0.0005, 'gamma': 0.95}. Best is trial 0 with value: -500.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA Trial 1] Env=Acrobot-v1\n",
      "hidden_sizes_theta=[32, 64, 32]  |  hidden_sizes_w=[16, 32, 16]\n",
      "        gamma=0.9800\n",
      "        alpha_theta=0.0007  |  alpha_w=0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 105/500 [00:52<03:17,  2.00episode/s, Avg Reward(100)=-124.69]\n",
      "[I 2025-01-04 15:41:32,283] Trial 1 finished with value: -99.14 and parameters: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[16, 32, 16]', 'alpha_theta': 0.0007, 'alpha_w': 0.0006000000000000001, 'gamma': 0.98}. Best is trial 1 with value: -99.14.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved Acrobot-v1 in 106 episodes!\n",
      "\n",
      "[OPTUNA Trial 2] Env=Acrobot-v1\n",
      "hidden_sizes_theta=[32, 64, 32]  |  hidden_sizes_w=[16, 32, 16]\n",
      "        gamma=0.9700\n",
      "        alpha_theta=0.0007  |  alpha_w=0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [19:46<00:00,  2.37s/episode, Avg Reward(100)=-500.00]\n",
      "[I 2025-01-04 16:01:18,958] Trial 2 finished with value: -500.0 and parameters: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[16, 32, 16]', 'alpha_theta': 0.0007, 'alpha_w': 0.0006000000000000001, 'gamma': 0.97}. Best is trial 1 with value: -99.14.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OPTUNA] Best trial: trail 1\n",
      "  Value (Reward): -99.14\n",
      "  Params: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[16, 32, 16]', 'alpha_theta': 0.0007, 'alpha_w': 0.0006000000000000001, 'gamma': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 500/500 [13:18<00:00,  1.60s/episode, Avg Reward(100)=-500.00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Optuna search time for Acrobot-v1: 2926.05s\n",
      "\n",
      "Done! Best parameters found by Optuna: {'hidden_sizes_theta': '[32, 64, 32]', 'hidden_sizes_w': '[16, 32, 16]', 'alpha_theta': 0.0007, 'alpha_w': 0.0006000000000000001, 'gamma': 0.98}\n",
      "Best reward from Optuna: -99.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
